{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from IPython.display import Image\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import data_processing.sliding_window as helper_functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Preprocessing\n",
    "This notebook will teach you how to preprocess a sensor based Human Activity Recognition dataset.\n",
    "\n",
    "Let's readn-in the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "data_folder = 'data'\n",
    "dataset = '/Users/ahoelzemann/Documents/git/dl-for-har/data/rwhar_3sbjs_data.csv'\n",
    "data = pd.read_csv(os.path.join(data_folder, dataset),\n",
    "                   names=['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity_label'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1 Cleaning\n",
    "\n",
    "##### 1.1.1 Sensor Orientation\n",
    "\n",
    "Whenever we are working with a multimodal dataset, which means a dataset that consists of data from different sensors,\n",
    "we need to make sure that the sensor orientation of the data matches each other.\n",
    "\n",
    "\n",
    "\n",
    "Depending on the circumstances we want to clean the data before we train our classifier.\n",
    "\n",
    "It is very important to double check if the dataset contains **NaN - Values**. If the dataset contains these values\n",
    "make sure, that missing values are interpolated, since we want to keep the original sampling rate.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/html": "<img src=\"../images/pamap_skoda_orientation.png\"/>",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"../images/pamap_skoda_orientation.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def getLastNonNaN(series, index, missingvalues=1):\n",
    "    if not pd.isna(series[index - 1]):\n",
    "        return series[index - 1], missingvalues\n",
    "    else:\n",
    "        return getLastNonNaN(series, index - 1)\n",
    "\n",
    "\n",
    "def getNextNonNaN(series, index, missingvalues=1):\n",
    "    if not pd.isna(series[index + 1]):\n",
    "        return series[index + 1], missingvalues\n",
    "    else:\n",
    "        return getNextNonNaN(series, index + 1, missingvalues=missingvalues + 1)\n",
    "\n",
    "\n",
    "def replaceNaNValues(series, output_dtype='float'):\n",
    "    if output_dtype == 'float':\n",
    "        if series is not np.array:\n",
    "            series = np.array(series)\n",
    "        if pd.isna(series[0]):\n",
    "            series[0] = series[1]\n",
    "        if pd.isna(series[series.shape[0] - 1]):\n",
    "            lastNonNan, numberOfMissingValues = getLastNonNaN(series, series.shape[0] - 1)\n",
    "            if numberOfMissingValues != 1:\n",
    "                for k in range(1, numberOfMissingValues):\n",
    "                    series[series.shape[0] - 1 - k] = lastNonNan\n",
    "            series[series.shape[0] - 1] = series[series.shape[0] - 2]\n",
    "        for x in range(0, series.shape[0]):\n",
    "            if pd.isna(series[x]):\n",
    "                lastNonNan, _ = getLastNonNaN(series, x)\n",
    "                nextNonNan, _ = getNextNonNaN(series, x)\n",
    "                missingValue = (lastNonNan + nextNonNan) / 2\n",
    "                series[x] = missingValue\n",
    "\n",
    "    elif output_dtype == 'int' or output_dtype == 'string':\n",
    "        if series is not np.array:\n",
    "            series = np.array(series)\n",
    "        if pd.isna(series[0]):\n",
    "            series[0] = series[1]\n",
    "        if pd.isna(series[series.shape[0] - 1]):\n",
    "            lastNonNan, numberOfMissingValues = getLastNonNaN(series, series.shape[0] - 1)\n",
    "            if numberOfMissingValues != 1:\n",
    "                for k in range(1, numberOfMissingValues):\n",
    "                    series[series.shape[0] - 1 - k] = lastNonNan\n",
    "            series[series.shape[0] - 1] = series[series.shape[0] - 2]\n",
    "        for x in range(0, series.shape[0]):\n",
    "            if pd.isna(series[x]):\n",
    "                lastNonNan, missingValuesLast = getLastNonNaN(series, x)\n",
    "                nextNonNan, missingValuesNext = getNextNonNaN(series, x)\n",
    "                if missingValuesLast < missingValuesNext:\n",
    "                    series[x] = lastNonNan\n",
    "                else:\n",
    "                    series[x] = nextNonNan\n",
    "    else:\n",
    "        print(\"Please choose a valid output dtype. You can choose between float, int and string.\")\n",
    "        exit(0)\n",
    "\n",
    "    return pd.DataFrame(series, dtype=output_dtype)\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    fill_index = random.randint(1, data.shape[0])\n",
    "    data.loc[fill_index] = [np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "\n",
    "subject_id = replaceNaNValues(data['subject_id'], output_dtype='int')\n",
    "acc_x = replaceNaNValues(data['acc_x'], output_dtype='float')\n",
    "acc_y = replaceNaNValues(data['acc_y'], output_dtype='float')\n",
    "acc_z = replaceNaNValues(data['acc_z'], output_dtype='float')\n",
    "activity_label = replaceNaNValues(data['activity_label'], output_dtype='string')\n",
    "\n",
    "\n",
    "interpolated_data = pd.concat([subject_id, acc_x, acc_y, acc_z, activity_label], axis=1)\n",
    "interpolated_data.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity_label']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Be careful with cleaning the data from noise or outlier, since it only is recommandable if the noise/outlier is not from any importance for the use case of our model.\n",
    "#### 1.2 Resampling\n",
    "\n",
    "Resampling is necessary if we work with sensor data from different sensors, that were not recorded with the sampling rate.\n",
    "The optimize the classifier we need to align the data sampling rates with each other.\n",
    "Resampling can either be done by up- or downsample the data.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def interpolate(data, freq_old, freq_new):\n",
    "    tsAligned = np.divide(np.arange(0, data.shape[0]), freq_old)\n",
    "    timeStep = 1 / freq_new\n",
    "    tsCount = round(tsAligned[-1] / timeStep)\n",
    "    tsMax = tsCount * timeStep\n",
    "    tsNew = np.linspace(tsAligned[0], tsMax, tsCount + 1)\n",
    "    dataNew = np.interp(tsNew, tsAligned, data)\n",
    "\n",
    "    return tsNew, dataNew\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3 Normalizing\n",
    "Normalizing is in an important part in the preprocessing chain, but can also the reason for many mistakes.\n",
    "Therefore it is important to choose the correct strategy for normalizing your dataset.\n",
    "\n",
    "##### 1.3.1 How to normalize?\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3 possible solutions to normalize correctly.\n",
    "Big pitfalls, since beginners tend to normalize the whole vector at once.\n",
    "\n",
    "Normalizing sensor-wise"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.0193068 ],\n       [ 0.51856662],\n       [ 0.04323806],\n       ...,\n       [-0.05468475],\n       [ 0.51707086],\n       [ 0.01787214]])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_sensorwise = MinMaxScaler(feature_range=[-1,1])\n",
    "\n",
    "scaled_sensorwise = scaler_sensorwise.fit_transform(interpolated_data[[\"acc_x\", \"acc_y\", \"acc_z\"]].values.reshape(-1,1))\n",
    "scaled_sensorwise"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "before / after\n",
    "\n",
    "Normalizing axis-wise\n",
    "\n",
    "before / after\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaler_axiswise = MinMaxScaler(feature_range=[-1,1])\n",
    "scaled_x = scaler_axiswise.fit_transform(interpolated_data[\"acc_x\"].values.reshape(-1,1))\n",
    "scaled_y = scaler_axiswise.fit_transform(interpolated_data[\"acc_y\"].values.reshape(-1,1))\n",
    "scaled_z = scaler_axiswise.fit_transform(interpolated_data[\"acc_z\"].values.reshape(-1,1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.4 Windowing\n",
    "##### 1.4.1 Jumping/Sliding Window\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, y_train = apply_sliding_window(scaled_data_X, scaled_data_Y,\n",
    "                                        sliding_window_size=25,\n",
    "                                        unit=None,\n",
    "                                        sampling_rate=50,\n",
    "                                        sliding_window_overlap=25,\n",
    "                                        )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}