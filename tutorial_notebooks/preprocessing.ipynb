{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from IPython.display import Image\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Preprocessing\n",
    "Welcome to the second part of our tutorial.\n",
    "This notebook will teach you how to preprocess a sensor based Human Activity Recognition dataset.\n",
    "\n",
    "Data preprocessing is an essential part of any Deep Learning project. In this part you will learn which steps can or should be executed on a dataset, in order to train a working classifer.\n",
    "To be able to choose the correct preprocessing steps, first we need to get to know our data. However, this topic has already been dealt with in Chapter 1.\n",
    "\n",
    "In the first part we will work on the same subset, that we already had been working with in Chapter 1.\n",
    "So let's start by reading in the dataset.\n",
    "\n",
    "**Step 1: Reading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_folder = 'data'\n",
    "dataset = '../data/rwhar_3sbjs_data.csv'\n",
    "data = pd.read_csv(dataset,\n",
    "                   names=['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Cleaning\n",
    "There can be several reasons why we need to clean up a dataset. For example, it is common that datasets has missing values.\n",
    "These values need to be interpolated. PAMAP2 is one of the datasets that is used very frequently in scientific publications, which contains missing values.\n",
    "\n",
    "An example to clean data from missing values, especially NaN-values, can be found in the file **data_precessing.preprocess.data.py**.\n",
    "\n",
    "Also it can be beneficial to clean a dataset from noisy data or from outliers.\n",
    "But be careful with cleaning the data from noise or outlier, since it only is recommendable if the noise/outlier is not from any importance for the use case of your model.\n",
    "\n",
    "##### 2.2 Sensor Orientation\n",
    "\n",
    "Whenever we are working with a multimodal dataset, which means a dataset that consists of data from different sensors,\n",
    "we need to make sure that the sensor orientation of the data matches each other.\n",
    "\n",
    "The following picture shows the different orientation of the two datasets PAMAP2 and Skoda mini checkpoint.\n",
    "It illustrates that whenever you are merging different datasets or even data from different sensors, you need be sure that\n",
    "the sensor orientations align with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": "<img src=\"../images/pamap_skoda_orientation.png\" width=\"533\" height=\"261\"/>",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"../images/pamap_skoda_orientation.png\", width=533, height=261)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.2 Resampling\n",
    "\n",
    "Resampling is necessary if we work with data from sensors that recorded with different sampling rates.\n",
    "Resampling can either be done by up- or downsample the data.\n",
    "\n",
    "An example for a function that either up- or downsamples time series data, can be found as well in our collection of preprocessing function: **data_precessing.preprocess.data.py**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.3 Normalizing\n",
    "Normalizing is in an important part in the preprocessing chain, but can also the reason for many mistakes.\n",
    "Therefore it is important to choose the correct strategy to normalize your dataset.\n",
    "Therefore, we will dig deeper into this topic at this point of our tutorial.\n",
    "\n",
    "##### 1.3.1 How to normalize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The best ways to normalize a dataset is either sensor- or axis-wise. However, there certain pitfalls a programmer can fall into.\n",
    "Normalizing sensor-wise means that whenever we have a dataset that contains data from different types of sensors, like e.g accelerometer and\n",
    "magnetometer data, these data should never have been normalized together.\n",
    "\n",
    "One of the reasons we want to normalize is, that we would like to give the same importance to every input feature.\n",
    "\n",
    "The numerical values of magnetometer measurements are much higher than accelerometer values. Since normalizing both sensor data together, will\n",
    "keep this imbalance, we can break this up by normalizing data sensor- or even axis-wise.\n",
    "\n",
    "However, data from one sensor or one axis should be kept together, and not be separated either user- or activity-wise before normalization. Otherwise,\n",
    "the data will be distorted.\n",
    "\n",
    "To illustrate the problem, let's look at the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           acc_x      acc_y     acc_z\n64990   2.234389   9.482835 -2.597709\n64991   2.196680   9.489418 -2.627636\n64992   2.173337   9.533711 -2.594118\n64993   2.239776   9.535507 -2.606089\n64994   2.275689   9.538500 -2.604293\n...          ...        ...       ...\n525338 -0.432752 -10.026917  1.263540\n525339 -0.403423 -10.029311  1.224634\n525340 -0.377685 -10.009559  1.183933\n525341 -0.377685  -9.996989  1.188721\n525342 -0.374693  -9.993398  1.193510\n\n[96810 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>acc_x</th>\n      <th>acc_y</th>\n      <th>acc_z</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>64990</th>\n      <td>2.234389</td>\n      <td>9.482835</td>\n      <td>-2.597709</td>\n    </tr>\n    <tr>\n      <th>64991</th>\n      <td>2.196680</td>\n      <td>9.489418</td>\n      <td>-2.627636</td>\n    </tr>\n    <tr>\n      <th>64992</th>\n      <td>2.173337</td>\n      <td>9.533711</td>\n      <td>-2.594118</td>\n    </tr>\n    <tr>\n      <th>64993</th>\n      <td>2.239776</td>\n      <td>9.535507</td>\n      <td>-2.606089</td>\n    </tr>\n    <tr>\n      <th>64994</th>\n      <td>2.275689</td>\n      <td>9.538500</td>\n      <td>-2.604293</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>525338</th>\n      <td>-0.432752</td>\n      <td>-10.026917</td>\n      <td>1.263540</td>\n    </tr>\n    <tr>\n      <th>525339</th>\n      <td>-0.403423</td>\n      <td>-10.029311</td>\n      <td>1.224634</td>\n    </tr>\n    <tr>\n      <th>525340</th>\n      <td>-0.377685</td>\n      <td>-10.009559</td>\n      <td>1.183933</td>\n    </tr>\n    <tr>\n      <th>525341</th>\n      <td>-0.377685</td>\n      <td>-9.996989</td>\n      <td>1.188721</td>\n    </tr>\n    <tr>\n      <th>525342</th>\n      <td>-0.374693</td>\n      <td>-9.993398</td>\n      <td>1.193510</td>\n    </tr>\n  </tbody>\n</table>\n<p>96810 rows Ã— 3 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_sensorwise = MinMaxScaler(feature_range=[-1,1])\n",
    "walking_data = data.loc[data['activity_label'] == \"walking\"]\n",
    "walking_data = walking_data[[\"acc_x\", \"acc_y\", \"acc_z\"]]\n",
    "\n",
    "#scaled_sensorwise = scaler_sensorwise.fit_transform(data[[\"acc_x\", \"acc_y\", \"acc_z\"]].values.reshape(-1,1))\n",
    "#scaled_sensorwise\n",
    "walking_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "before / after\n",
    "\n",
    "Normalizing axis-wise\n",
    "\n",
    "before / after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interpolated_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_6020/2951924301.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mscaler_axiswise\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMinMaxScaler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeature_range\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mscaled_x\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mscaler_axiswise\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit_transform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minterpolated_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"acc_x\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mscaled_y\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mscaler_axiswise\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit_transform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minterpolated_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"acc_y\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mscaled_z\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mscaler_axiswise\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit_transform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minterpolated_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"acc_z\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'interpolated_data' is not defined"
     ]
    }
   ],
   "source": [
    "scaler_axiswise = MinMaxScaler(feature_range=[-1,1])\n",
    "scaled_x = scaler_axiswise.fit_transform(interpolated_data[\"acc_x\"].values.reshape(-1,1))\n",
    "scaled_y = scaler_axiswise.fit_transform(interpolated_data[\"acc_y\"].values.reshape(-1,1))\n",
    "scaled_z = scaler_axiswise.fit_transform(interpolated_data[\"acc_z\"].values.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch-Normalization\n",
    "Where to put in the architecture?\n",
    "According to citation[] batch normalization layers should be placed after convolutional layers.\n",
    "\n",
    "\n",
    "#### 1.4 Windowing\n",
    "##### 1.4.1 Jumping/Sliding Window\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions\n",
    "\n",
    "Shuffling\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}