{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N6nfqp9WI4G"
   },
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOse0E6AWMv0"
   },
   "source": [
    "Welcome to the second part of our tutorial.\n",
    "This notebook will teach you how to preprocess a sensor based Human Activity Recognition dataset.\n",
    "\n",
    "Data preprocessing is an essential part of any Deep Learning project. In this part you \n",
    "To be able to choose the correct preprocessing steps, first we need to get to know our data. However, this topic has already been dealt with in Chapter 1.\n",
    "\n",
    "In the first part we will work on the same subset, that we already had been working with in Chapter 1.\n",
    "So let's start by reading in the dataset.\n",
    "\n",
    "Welcome to the second notebook of our six part series part of our tutorial on Deep Learning for Human Activity Recognition. Within the last notebook you learned:\n",
    "\n",
    "- How do I use Google Colab and Jupyter Notebooks? \n",
    "- How do I load a dataset using pandas?\n",
    "- How do I analyze the labeling? How do I plot sample activity data?\n",
    "- What are sample, more detailled analysis that one can apply on a HAR dataset?\n",
    "\n",
    "This notebook will teach you everything you need to know about preprocessing. Sensor datasets in their raw form are (usually) very messy. This notebook will teach you which preprocessing steps can or should be executed on a dataset, in order to train a working classifer, i.e. our neural network architecture, which we will define in later notebooks. \n",
    "\n",
    "After completing this notebook you will be answer the following questions:\n",
    "- What data cleaning steps usually need to be performed on a raw sensor dataset?\n",
    "- How and why do we perform normalization?\n",
    "- What is a sliding window? How do we apply it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHd4_XMZV9uU"
   },
   "source": [
    "## 2.1. Important Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0U4jPu57dKh"
   },
   "source": [
    "If you are accessing this tutorial via [Google Colab](https://colab.research.google.com/github/mariusbock/dl-for-har/blob/main/tutorial_notebooks/training.ipynb), first make sure to use Google Colab in English. This will help us to better assist you with issues that might arise during the tutorial. There are two ways to change the default language if it isn't English already:\n",
    "1. On Google Colab, go to `Help` -> `View in English` \n",
    "2. Change the default language of your browser to `English`.\n",
    "\n",
    "To also ease the communication when communicating errors, enable line numbers within the settings of Colab.\n",
    "\n",
    "1. On Google Colab, go to `Tools` -> `Settings` -> `Editor` -> `Show line numbers`\n",
    "\n",
    "In general, we strongly advise you to use Google Colab as it provides you with a working Python distribution as well as free GPU resources. To make Colab use GPUs, you need to change the current notebooks runtime type via:\n",
    "\n",
    "- `Runtime` -> `Change runtime type` -> `Dropdown` -> `GPU` -> `Save`\n",
    "\n",
    "**Hint:** you can auto-complete code in Colab via `ctrl` + `spacebar`\n",
    "\n",
    "For the live tutorial, we require all participants to use Colab. If you decide to rerun the tutorial at later points and rather want to have it run locally on your machine, feel free to clone our [GitHub repository](https://github.com/mariusbock/dl-for-har).\n",
    "\n",
    "To get started with this notebook, you need to first run the code cell below. Please set `use_colab` to be `True` if you are accessing this notebook via Colab. If not, please set it to `False`. This code cell will make sure that imports from our GitHub repository will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "q7TrTsQ07dKi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content/'\n",
      "/Users/ahoelzemann/Documents/git/dl-for-har/tutorial_notebooks/dl-for-har/dl-for-har\n",
      "Cloning into 'dl-for-har'...\r\n",
      "remote: Enumerating objects: 1109, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (1109/1109), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (756/756), done.\u001B[K\r\n",
      "remote: Total 1109 (delta 563), reused 862 (delta 343), pack-reused 0\u001B[K\r\n",
      "Receiving objects: 100% (1109/1109), 34.65 MiB | 16.53 MiB/s, done.\r\n",
      "Resolving deltas: 100% (563/563), done.\r\n",
      "/Users/ahoelzemann/Documents/git/dl-for-har/tutorial_notebooks/dl-for-har/dl-for-har/dl-for-har\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "use_colab = True\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if use_colab:\n",
    "    # move to content directory and remove directory for a clean start \n",
    "    %cd /content/         \n",
    "    %rm -rf dl-for-har\n",
    "    # clone package repository (will throw error if already cloned)\n",
    "    !git clone https://github.com/mariusbock/dl-for-har.git\n",
    "    # navigate to dl-for-har directory\n",
    "    %cd dl-for-har/       \n",
    "else:\n",
    "    os.chdir(module_path)\n",
    "    \n",
    "# this statement is needed so that we can use the methods of the DL-ARC pipeline\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t99_nGNF7dKi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.2. Reading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5WDJwC3sDCm"
   },
   "source": [
    "Before getting into the actual content of this notebook, we need to load the data again. Instead of using the same way as previously and loading the dataset we will use a predefined method of the DL-ARC feature stack called `load_dataset()`. Since the method returns [numpy](https://numpy.org/) arrays we also need to adjust our workflow from now on to index arrays according to [numpy](https://numpy.org/) syntax. If you want to familiarise yourself how to index check out this [webpage](https://numpy.org/doc/stable/reference/arrays.indexing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UylUlb4oudF9"
   },
   "source": [
    "### Task 1: Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6ivdlKUunCC"
   },
   "source": [
    "1. Load the `rwhar_3sbjs` data using the load_dataset function. The function is already imported for you. (`lines 8-9`)\n",
    "2. The method returns additional attributes. Have a look at them. You can also print them to see what values they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9HrDYNAA7dKj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3060183336.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"/var/folders/5h/2n61808s4sbcx1xmqcpbrdl80000gn/T/ipykernel_93834/3060183336.py\"\u001B[0;36m, line \u001B[0;32m9\u001B[0m\n\u001B[0;31m    X, y, num_classes, class_names, sampling_rate, has_null =\u001B[0m\n\u001B[0m                                                             ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from data_processing.preprocess_data import load_dataset\n",
    "\n",
    "\n",
    "# load the dataset using the load_dataset() function; pass the method the name of the dataset as a string\n",
    "X, y, num_classes, class_names, sampling_rate, has_null =\n",
    "\n",
    "# since the method returns features and labels separately, we need to concat them\n",
    "# since y is \n",
    "data = np.concatenate((X, y[:, None]), axis=1)\n",
    "\n",
    "print('\\nShape of the dataset:')\n",
    "print(data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qY_A3yAQy7zg"
   },
   "source": [
    "## 2.3. Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpR7wY2i7dKj"
   },
   "source": [
    "There can be several reasons why we need to clean up a dataset. For example, it is common that datasets has missing values.\n",
    "These values need to be interpolated. PAMAP2 is one of the datasets that is used very frequently in scientific publications, which contains missing values.\n",
    "\n",
    "An example to clean data from missing values, especially NaN-values, can be found in the file **data_precessing.preprocess.data.py**.\n",
    "\n",
    "Also it can be beneficial to clean a dataset from noisy data or from outliers.\n",
    "But be careful with cleaning the data from noise or outlier, since it only is recommendable if the noise/outlier is not from any importance for the use case of your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "from data_processing.preprocess_data import replaceNaNValues\n",
    "data_with_nan = data.copy()\n",
    "\n",
    "for i in range(0, 10):\n",
    "    fill_index = random.randint(1, 20)\n",
    "    data_with_nan[fill_index] = [np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "\n",
    "print(data_with_nan[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "participant_interpolated = replaceNaNValues(data_with_nan[:,0], 'int')\n",
    "acc_x_interpolated = replaceNaNValues(data_with_nan[:,1])\n",
    "acc_y_interpolated = replaceNaNValues(data_with_nan[:,2])\n",
    "acc_z_interpolated = replaceNaNValues(data_with_nan[:,3])\n",
    "label_interpolated = replaceNaNValues(data_with_nan[:,4], 'int')\n",
    "\n",
    "data_interpolated = np.array([participant_interpolated, acc_x_interpolated, acc_y_interpolated, acc_z_interpolated, label_interpolated]).T\n",
    "print(data_interpolated[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4. Resampling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qs10j5tc7dKl"
   },
   "source": [
    "Resampling is necessary if we work with data from sensors that recorded with different sampling rates.\n",
    "Resampling can either be done by up- or downsample the data.\n",
    "\n",
    "An example for a function that either up- or downsamples time series data, can be found as well in our collection of preprocessing functions: **data_precessing.preprocess.data.py**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBZBJVHCzgQY"
   },
   "source": [
    "## 2.5. Normalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqQ5H34I7dKl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Normalizing is in an important part in the preprocessing chain, but can also the reason for many mistakes.\n",
    "Therefore it is important to choose the correct strategy to normalize your dataset.\n",
    "Therefore, we will dig deeper into this topic at this point of our tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0osXSLSzjzU"
   },
   "source": [
    "### 2.5.1 How to normalize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEbh2tRg7dKl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The best ways to normalize a dataset is either sensor- or axis-wise. However, there certain pitfalls a programmer can fall into.\n",
    "Normalizing sensor-wise means that whenever we have a dataset that contains data from different types of sensors, like e.g accelerometer and\n",
    "magnetometer data, these data should never have been normalized together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU5ASztmBJCK"
   },
   "source": [
    "![](https://github.com/mariusbock/dl-for-har/blob/main/images/pamap2_values_example.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8fnuz5N7dKm"
   },
   "source": [
    "This image shows how our datasets are often organized. We have a 2D matrix that contains the data from different sensor axes and sensor types.\n",
    "Whenever you start working with deep learning, you normally start with a tutorial that works with image data. For image data it is fine to normalize your\n",
    "data all at once, since all input data share the same boundaries (0-255 when working with RGB).\n",
    "However, when it comes to sensor data this rule doesn't apply. Accelerometer, Gyroscopes, Magnetometer or any other sensor do not share\n",
    "the same boundaries. Furthermore, the boundaries also depend on the sensitivity used while recording the data.\n",
    "\n",
    "So be sure to organize your data correctly before you start normalizing them.\n",
    "\n",
    "Another reason why we want to normalize is, that we would like to give the same importance to every input feature.\n",
    "\n",
    "The numerical values of magnetometer measurements are much higher than accelerometer values. Since normalizing both sensor data together, will\n",
    "keep this imbalance, we can break this up by normalizing data sensor- or even axis-wise.\n",
    "\n",
    "However, data from one sensor or one axis should be kept together, and not be separated either user- or activity-wise before normalization. Otherwise,\n",
    "the data will be distorted.\n",
    "\n",
    "To illustrate the problem, let's look at the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgJks0Yv7dKm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from data_processing.plotting import plot_imu_data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=[-1,1])\n",
    "data_activity_wise = {}\n",
    "data_activity_wise_scaled = []\n",
    "all_activites = pd.unique(pd.Series(data[:, -1]))\n",
    "\n",
    "for activity in all_activites:\n",
    "    data_activity_wise[activity] = data[data[:, -1] == activity]\n",
    "\n",
    "for activity, activity_data in data_activity_wise.items():\n",
    "    data_activity_wise_scaled.append(scaler.fit_transform(activity_data[:, 1:4]))\n",
    "\n",
    "scaled_data = scaler.fit_transform(data[:, 1:4])\n",
    "data_scaled_at_once = np.concatenate((scaled_data, data[:, -1][:, None]), axis=1)[:32800]\n",
    "data_activity_wise = np.concatenate(data_activity_wise_scaled)[:32800]\n",
    "\n",
    "plot_imu_data(data[:, 1:4][:32800], data_scaled_at_once[:, 0:3], data_activity_wise, \"Scaling Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mo06fmCUzzoL"
   },
   "source": [
    "## 2.6. Jumping/Sliding Window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s14WcR-17dKm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In order to be able to feed our neural network with data, we need to organize it into smaller chunks.\n",
    "Something that is more similar to the size of an image.\n",
    "Therefore we apply a jumping/sliding window alogorithm with which we are able to split our time series data into chunks that our input layer can work with.\n",
    "\n",
    "As already described in the slides, the algorithm has a parameter that describes how much of the data of each window should overlap with the window before. This is often\n",
    "useful when we want to be sure that no transition from one activity to another is lost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vm6YaKV8z3B6"
   },
   "source": [
    "### Task 2: Applying different sliding windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70aa9jhz0Cjo"
   },
   "source": [
    "1. The RWHAR dataset has a sampling rate of 50 Hz. Using the function below, apply a sliding window on top of RWHAR dataset whose windows are 2 seconds long. Set the overlap ratio to be 0%. What are the dimensions of the resulting dataset? (`lines 34-38`)\n",
    "2. Change the overlap ratio of the slding window to 25%. What differences can you see and what do you think you need to be aware of when using an `overlap_ratio`? (`lines 40-43`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdc0r_377dKn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sliding_window_samples(data, samples_per_window, overlap_ratio):\n",
    "    \"\"\"\n",
    "    Return a sliding window measured in number of samples over a data array.\n",
    "\n",
    "    :param data: input array, can be numpy or pandas dataframe\n",
    "    :param samples_per_window: window length as number of samples\n",
    "    :param overlap_ratio: overlap is meant as percentage and should be an integer value\n",
    "    :return: tuple of windows and indices\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    indices = []\n",
    "    curr = 0\n",
    "    win_len = int(samples_per_window)\n",
    "    if overlap_ratio is not None:\n",
    "        overlapping_elements = int((overlap_ratio / 100) * (win_len))\n",
    "        if overlapping_elements >= win_len:\n",
    "            print('Number of overlapping elements exceeds window size.')\n",
    "            return\n",
    "    while curr < len(data) - win_len:\n",
    "        windows.append(data[curr:curr + win_len])\n",
    "        indices.append([curr, curr + win_len])\n",
    "        curr = curr + win_len - overlapping_elements\n",
    "    try:\n",
    "        result_windows = np.array(windows)\n",
    "        result_indices = np.array(indices)\n",
    "    except:\n",
    "        result_windows = np.empty(shape=(len(windows), win_len, data.shape[1]), dtype=object)\n",
    "        result_indices = np.array(indices)\n",
    "        for i in range(0, len(windows)):\n",
    "            result_windows[i] = windows[i]\n",
    "            result_indices[i] = indices[i]\n",
    "    return result_windows, result_indices\n",
    "\n",
    "# apply the sliding_window_samples() function on top of the data\n",
    "# samples_per_window shall be equivalent to two seconds; overlap_ratio shall be 0%\n",
    "print(\"Shape of the windowed dataset (2 seconds with 0% overlap):\")\n",
    "windowed_data, _ = sliding_window_samples(data, 100, 0)\n",
    "print(windowed_data.shape)\n",
    "\n",
    "# change the overlap_ratio shall to be 25%; What do you need to be aware of?\n",
    "windowed_data, _ =\n",
    "print(\"\\nShape of the windowed dataset (2 seconds with 25% overlap):\")\n",
    "print(windowed_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6N046mZkE0O3"
   },
   "source": [
    "For evalution purposes the data will be organized in Leave One Out Folds. This is a critical step, since we have data in each window that overlaps with data in another window\n",
    "it can happen that we test our classifier on data the has been fed while training."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}