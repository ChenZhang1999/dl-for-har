{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from IPython.display import Image\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Preprocessing\n",
    "Welcome to the second part of our tutorial.\n",
    "This notebook will teach you how to preprocess a sensor based Human Activity Recognition dataset.\n",
    "\n",
    "Data preprocessing is an essential part of any Deep Learning project. In this part you will learn which steps can or should be executed on a dataset, in order to train a working classifer.\n",
    "To be able to choose the correct preprocessing steps, first we need to get to know our data. However, this topic has already been dealt with in Chapter 1.\n",
    "\n",
    "In this Chapter we will distinguish between a single dataset and multimodal datasets.\n",
    "\n",
    "In the first part we will work on the same subset, that we already had been working with in Chapter 1.\n",
    "\n",
    "**Step 1: das**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Whenever we work with more than one dataset or with data from the same dataset but recorded with different sensor modalities, we need to somehow bring this data into a homgeneous format.\n",
    "\n",
    "data_folder = 'data'\n",
    "dataset = '/Users/ahoelzemann/Documents/git/dl-for-har/data/rwhar_3sbjs_data.csv'\n",
    "data = pd.read_csv(os.path.join(data_folder, dataset),\n",
    "                   names=['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Cleaning\n",
    "\n",
    "##### 1.1.1 Sensor Orientation\n",
    "\n",
    "Whenever we are working with a multimodal dataset, which means a dataset that consists of data from different sensors,\n",
    "we need to make sure that the sensor orientation of the data matches each other.\n",
    "\n",
    "\n",
    "\n",
    "Depending on the circumstances we want to clean the data before we train our classifier.\n",
    "\n",
    "It is very important to double check if the dataset contains **NaN - Values**. If the dataset contains these values\n",
    "make sure, that missing values are interpolated, since we want to keep the original sampling rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": "<img src=\"../images/pamap_skoda_orientation.png\"/>",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"../images/pamap_skoda_orientation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subject_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/5h/2n61808s4sbcx1xmqcpbrdl80000gn/T/ipykernel_84066/3171594612.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0minterpolated_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0msubject_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0macc_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0macc_y\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0macc_z\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mactivity_label\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0minterpolated_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'subject_id'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'acc_x'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'acc_y'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'acc_z'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'activity_label'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'subject_id' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "interpolated_data = pd.concat([subject_id, acc_x, acc_y, acc_z, activity_label], axis=1)\n",
    "interpolated_data.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Be careful with cleaning the data from noise or outlier, since it only is recommandable if the noise/outlier is not from any importance for the use case of our model.\n",
    "#### 1.2 Resampling\n",
    "\n",
    "Resampling is necessary if we work with sensor data from different sensors, that were not recorded with the sampling rate.\n",
    "The optimize the classifier we need to align the data sampling rates with each other.\n",
    "Resampling can either be done by up- or downsample the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.3 Normalizing\n",
    "Normalizing is in an important part in the preprocessing chain, but can also the reason for many mistakes.\n",
    "Therefore it is important to choose the correct strategy for normalizing your dataset.\n",
    "\n",
    "##### 1.3.1 How to normalize?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3 possible solutions to normalize correctly.\n",
    "Big pitfalls, since beginners tend to normalize the whole vector at once.\n",
    "\n",
    "Normalizing sensor-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scaler_sensorwise = MinMaxScaler(feature_range=[-1,1])\n",
    "\n",
    "scaled_sensorwise = scaler_sensorwise.fit_transform(interpolated_data[[\"acc_x\", \"acc_y\", \"acc_z\"]].values.reshape(-1,1))\n",
    "scaled_sensorwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "before / after\n",
    "\n",
    "Normalizing axis-wise\n",
    "\n",
    "before / after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scaler_axiswise = MinMaxScaler(feature_range=[-1,1])\n",
    "scaled_x = scaler_axiswise.fit_transform(interpolated_data[\"acc_x\"].values.reshape(-1,1))\n",
    "scaled_y = scaler_axiswise.fit_transform(interpolated_data[\"acc_y\"].values.reshape(-1,1))\n",
    "scaled_z = scaler_axiswise.fit_transform(interpolated_data[\"acc_z\"].values.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch-Normalization\n",
    "Where to put in the architecture?\n",
    "According to citation[] batch normalization layers should be placed after convolutional layers.\n",
    "\n",
    "\n",
    "#### 1.4 Windowing\n",
    "##### 1.4.1 Jumping/Sliding Window\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions\n",
    "\n",
    "Shuffling\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (dl-for-har)",
   "language": "python",
   "name": "pycharm-1f473f48"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}