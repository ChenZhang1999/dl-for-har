{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N6nfqp9WI4G"
   },
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOse0E6AWMv0"
   },
   "source": [
    "Welcome to the second part of our tutorial.\n",
    "This notebook will teach you how to preprocess a sensor based Human Activity Recognition dataset.\n",
    "\n",
    "Data preprocessing is an essential part of any Deep Learning project. In this part you \n",
    "To be able to choose the correct preprocessing steps, first we need to get to know our data. However, this topic has already been dealt with in Chapter 1.\n",
    "\n",
    "In the first part we will work on the same subset, that we already had been working with in Chapter 1.\n",
    "So let's start by reading in the dataset.\n",
    "\n",
    "Welcome to the second notebook of our six part series part of our tutorial on Deep Learning for Human Activity Recognition. Within the last notebook you learned:\n",
    "\n",
    "- How do I use Google Colab and Jupyter Notebooks? \n",
    "- How do I load a dataset using pandas?\n",
    "- How do I analyze the labeling? How do I plot sample activity data?\n",
    "- What are sample, more detailled analysis that one can apply on a HAR dataset?\n",
    "\n",
    "This notebook will teach you everything you need to know about preprocessing. Sensor datasets in their raw form are (usually) very messy. This notebook will teach you which preprocessing steps can or should be executed on a dataset, in order to train a working classifer, i.e. our neural network architecture, which we will define in later notebooks. \n",
    "\n",
    "After completing this notebook you will be answer the following questions:\n",
    "- What data cleaning steps usually need to be performed on a raw sensor dataset?\n",
    "- How and why do we perform scaling/normalization?\n",
    "- What is a sliding window? How do we apply it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHd4_XMZV9uU"
   },
   "source": [
    "## 2.1. Important Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0U4jPu57dKh"
   },
   "source": [
    "If you are accessing this tutorial via [Google Colab](https://colab.research.google.com/github/mariusbock/dl-for-har/blob/main/tutorial_notebooks/training.ipynb), first make sure to use Google Colab in English. This will help us to better assist you with issues that might arise during the tutorial. There are two ways to change the default language if it isn't English already:\n",
    "1. On Google Colab, go to `Help` -> `View in English`\n",
    "2. Change the default language of your browser to `English`.\n",
    "\n",
    "To also ease the communication when communicating errors, enable line numbers within the settings of Colab.\n",
    "\n",
    "1. On Google Colab, go to `Tools` -> `Settings` -> `Editor` -> `Show line numbers`\n",
    "\n",
    "In general, we strongly advise you to use Google Colab as it provides you with a working Python distribution as well as free GPU resources. To make Colab use GPUs, you need to change the current notebooks runtime type via:\n",
    "\n",
    "- `Runtime` -> `Change runtime type` -> `Dropdown` -> `GPU` -> `Save`\n",
    "\n",
    "**Hint:** you can auto-complete code in Colab via `ctrl` + `spacebar`\n",
    "\n",
    "For the live tutorial, we require all participants to use Colab. If you decide to rerun the tutorial at later points and rather want to have it run locally on your machine, feel free to clone our [GitHub repository](https://github.com/mariusbock/dl-for-har).\n",
    "\n",
    "To get started with this notebook, you need to first run the code cell below. Please set `use_colab` to be `True` if you are accessing this notebook via Colab. If not, please set it to `False`. This code cell will make sure that imports from our GitHub repository will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "q7TrTsQ07dKi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content/'\n",
      "/Users/ahoelzemann/Documents/git/dl-for-har/tutorial_notebooks/solutions/dl-for-har\n",
      "Cloning into 'dl-for-har'...\r\n",
      "remote: Enumerating objects: 1123, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (1123/1123), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (765/765), done.\u001B[K\r\n",
      "remote: Total 1123 (delta 574), reused 868 (delta 348), pack-reused 0\u001B[K\r\n",
      "Receiving objects: 100% (1123/1123), 34.66 MiB | 11.99 MiB/s, done.\r\n",
      "Resolving deltas: 100% (574/574), done.\r\n",
      "Updating files: 100% (263/263), done.\r\n",
      "/Users/ahoelzemann/Documents/git/dl-for-har/tutorial_notebooks/solutions/dl-for-har/dl-for-har\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "use_colab = True\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if use_colab:\n",
    "    # move to content directory and remove directory for a clean start\n",
    "    %cd /content/\n",
    "    %rm -rf dl-for-har\n",
    "    # clone package repository (will throw error if already cloned)\n",
    "    !git clone https://github.com/mariusbock/dl-for-har.git\n",
    "    # navigate to dl-for-har directory\n",
    "    %cd dl-for-har/\n",
    "else:\n",
    "    os.chdir(module_path)\n",
    "\n",
    "# this statement is needed so that we can use the methods of the DL-ARC pipeline\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t99_nGNF7dKi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.2. Reading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5WDJwC3sDCm"
   },
   "source": [
    "Before getting into the actual content of this notebook, we need to load the data again. Instead of using the same way as previously and loading the dataset we will use a predefined method of the DL-ARC feature stack called `load_dataset()`. Since the method returns [numpy](https://numpy.org/) arrays we also need to adjust our workflow from now on to index arrays according to [numpy](https://numpy.org/) syntax. If you want to familiarise yourself how to index check out this [webpage](https://numpy.org/doc/stable/reference/arrays.indexing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UylUlb4oudF9"
   },
   "source": [
    "### Task 1: Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6ivdlKUunCC"
   },
   "source": [
    "1. Load the `rwhar_3sbjs` data using the load_dataset function. The function is already imported for you. (`lines 8-9`)\n",
    "2. The method returns additional attributes. Have a look at them. You can also print them to see what values they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9HrDYNAA7dKj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/rwhar_1sbjs_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/5h/2n61808s4sbcx1xmqcpbrdl80000gn/T/ipykernel_3133/1910644760.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;31m# load the dataset using the load_dataset() function; pass the method the name of the dataset as a string\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_classes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_names\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msampling_rate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhas_null\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_dataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'rwhar_1sbjs'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;31m# since the method returns features and labels separately, we need to concat them\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/dl-for-har/data_processing/preprocess_data.py\u001B[0m in \u001B[0;36mload_dataset\u001B[0;34m(dataset, pred_type, include_null)\u001B[0m\n\u001B[1;32m     44\u001B[0m             \u001B[0mclass_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'stand'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'walk'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'sit'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'lie'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 46\u001B[0;31m     \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'data/'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'_data.csv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msep\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m','\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex_col\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     47\u001B[0m     \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpreprocess_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpred_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhas_null\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minclude_null\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/dl-for-har/venv/venv/lib/python3.9/site-packages/pandas/util/_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m                 )\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/dl-for-har/venv/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    584\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    585\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 586\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    587\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    588\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/dl-for-har/venv/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    480\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    481\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 482\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    483\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    484\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/dl-for-har/venv/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    809\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    810\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 811\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    812\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    813\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/dl-for-har/venv/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1038\u001B[0m             )\n\u001B[1;32m   1039\u001B[0m         \u001B[0;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1040\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1041\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1042\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/dl-for-har/venv/venv/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m         \u001B[0;31m# open handles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/dl-for-har/venv/venv/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001B[0m in \u001B[0;36m_open_handles\u001B[0;34m(self, src, kwds)\u001B[0m\n\u001B[1;32m    220\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHandles\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    221\u001B[0m         \"\"\"\n\u001B[0;32m--> 222\u001B[0;31m         self.handles = get_handle(\n\u001B[0m\u001B[1;32m    223\u001B[0m             \u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    224\u001B[0m             \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/dl-for-har/venv/venv/lib/python3.9/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    700\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;34m\"b\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    701\u001B[0m             \u001B[0;31m# Encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 702\u001B[0;31m             handle = open(\n\u001B[0m\u001B[1;32m    703\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    704\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/rwhar_1sbjs_data.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from data_processing.preprocess_data import load_dataset\n",
    "\n",
    "\n",
    "# load the dataset using the load_dataset() function; pass the method the name of the dataset as a string\n",
    "X, y, num_classes, class_names, sampling_rate, has_null = load_dataset('rwhar_3sbjs')\n",
    "\n",
    "# since the method returns features and labels separately, we need to concat them\n",
    "# since y is\n",
    "data = np.concatenate((X, y[:, None]), axis=1)\n",
    "\n",
    "print('\\nShape of the dataset:')\n",
    "print(data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qY_A3yAQy7zg"
   },
   "source": [
    "## 2.3. Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpR7wY2i7dKj"
   },
   "source": [
    "There can be several reasons why we need to clean up a dataset. For example, it is common that datasets has missing values.\n",
    "These values need to be interpolated. PAMAP2 is one of the datasets that is used very frequently in scientific publications, which contains missing values.\n",
    "\n",
    "An example to clean data from missing values, especially NaN-values, can be found in the file **data_precessing.preprocess.data.py**.\n",
    "\n",
    "Also, it can be beneficial to clean a dataset from noisy data or from outliers.\n",
    "But be careful with cleaning the data from noise or outlier, since it only is recommendable if the noise/outlier is not from any importance for the use case of your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "from data_processing.preprocess_data import replaceNaNValues\n",
    "data_with_nan = data.copy()\n",
    "\n",
    "for i in range(0, 10):\n",
    "    fill_index = random.randint(1, 20)\n",
    "    data_with_nan[fill_index] = [np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "\n",
    "print(data_with_nan[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "participant_interpolated = replaceNaNValues(data_with_nan[:,0], 'int')\n",
    "acc_x_interpolated = replaceNaNValues(data_with_nan[:,1])\n",
    "acc_y_interpolated = replaceNaNValues(data_with_nan[:,2])\n",
    "acc_z_interpolated = replaceNaNValues(data_with_nan[:,3])\n",
    "label_interpolated = replaceNaNValues(data_with_nan[:,4], 'int')\n",
    "\n",
    "data_interpolated = np.array([participant_interpolated, acc_x_interpolated, acc_y_interpolated, acc_z_interpolated, label_interpolated]).T\n",
    "print(data_interpolated[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4. Resampling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qs10j5tc7dKl"
   },
   "source": [
    "Resampling is necessary if we work with data from sensors that did record with different sampling rates.\n",
    "It can either be done by up- or downsample the data.\n",
    "\n",
    "An example for a function that either up- or downsamples time series data, can be found as well in our collection of preprocessing functions: **data_precessing.preprocess.data.py**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBZBJVHCzgQY"
   },
   "source": [
    "## 2.5. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqQ5H34I7dKl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Scaling is in an important part in the preprocessing chain, but can also the reason for many mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0osXSLSzjzU"
   },
   "source": [
    "### 2.5.1 How to (re)scale?\n",
    "\n",
    "We should have a look on our dataset before we apply rescaling.\n",
    "\n",
    "However, we will first reduce our dataset to only one subject."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EEbh2tRg7dKl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#Please reduce your dataset to only one subject and plot the data by executing this code cell.\n",
    "from data_processing.plotting import plot_data\n",
    "\n",
    "data = data[data[:, 0] == 0]\n",
    "plot_data(data[:,1:4], 'Original Dataset')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU5ASztmBJCK"
   },
   "source": [
    "This plot shows our unpreprocessed 3D accelerometer data.\n",
    "\n",
    "Depending on how we organize our dataset before applying rescaling, the outcome differs a lot.\n",
    "To illustrate this problem, we prepared the following example.\n",
    "In this example we separated our data into one array for every type of activity before we applied rescaling."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f8fnuz5N7dKm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=[-1,1])\n",
    "data_activity_wise = {}\n",
    "data_activity_wise_scaled = []\n",
    "all_activites = pd.unique(pd.Series(data[:, -1]))\n",
    "\n",
    "for activity in all_activites:\n",
    "    data_activity_wise[activity] = data[data[:, -1] == activity]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgJks0Yv7dKm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mo06fmCUzzoL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "for activity, activity_data in data_activity_wise.items():\n",
    "    data_activity_wise_scaled.append(scaler.fit_transform(activity_data[:, 1:4]))\n",
    "\n",
    "scaled_data = scaler.fit_transform(data[:, 1:4])\n",
    "data_scaled_at_once = np.concatenate((scaled_data, data[:, -1][:, None]), axis=1)\n",
    "data_activity_wise = np.concatenate(data_activity_wise_scaled)\n",
    "\n",
    "plot_data(data_scaled_at_once[:, 0:3], \"Scaled at once\")\n",
    "plot_data(data_activity_wise, 'Activity-Wise Scaled')\n",
    "\n",
    "print(\"Scaled at once:\")\n",
    "print(\"Mean(x): \" + str(np.mean(data_scaled_at_once[:, 1])) + \"; Std(x): \" + str(np.std(data_scaled_at_once[:, 1])))\n",
    "print(\"Mean(y): \" + str(np.mean(data_scaled_at_once[:, 2])) + \"; Std(y): \" + str(np.std(data_scaled_at_once[:, 2])))\n",
    "print(\"Mean(z): \" + str(np.mean(data_scaled_at_once[:, 3])) + \"; Std(z): \" + str(np.std(data_scaled_at_once[:, 3])))\n",
    "\n",
    "print(\"\\nScaled activity-wise:\")\n",
    "print(\"Mean(x): \" + str(np.mean(data_activity_wise[:, 0])) + \"; Std(x): \" + str(np.std(data_activity_wise[:, 0])))\n",
    "print(\"Mean(y): \" + str(np.mean(data_activity_wise[:, 1])) + \"; Std(y): \" + str(np.std(data_activity_wise[:, 1])))\n",
    "print(\"Mean(z): \" + str(np.mean(data_activity_wise[:, 2])) + \"; Std(z): \" + str(np.std(data_activity_wise[:, 2])))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s14WcR-17dKm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This was kind of an \"unrealistic\" toy example and probably not so many programmers make this mistake.\n",
    "However, this rule also applies when our dataset consists of different inertial sensors.\n",
    "\n",
    "The following image shows how our datasets are often organized. We have a 2D matrix that contains the data from different sensor axes and inertial sensors.\n",
    "However, accelerometer, gyroscopes, magnetometer or any other sensor do not share\n",
    "the same boundaries. Furthermore, the boundaries also depend on the sensitivity used while recording the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vm6YaKV8z3B6"
   },
   "source": [
    "![](https://github.com/mariusbock/dl-for-har/blob/main/images/pamap2_values_example.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70aa9jhz0Cjo"
   },
   "source": [
    "The numerical values of the magnetometer are much higher than accelerometer and gyroscope values. If we train our network with this data,\n",
    "the magnetometer data will have a much higher importance than the accelerometer and gyroscope.\n",
    "\n",
    "Unfortunately, scaling all sensor data together, will keep this imbalance, but we can break this up by rescaling data sensor wise.\n",
    "## 2.6. Jumping/Sliding Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdc0r_377dKn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In order to be able to feed our neural network with data, we need to organize it into smaller windows.\n",
    "Therefore, we apply a jumping/sliding window algorithm with which we are able to split our time series data into chunks that our input layer can work with.\n",
    "\n",
    "As already described in the slides, the algorithm has the parameter overlap_ratio. This parameter describes how much of the data of each window should overlap with the window before.\n",
    "\n",
    "\"It is generally assumed that due to the higher number of data points, overlapping sliding windows increase the performance of HAR classifiers compared to non-overlapping ones [2], and they are not prone to missing important events [3], particularly within activity transition periods.\" [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6N046mZkE0O3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2: Applying different sliding windows"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. The RWHAR dataset has a sampling rate of 50 Hz. Using the function below, apply a sliding window on top of RWHAR dataset whose windows are 2 seconds long. Set the overlap ratio to be 0%. What are the dimensions of the resulting dataset? (`lines 34-38`)\n",
    "2. Change the overlap ratio of the slding window to 25%. What differences can you see and what do you think you need to be aware of when using an `overlap_ratio`? (`lines 40-43`)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sliding_window_samples(data, samples_per_window, overlap_ratio):\n",
    "    \"\"\"\n",
    "    Return a sliding window measured in number of samples over a data array.\n",
    "\n",
    "    :param data: input array, can be numpy or pandas dataframe\n",
    "    :param samples_per_window: window length as number of samples\n",
    "    :param overlap_ratio: overlap is meant as percentage and should be an integer value\n",
    "    :return: tuple of windows and indices\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    indices = []\n",
    "    curr = 0\n",
    "    win_len = int(samples_per_window)\n",
    "    if overlap_ratio is not None:\n",
    "        overlapping_elements = int((overlap_ratio / 100) * (win_len))\n",
    "        if overlapping_elements >= win_len:\n",
    "            print('Number of overlapping elements exceeds window size.')\n",
    "            return\n",
    "    while curr < len(data) - win_len:\n",
    "        windows.append(data[curr:curr + win_len])\n",
    "        indices.append([curr, curr + win_len])\n",
    "        curr = curr + win_len - overlapping_elements\n",
    "    try:\n",
    "        result_windows = np.array(windows)\n",
    "        result_indices = np.array(indices)\n",
    "    except:\n",
    "        result_windows = np.empty(shape=(len(windows), win_len, data.shape[1]), dtype=object)\n",
    "        result_indices = np.array(indices)\n",
    "        for i in range(0, len(windows)):\n",
    "            result_windows[i] = windows[i]\n",
    "            result_indices[i] = indices[i]\n",
    "    return result_windows, result_indices\n",
    "\n",
    "# apply the sliding_window_samples() function on top of the data\n",
    "# samples_per_window shall be equivalent to two seconds; overlap_ratio shall be 0%\n",
    "print(\"Shape of the windowed dataset (2 seconds with 0% overlap):\")\n",
    "windowed_data, _ = sliding_window_samples(data, 100, 0)\n",
    "print(windowed_data.shape)\n",
    "\n",
    "# change the overlap_ratio shall to be 25%; What do you need to be aware of?\n",
    "windowed_data, _ = sliding_window_samples(data, 100, 25)\n",
    "print(\"\\nShape of the windowed dataset (2 seconds with 25% overlap):\")\n",
    "print(windowed_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References:\n",
    "[1] Dehghani, Akbar, et al. \"A quantitative comparison of overlapping and non-overlapping sliding windows for human activity recognition using inertial sensors.\" Sensors 19.22 (2019): 5026.\n",
    "\n",
    "[2] Janidarmian, Majid, Katarzyna Radecka, and Zeljko Zilic. \"Automated diagnosis of knee pathology using sensory data.\" 2014 4th international conference on wireless mobile communication and healthcare-transforming healthcare through innovations in mobile and wireless technologies (mobihealth). IEEE, 2014.\n",
    "\n",
    "[3] Coggeshall, Stephen, and Guowei Wu. \"Asset allocation and long-term returns: An empirical approach.\" Available at SSRN 873184 (2005)."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}