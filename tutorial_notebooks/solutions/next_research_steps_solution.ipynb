{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "next_research_steps_solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCl0_WQzkm89"
      },
      "source": [
        "# 6. Recent Advances in Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAMW_jXBYC5N"
      },
      "source": [
        "## 6.1. Important Remarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_jHD4jxksEJ"
      },
      "source": [
        "If you are accessing this tutorial via [Google Colab](https://colab.research.google.com/github/mariusbock/dl-for-har/blob/main/tutorial_notebooks/training.ipynb), first make sure to use Google Colab in English. This will help us to better assist you with issues that might arise during the tutorial. There are two ways to change the default language if it isn't English already:\n",
        "1. On Google Colab, go to `Help` -> `View in English` \n",
        "2. Change the default language of your browser to `English`.\n",
        "\n",
        "To also ease the communication when communicating errors, enable line numbers within the settings of Colab.\n",
        "\n",
        "1. On Google Colab, go to `Tools` -> `Settings` -> `Editor` -> `Show line numbers`\n",
        "\n",
        "In general, we strongly advise you to use Google Colab as it provides you with a working Python distribution as well as free GPU resources. To make Colab use GPUs, you need to change the current notebooks runtime type via:\n",
        "\n",
        "- `Runtime` -> `Change runtime type` -> `Dropdown` -> `GPU` -> `Save`\n",
        "\n",
        "**Hint:** you can auto-complete code in Colab via `ctrl` + `spacebar`\n",
        "\n",
        "For the live tutorial, we require all participants to use Colab. If you decide to rerun the tutorial at later points and rather want to have it run locally on your machine, feel free to clone our [GitHub repository](https://github.com/mariusbock/dl-for-har).\n",
        "\n",
        "To get started with this notebook, you need to first run the code cell below. Please set `use_colab` to be `True` if you are accessing this notebook via Colab. If not, please set it to `False`. This code cell will make sure that imports from our GitHub repository will work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU7hQXifkr0n",
        "outputId": "8280385c-4866-4a5b-ccaa-8ae270bdd4db"
      },
      "source": [
        "import os, sys\n",
        "\n",
        "use_colab = True\n",
        "\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "\n",
        "if use_colab:\n",
        "    # move to content directory and remove directory for a clean start \n",
        "    %cd /content/         \n",
        "    %rm -rf dl-for-har\n",
        "    # clone package repository (will throw error if already cloned)\n",
        "    !git clone https://github.com/mariusbock/dl-for-har.git\n",
        "    # navigate to dl-for-har directory\n",
        "    %cd dl-for-har/       \n",
        "else:\n",
        "    os.chdir(module_path)\n",
        "    \n",
        "# this statement is needed so that we can use the methods of the DL-ARC pipeline\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'dl-for-har'...\n",
            "remote: Enumerating objects: 940, done.\u001b[K\n",
            "remote: Counting objects: 100% (940/940), done.\u001b[K\n",
            "remote: Compressing objects: 100% (624/624), done.\u001b[K\n",
            "remote: Total 940 (delta 445), reused 789 (delta 306), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (940/940), 25.65 MiB | 13.36 MiB/s, done.\n",
            "Resolving deltas: 100% (445/445), done.\n",
            "/content/dl-for-har\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz2hB3O3YJX6"
      },
      "source": [
        "## 6.2. Loading and preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRMk_5R_kcrJ",
        "outputId": "6ada39db-44d6-4461-8116-4cee67c37ee5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "from data_processing.sliding_window import apply_sliding_window\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# data loading\n",
        "data_folder = 'data'\n",
        "dataset = 'rwhar_3sbjs_data.csv'\n",
        "data = pd.read_csv(os.path.join(data_folder, dataset), names=['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity_label'])\n",
        "print(\"\\nValue counts before label encoding: \")\n",
        "print(data['activity_label'].value_counts())\n",
        "\n",
        "# all activity names (you need them to define the label_dict!)\n",
        "class_names = ['climbing_down', 'climbing_up', 'jumping', 'lying', 'running', 'sitting', 'standing', 'walking']\n",
        "\n",
        "# label encoding dict\n",
        "label_dict = {\n",
        "    'climbing_down': 0,\n",
        "    'climbing_up': 1,\n",
        "    'jumping': 2,\n",
        "    'lying': 3,\n",
        "    'running': 4,\n",
        "    'sitting': 5,\n",
        "    'standing': 6,\n",
        "    'walking': 7\n",
        "}\n",
        "\n",
        "# apply label encoding\n",
        "data['activity_label'] = data['activity_label'].replace(label_dict) \n",
        "\n",
        "# define the train data to be all data belonging to the first two subjects\n",
        "train_data = data[data.subject_id <= 1]\n",
        "# define the validation data to be all data belonging to the third subject\n",
        "valid_data = data[data.subject_id == 2]\n",
        "\n",
        "# settings for the sliding window (change them if you want to!)\n",
        "sw_length = 50\n",
        "sw_unit = 'units'\n",
        "sw_overlap = 50\n",
        "\n",
        "# apply a sliding window on top of both the train and validation data; you can use our predefined method\n",
        "X_train, y_train = apply_sliding_window(train_data.iloc[:, :-1], train_data.iloc[:, -1], sliding_window_size=sw_length, unit=sw_unit, sampling_rate=50, sliding_window_overlap=sw_overlap)\n",
        "X_valid, y_valid = apply_sliding_window(valid_data.iloc[:, :-1], valid_data.iloc[:, -1], sliding_window_size=sw_length, unit=sw_unit, sampling_rate=50, sliding_window_overlap=sw_overlap)\n",
        "\n",
        "# (optional) omit the first feature column (subject_identifier) from the train and validation dataset\n",
        "X_train, X_valid = X_train[:, :, 1:], X_valid[:, :, 1:]\n",
        "\n",
        "# convert the features of the train and validation to float32 and labels to uint8 for GPU compatibility \n",
        "X_train, y_train = X_train.astype(np.float32), y_train.astype(np.uint8)\n",
        "X_valid, y_valid = X_valid.astype(np.float32), y_valid.astype(np.uint8)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Value counts before label encoding: \n",
            "running          99204\n",
            "walking          96810\n",
            "sitting          95265\n",
            "standing         94106\n",
            "lying            94038\n",
            "climbing_up      87572\n",
            "climbing_down    78004\n",
            "jumping          14261\n",
            "Name: activity_label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SmPHMMxnB2d"
      },
      "source": [
        "## 6.3. The Label Smoothing Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usYpGVDXnCUg"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, smoothing=0.0):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, prediction, target):\n",
        "        assert 0 <= self.smoothing < 1\n",
        "        neglog_softmaxPrediction = -prediction.log_softmax(dim=1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            smoothedLabels = self.smoothing / (prediction.size(1) - 1)* torch.ones_like(prediction)\n",
        "            smoothedLabels.scatter_(1, target.data.unsqueeze(1), 1-self.smoothing)\n",
        "        return torch.mean(torch.sum(smoothedLabels * neglog_softmaxPrediction, dim=1)) "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLOXAncvYiRV"
      },
      "source": [
        "## 6.4. Training with and without Label Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whlsFyrVYpiL"
      },
      "source": [
        "### 6.4.1. Define the Config Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svIhikLBn_13"
      },
      "source": [
        "config = {\n",
        "    #### TRY AND CHANGE THESE PARAMETERS ####\n",
        "    # sliding window settings\n",
        "    'sw_length': 50,\n",
        "    'sw_unit': 'units',\n",
        "    'sampling_rate': 50,\n",
        "    'sw_overlap': 30,\n",
        "    # network settings\n",
        "    'nb_conv_blocks': 2,\n",
        "    'conv_block_type': 'normal',\n",
        "    'nb_filters': 64,\n",
        "    'filter_width': 11,\n",
        "    'nb_units_lstm': 128,\n",
        "    'nb_layers_lstm': 1,\n",
        "    'drop_prob': 0.5,\n",
        "    # training settings\n",
        "    'epochs': 30,\n",
        "    'batch_size': 100,\n",
        "    'loss': 'cross_entropy',\n",
        "    'use_weights': True,\n",
        "    'weights_init': 'xavier_uniform',\n",
        "    'optimizer': 'adam',\n",
        "    'lr': 1e-4,\n",
        "    'weight_decay': 1e-6,\n",
        "    ### UP FROM HERE YOU SHOULD RATHER NOT CHANGE THESE ####\n",
        "    'batch_norm': False,\n",
        "    'dilation': 1,\n",
        "    'pooling': False,\n",
        "    'pool_type': 'max',\n",
        "    'pool_kernel_width': 2,\n",
        "    'reduce_layer': False,\n",
        "    'reduce_layer_output': 10,\n",
        "    'nb_classes': 8,\n",
        "    'seed': 1,\n",
        "    'gpu': 'cuda:0',\n",
        "    'verbose': False,\n",
        "    'print_freq': 10,\n",
        "    'save_gradient_plot': False,\n",
        "    'print_counts': False,\n",
        "    'adj_lr': False,\n",
        "    'adj_lr_patience': 5,\n",
        "    'early_stopping': False,\n",
        "    'es_patience': 5,\n",
        "    'save_test_preds': False\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_8AsJ_MY8xR"
      },
      "source": [
        "### 6.4.3. With Label Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF_SUtmFrVHd",
        "outputId": "a0467bc4-b62e-4bba-d17a-528bb3b6ab7f"
      },
      "source": [
        "# initialize your DeepConvLSTM object \n",
        "network = DeepConvLSTM(config)\n",
        "\n",
        "# sends network to the GPU and sets it to training mode\n",
        "network.to(config['gpu'])\n",
        "network.train()\n",
        "\n",
        "# initialize the optimizer and loss\n",
        "optimizer = torch.optim.Adam(network.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "criterion = LabelSmoothingLoss(0.1)\n",
        "\n",
        "# define your training loop; iterates over the number of epochs\n",
        "for e in range(config['epochs']):\n",
        "    # helper objects needed for proper documentation\n",
        "    train_losses = []\n",
        "    train_preds = []\n",
        "    train_gt = []\n",
        "    start_time = time.time()\n",
        "    batch_num = 1\n",
        "\n",
        "    # initializes train dataset in Torch format\n",
        "    dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "    \n",
        "    # define your trainloader; use from torch.utils.data import DataLoader\n",
        "    trainloader = DataLoader(dataset, batch_size=config['batch_size'])\n",
        "\n",
        "    # iterate over the trainloader object (it'll return batches which you can use)\n",
        "    for i, (x, y) in enumerate(trainloader):\n",
        "        # sends batch x and y to the GPU\n",
        "        inputs, targets = x.to(config['gpu']), y.to(config['gpu'])\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # send inputs through network to get predictions\n",
        "        train_output = network(inputs)\n",
        "\n",
        "        # calculates loss\n",
        "        loss = criterion(train_output, targets.long())\n",
        "\n",
        "        # backprogate your computed loss through the network\n",
        "        # use the .backward() and .step() function on your loss and optimizer\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # calculate actual predictions (i.e. softmax probabilites); use torch.nn.functional.softmax()\n",
        "        train_output = torch.nn.functional.softmax(train_output, dim=1)\n",
        "\n",
        "        # appends the computed batch loss to list\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # creates predictions and true labels; appends them to the final lists\n",
        "        y_preds = np.argmax(train_output.cpu().detach().numpy(), axis=-1)\n",
        "        y_true = targets.cpu().numpy().flatten()\n",
        "        train_preds = np.concatenate((np.array(train_preds, int), np.array(y_preds, int)))\n",
        "        train_gt = np.concatenate((np.array(train_gt, int), np.array(y_true, int)))\n",
        "\n",
        "        # prints out every 100 batches information about the current loss and time per batch\n",
        "        if batch_num % 100 == 0 and batch_num > 0:\n",
        "            cur_loss = np.mean(train_losses)\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d} batches | ms/batch {:5.2f} | train loss {:5.2f}'.format(e, batch_num, elapsed * 1000 / config['batch_size'], cur_loss))\n",
        "            start_time = time.time()\n",
        "            batch_num += 1\n",
        "\n",
        "    # helper objects\n",
        "    val_preds = []\n",
        "    val_gt = []\n",
        "    val_losses = []\n",
        "\n",
        "    # initialize validation dataset in Torch format\n",
        "    dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_valid).float(), torch.from_numpy(y_valid))\n",
        "    \n",
        "    # define your valloader; use from torch.utils.data import DataLoader\n",
        "    valloader = DataLoader(dataset, batch_size=config['batch_size'])\n",
        "\n",
        "    # sets network to eval mode and \n",
        "    network.eval()\n",
        "    with torch.no_grad():\n",
        "        # iterate over the valloader object (it'll return batches which you can use)\n",
        "        for i, (x, y) in enumerate(valloader):\n",
        "            # sends batch x and y to the GPU\n",
        "            inputs, targets = x.to(config['gpu']), y.to(config['gpu'])\n",
        "\n",
        "            # send inputs through network to get predictions\n",
        "            val_output = network(inputs)\n",
        "\n",
        "            # calculates loss by passing criterion both predictions and true labels \n",
        "            val_loss = criterion(val_output, targets.long())\n",
        "\n",
        "            # calculate actual predictions (i.e. softmax probabilites); use torch.nn.functional.softmax() on dim=1\n",
        "            val_output = torch.nn.functional.softmax(val_output, dim=1)\n",
        "\n",
        "            # appends validation loss to list\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "            # creates predictions and true labels; appends them to the final lists\n",
        "            y_preds = np.argmax(val_output.cpu().numpy(), axis=-1)\n",
        "            y_true = targets.cpu().numpy().flatten()\n",
        "            val_preds = np.concatenate((np.array(val_preds, int), np.array(y_preds, int)))\n",
        "            val_gt = np.concatenate((np.array(val_gt, int), np.array(y_true, int)))\n",
        "\n",
        "        # print epoch evaluation results for train and validation dataset\n",
        "        print(\"\\nEPOCH: {}/{}\".format(e + 1, config['epochs']),\n",
        "                  \"\\nTrain Loss: {:.4f}\".format(np.mean(train_losses)),\n",
        "                  \"Train Acc: {:.4f}\".format(jaccard_score(train_gt, train_preds, average='macro')),\n",
        "                  \"Train Prec: {:.4f}\".format(precision_score(train_gt, train_preds, average='macro')),\n",
        "                  \"Train Rcll: {:.4f}\".format(recall_score(train_gt, train_preds, average='macro')),\n",
        "                  \"Train F1: {:.4f}\".format(f1_score(train_gt, train_preds, average='macro')),\n",
        "                  \"\\nVal Loss: {:.4f}\".format(np.mean(val_losses)),\n",
        "                  \"Val Acc: {:.4f}\".format(jaccard_score(val_gt, val_preds, average='macro')),\n",
        "                  \"Val Prec: {:.4f}\".format(precision_score(val_gt, val_preds, average='macro')),\n",
        "                  \"Val Rcll: {:.4f}\".format(recall_score(val_gt, val_preds, average='macro')),\n",
        "                  \"Val F1: {:.4f}\".format(f1_score(val_gt, val_preds, average='macro')))\n",
        "\n",
        "        # if chosen, print the value counts of the predicted labels for train and validation dataset\n",
        "        if config['print_counts']:\n",
        "            print('Predicted Train Labels: ')\n",
        "            print(np.vstack((np.nonzero(np.bincount(train_preds))[0], np.bincount(train_preds)[np.nonzero(np.bincount(train_preds))[0]])).T)\n",
        "            print('Predicted Val Labels: ')\n",
        "            print(np.vstack((np.nonzero(np.bincount(val_preds))[0], np.bincount(val_preds)[np.nonzero(np.bincount(val_preds))[0]])).T)\n",
        "\n",
        "\n",
        "    # set network to train mode again\n",
        "    network.train()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH: 1/30 \n",
            "Train Loss: 2.2150 Train Acc: 0.0550 Train Prec: 0.1202 Train Rcll: 0.1165 Train F1: 0.0970 \n",
            "Val Loss: 1.9732 Val Acc: 0.1506 Val Prec: 0.1734 Val Rcll: 0.2499 Val F1: 0.2033\n",
            "\n",
            "EPOCH: 2/30 \n",
            "Train Loss: 2.0666 Train Acc: 0.0862 Train Prec: 0.1728 Train Rcll: 0.1396 Train F1: 0.1457 \n",
            "Val Loss: 1.8832 Val Acc: 0.1721 Val Prec: 0.1912 Val Rcll: 0.3018 Val F1: 0.2109\n",
            "\n",
            "EPOCH: 3/30 \n",
            "Train Loss: 2.0197 Train Acc: 0.0980 Train Prec: 0.1724 Train Rcll: 0.1790 Train F1: 0.1653 \n",
            "Val Loss: 1.9243 Val Acc: 0.1138 Val Prec: 0.1785 Val Rcll: 0.2647 Val F1: 0.1689\n",
            "\n",
            "EPOCH: 4/30 \n",
            "Train Loss: 1.8225 Train Acc: 0.2098 Train Prec: 0.3035 Train Rcll: 0.3000 Train F1: 0.2950 \n",
            "Val Loss: 1.8468 Val Acc: 0.1258 Val Prec: 0.2963 Val Rcll: 0.2762 Val F1: 0.2001\n",
            "\n",
            "EPOCH: 5/30 \n",
            "Train Loss: 1.6994 Train Acc: 0.2657 Train Prec: 0.3504 Train Rcll: 0.3718 Train F1: 0.3544 \n",
            "Val Loss: 1.7919 Val Acc: 0.1467 Val Prec: 0.2427 Val Rcll: 0.3156 Val F1: 0.2197\n",
            "\n",
            "EPOCH: 6/30 \n",
            "Train Loss: 1.6187 Train Acc: 0.3195 Train Prec: 0.3898 Train Rcll: 0.4215 Train F1: 0.3984 \n",
            "Val Loss: 1.7558 Val Acc: 0.1329 Val Prec: 0.2067 Val Rcll: 0.2829 Val F1: 0.1934\n",
            "\n",
            "EPOCH: 7/30 \n",
            "Train Loss: 1.5642 Train Acc: 0.3351 Train Prec: 0.3938 Train Rcll: 0.4279 Train F1: 0.4065 \n",
            "Val Loss: 1.7402 Val Acc: 0.1452 Val Prec: 0.2945 Val Rcll: 0.3044 Val F1: 0.2097\n",
            "\n",
            "EPOCH: 8/30 \n",
            "Train Loss: 1.4857 Train Acc: 0.3606 Train Prec: 0.4216 Train Rcll: 0.4581 Train F1: 0.4364 \n",
            "Val Loss: 1.7563 Val Acc: 0.1365 Val Prec: 0.2358 Val Rcll: 0.2950 Val F1: 0.1991\n",
            "\n",
            "EPOCH: 9/30 \n",
            "Train Loss: 1.4390 Train Acc: 0.3657 Train Prec: 0.4241 Train Rcll: 0.4576 Train F1: 0.4349 \n",
            "Val Loss: 1.7741 Val Acc: 0.1273 Val Prec: 0.2238 Val Rcll: 0.2862 Val F1: 0.1864\n",
            "\n",
            "EPOCH: 10/30 \n",
            "Train Loss: 1.4088 Train Acc: 0.3779 Train Prec: 0.4444 Train Rcll: 0.4790 Train F1: 0.4556 \n",
            "Val Loss: 1.7322 Val Acc: 0.1375 Val Prec: 0.2192 Val Rcll: 0.2933 Val F1: 0.1982\n",
            "\n",
            "EPOCH: 11/30 \n",
            "Train Loss: 1.3722 Train Acc: 0.3880 Train Prec: 0.4561 Train Rcll: 0.4853 Train F1: 0.4656 \n",
            "Val Loss: 1.7977 Val Acc: 0.1286 Val Prec: 0.2148 Val Rcll: 0.2868 Val F1: 0.1874\n",
            "\n",
            "EPOCH: 12/30 \n",
            "Train Loss: 1.3628 Train Acc: 0.3914 Train Prec: 0.5859 Train Rcll: 0.4917 Train F1: 0.4716 \n",
            "Val Loss: 1.8005 Val Acc: 0.1292 Val Prec: 0.2177 Val Rcll: 0.2855 Val F1: 0.1888\n",
            "\n",
            "EPOCH: 13/30 \n",
            "Train Loss: 1.3493 Train Acc: 0.3901 Train Prec: 0.4642 Train Rcll: 0.4963 Train F1: 0.4763 \n",
            "Val Loss: 1.8036 Val Acc: 0.1356 Val Prec: 0.2143 Val Rcll: 0.2916 Val F1: 0.1961\n",
            "\n",
            "EPOCH: 14/30 \n",
            "Train Loss: 1.3118 Train Acc: 0.3993 Train Prec: 0.4759 Train Rcll: 0.5065 Train F1: 0.4876 \n",
            "Val Loss: 1.7491 Val Acc: 0.1426 Val Prec: 0.2142 Val Rcll: 0.2994 Val F1: 0.2040\n",
            "\n",
            "EPOCH: 15/30 \n",
            "Train Loss: 1.2770 Train Acc: 0.4225 Train Prec: 0.5061 Train Rcll: 0.5303 Train F1: 0.5149 \n",
            "Val Loss: 1.7702 Val Acc: 0.1285 Val Prec: 0.2063 Val Rcll: 0.2858 Val F1: 0.1879\n",
            "\n",
            "EPOCH: 16/30 \n",
            "Train Loss: 1.2640 Train Acc: 0.4373 Train Prec: 0.5950 Train Rcll: 0.5374 Train F1: 0.5235 \n",
            "Val Loss: 1.8215 Val Acc: 0.1179 Val Prec: 0.2179 Val Rcll: 0.2732 Val F1: 0.1744\n",
            "\n",
            "EPOCH: 17/30 \n",
            "Train Loss: 1.2741 Train Acc: 0.4271 Train Prec: 0.5124 Train Rcll: 0.5422 Train F1: 0.5229 \n",
            "Val Loss: 1.7465 Val Acc: 0.1367 Val Prec: 0.2289 Val Rcll: 0.2920 Val F1: 0.1987\n",
            "\n",
            "EPOCH: 18/30 \n",
            "Train Loss: 1.2092 Train Acc: 0.4534 Train Prec: 0.6621 Train Rcll: 0.5603 Train F1: 0.5481 \n",
            "Val Loss: 1.6752 Val Acc: 0.1443 Val Prec: 0.2148 Val Rcll: 0.2993 Val F1: 0.2065\n",
            "\n",
            "EPOCH: 19/30 \n",
            "Train Loss: 1.1819 Train Acc: 0.4609 Train Prec: 0.5450 Train Rcll: 0.5691 Train F1: 0.5555 \n",
            "Val Loss: 1.7058 Val Acc: 0.1455 Val Prec: 0.2114 Val Rcll: 0.2945 Val F1: 0.2078\n",
            "\n",
            "EPOCH: 20/30 \n",
            "Train Loss: 1.1672 Train Acc: 0.4713 Train Prec: 0.6539 Train Rcll: 0.5756 Train F1: 0.5630 \n",
            "Val Loss: 1.6878 Val Acc: 0.1501 Val Prec: 0.2114 Val Rcll: 0.3064 Val F1: 0.2115\n",
            "\n",
            "EPOCH: 21/30 \n",
            "Train Loss: 1.1444 Train Acc: 0.5008 Train Prec: 0.6805 Train Rcll: 0.6059 Train F1: 0.5962 \n",
            "Val Loss: 1.6719 Val Acc: 0.1739 Val Prec: 0.2150 Val Rcll: 0.3322 Val F1: 0.2343\n",
            "\n",
            "EPOCH: 22/30 \n",
            "Train Loss: 1.1705 Train Acc: 0.4566 Train Prec: 0.5371 Train Rcll: 0.5717 Train F1: 0.5498 \n",
            "Val Loss: 1.6501 Val Acc: 0.1672 Val Prec: 0.2307 Val Rcll: 0.3264 Val F1: 0.2309\n",
            "\n",
            "EPOCH: 23/30 \n",
            "Train Loss: 1.0803 Train Acc: 0.5441 Train Prec: 0.7350 Train Rcll: 0.6537 Train F1: 0.6407 \n",
            "Val Loss: 1.7070 Val Acc: 0.1532 Val Prec: 0.2066 Val Rcll: 0.3009 Val F1: 0.2121\n",
            "\n",
            "EPOCH: 24/30 \n",
            "Train Loss: 1.0543 Train Acc: 0.5450 Train Prec: 0.7247 Train Rcll: 0.6502 Train F1: 0.6408 \n",
            "Val Loss: 1.6704 Val Acc: 0.1713 Val Prec: 0.2064 Val Rcll: 0.3121 Val F1: 0.2256\n",
            "\n",
            "EPOCH: 25/30 \n",
            "Train Loss: 1.0440 Train Acc: 0.5523 Train Prec: 0.7477 Train Rcll: 0.6557 Train F1: 0.6398 \n",
            "Val Loss: 1.6619 Val Acc: 0.1701 Val Prec: 0.2027 Val Rcll: 0.3288 Val F1: 0.2261\n",
            "\n",
            "EPOCH: 26/30 \n",
            "Train Loss: 1.0007 Train Acc: 0.5831 Train Prec: 0.7759 Train Rcll: 0.6842 Train F1: 0.6735 \n",
            "Val Loss: 1.7391 Val Acc: 0.1723 Val Prec: 0.1963 Val Rcll: 0.3413 Val F1: 0.2310\n",
            "\n",
            "EPOCH: 27/30 \n",
            "Train Loss: 0.9969 Train Acc: 0.5778 Train Prec: 0.7658 Train Rcll: 0.6773 Train F1: 0.6678 \n",
            "Val Loss: 1.6885 Val Acc: 0.1480 Val Prec: 0.1981 Val Rcll: 0.3113 Val F1: 0.2105\n",
            "\n",
            "EPOCH: 28/30 \n",
            "Train Loss: 0.9929 Train Acc: 0.5920 Train Prec: 0.7664 Train Rcll: 0.6905 Train F1: 0.6894 \n",
            "Val Loss: 1.8237 Val Acc: 0.0930 Val Prec: 0.0973 Val Rcll: 0.2186 Val F1: 0.1328\n",
            "\n",
            "EPOCH: 29/30 \n",
            "Train Loss: 0.9477 Train Acc: 0.6204 Train Prec: 0.7976 Train Rcll: 0.7146 Train F1: 0.7122 \n",
            "Val Loss: 1.8141 Val Acc: 0.0916 Val Prec: 0.1110 Val Rcll: 0.2203 Val F1: 0.1372\n",
            "\n",
            "EPOCH: 30/30 \n",
            "Train Loss: 0.9359 Train Acc: 0.6200 Train Prec: 0.8040 Train Rcll: 0.7173 Train F1: 0.7202 \n",
            "Val Loss: 1.8160 Val Acc: 0.1108 Val Prec: 0.1541 Val Rcll: 0.2278 Val F1: 0.1670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD8e-ZOXY0VK"
      },
      "source": [
        "### 6.4.2. Without Label Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "657SF1Ivm6g5",
        "outputId": "ad234c00-5999-4f57-a6fd-be79665dbb50"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
        "\n",
        "from model.DeepConvLSTM import DeepConvLSTM\n",
        "\n",
        "import time\n",
        "\n",
        "# define the missing parameters within the config file. \n",
        "# window_size = size of the sliding window in units\n",
        "# nb_channels = number of feature channels\n",
        "# nb_classes = number of classes that can be predicted\n",
        "config['window_size'] = X_train.shape[1]\n",
        "config['nb_channels'] = X_train.shape[2]\n",
        "config['nb_classes'] = len(class_names)\n",
        "\n",
        "# initialize your DeepConvLSTM object \n",
        "network = DeepConvLSTM(config)\n",
        "\n",
        "# sends network to the GPU and sets it to training mode\n",
        "network.to(config['gpu'])\n",
        "network.train()\n",
        "\n",
        "# initialize the optimizer and loss\n",
        "optimizer = torch.optim.Adam(network.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "criterion = LabelSmoothingLoss()\n",
        "\n",
        "# define your training loop; iterates over the number of epochs\n",
        "for e in range(config['epochs']):\n",
        "    # helper objects needed for proper documentation\n",
        "    train_losses = []\n",
        "    train_preds = []\n",
        "    train_gt = []\n",
        "    start_time = time.time()\n",
        "    batch_num = 1\n",
        "\n",
        "    # initializes train dataset in Torch format\n",
        "    dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "    \n",
        "    # define your trainloader; use from torch.utils.data import DataLoader\n",
        "    trainloader = DataLoader(dataset, batch_size=config['batch_size'])\n",
        "\n",
        "    # iterate over the trainloader object (it'll return batches which you can use)\n",
        "    for i, (x, y) in enumerate(trainloader):\n",
        "        # sends batch x and y to the GPU\n",
        "        inputs, targets = x.to(config['gpu']), y.to(config['gpu'])\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # send inputs through network to get predictions\n",
        "        train_output = network(inputs)\n",
        "\n",
        "        # calculates loss\n",
        "        loss = criterion(train_output, targets.long())\n",
        "\n",
        "        # backprogate your computed loss through the network\n",
        "        # use the .backward() and .step() function on your loss and optimizer\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # calculate actual predictions (i.e. softmax probabilites); use torch.nn.functional.softmax()\n",
        "        train_output = torch.nn.functional.softmax(train_output, dim=1)\n",
        "\n",
        "        # appends the computed batch loss to list\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # creates predictions and true labels; appends them to the final lists\n",
        "        y_preds = np.argmax(train_output.cpu().detach().numpy(), axis=-1)\n",
        "        y_true = targets.cpu().numpy().flatten()\n",
        "        train_preds = np.concatenate((np.array(train_preds, int), np.array(y_preds, int)))\n",
        "        train_gt = np.concatenate((np.array(train_gt, int), np.array(y_true, int)))\n",
        "\n",
        "        # prints out every 100 batches information about the current loss and time per batch\n",
        "        if batch_num % 100 == 0 and batch_num > 0:\n",
        "            cur_loss = np.mean(train_losses)\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d} batches | ms/batch {:5.2f} | train loss {:5.2f}'.format(e, batch_num, elapsed * 1000 / config['batch_size'], cur_loss))\n",
        "            start_time = time.time()\n",
        "            batch_num += 1\n",
        "\n",
        "    # helper objects\n",
        "    val_preds = []\n",
        "    val_gt = []\n",
        "    val_losses = []\n",
        "\n",
        "    # initialize validation dataset in Torch format\n",
        "    dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_valid).float(), torch.from_numpy(y_valid))\n",
        "    \n",
        "    # define your valloader; use from torch.utils.data import DataLoader\n",
        "    valloader = DataLoader(dataset, batch_size=config['batch_size'])\n",
        "\n",
        "    # sets network to eval mode and \n",
        "    network.eval()\n",
        "    with torch.no_grad():\n",
        "        # iterate over the valloader object (it'll return batches which you can use)\n",
        "        for i, (x, y) in enumerate(valloader):\n",
        "            # sends batch x and y to the GPU\n",
        "            inputs, targets = x.to(config['gpu']), y.to(config['gpu'])\n",
        "\n",
        "            # send inputs through network to get predictions\n",
        "            val_output = network(inputs)\n",
        "\n",
        "            # calculates loss by passing criterion both predictions and true labels \n",
        "            val_loss = criterion(val_output, targets.long())\n",
        "\n",
        "            # calculate actual predictions (i.e. softmax probabilites); use torch.nn.functional.softmax() on dim=1\n",
        "            val_output = torch.nn.functional.softmax(val_output, dim=1)\n",
        "\n",
        "            # appends validation loss to list\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "            # creates predictions and true labels; appends them to the final lists\n",
        "            y_preds = np.argmax(val_output.cpu().numpy(), axis=-1)\n",
        "            y_true = targets.cpu().numpy().flatten()\n",
        "            val_preds = np.concatenate((np.array(val_preds, int), np.array(y_preds, int)))\n",
        "            val_gt = np.concatenate((np.array(val_gt, int), np.array(y_true, int)))\n",
        "\n",
        "        # print epoch evaluation results for train and validation dataset\n",
        "        print(\"\\nEPOCH: {}/{}\".format(e + 1, config['epochs']),\n",
        "                  \"\\nTrain Loss: {:.4f}\".format(np.mean(train_losses)),\n",
        "                  \"Train Acc: {:.4f}\".format(jaccard_score(train_gt, train_preds, average='macro')),\n",
        "                  \"Train Prec: {:.4f}\".format(precision_score(train_gt, train_preds, average='macro')),\n",
        "                  \"Train Rcll: {:.4f}\".format(recall_score(train_gt, train_preds, average='macro')),\n",
        "                  \"Train F1: {:.4f}\".format(f1_score(train_gt, train_preds, average='macro')),\n",
        "                  \"\\nVal Loss: {:.4f}\".format(np.mean(val_losses)),\n",
        "                  \"Val Acc: {:.4f}\".format(jaccard_score(val_gt, val_preds, average='macro')),\n",
        "                  \"Val Prec: {:.4f}\".format(precision_score(val_gt, val_preds, average='macro')),\n",
        "                  \"Val Rcll: {:.4f}\".format(recall_score(val_gt, val_preds, average='macro')),\n",
        "                  \"Val F1: {:.4f}\".format(f1_score(val_gt, val_preds, average='macro')))\n",
        "\n",
        "        # if chosen, print the value counts of the predicted labels for train and validation dataset\n",
        "        if config['print_counts']:\n",
        "            print('Predicted Train Labels: ')\n",
        "            print(np.vstack((np.nonzero(np.bincount(train_preds))[0], np.bincount(train_preds)[np.nonzero(np.bincount(train_preds))[0]])).T)\n",
        "            print('Predicted Val Labels: ')\n",
        "            print(np.vstack((np.nonzero(np.bincount(val_preds))[0], np.bincount(val_preds)[np.nonzero(np.bincount(val_preds))[0]])).T)\n",
        "\n",
        "\n",
        "    # set network to train mode again\n",
        "    network.train()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH: 1/30 \n",
            "Train Loss: 2.1729 Train Acc: 0.0596 Train Prec: 0.1057 Train Rcll: 0.1288 Train F1: 0.1032 \n",
            "Val Loss: 2.0512 Val Acc: 0.0701 Val Prec: 0.2099 Val Rcll: 0.1298 Val F1: 0.1184\n",
            "\n",
            "EPOCH: 2/30 \n",
            "Train Loss: 2.1090 Train Acc: 0.0480 Train Prec: 0.1173 Train Rcll: 0.1062 Train F1: 0.0878 \n",
            "Val Loss: 1.9685 Val Acc: 0.0467 Val Prec: 0.0487 Val Rcll: 0.1521 Val F1: 0.0738\n",
            "\n",
            "EPOCH: 3/30 \n",
            "Train Loss: 1.9939 Train Acc: 0.0770 Train Prec: 0.1589 Train Rcll: 0.1713 Train F1: 0.1339 \n",
            "Val Loss: 1.9444 Val Acc: 0.0325 Val Prec: 0.0375 Val Rcll: 0.1445 Val F1: 0.0565\n",
            "\n",
            "EPOCH: 4/30 \n",
            "Train Loss: 1.8732 Train Acc: 0.1172 Train Prec: 0.2097 Train Rcll: 0.2296 Train F1: 0.1911 \n",
            "Val Loss: 1.9474 Val Acc: 0.0182 Val Prec: 0.0252 Val Rcll: 0.1237 Val F1: 0.0322\n",
            "\n",
            "EPOCH: 5/30 \n",
            "Train Loss: 1.7912 Train Acc: 0.1505 Train Prec: 0.2443 Train Rcll: 0.2738 Train F1: 0.2334 \n",
            "Val Loss: 1.8003 Val Acc: 0.0728 Val Prec: 0.1523 Val Rcll: 0.2128 Val F1: 0.1184\n",
            "\n",
            "EPOCH: 6/30 \n",
            "Train Loss: 1.6953 Train Acc: 0.1961 Train Prec: 0.2803 Train Rcll: 0.3277 Train F1: 0.2833 \n",
            "Val Loss: 1.7604 Val Acc: 0.0892 Val Prec: 0.2035 Val Rcll: 0.2436 Val F1: 0.1308\n",
            "\n",
            "EPOCH: 7/30 \n",
            "Train Loss: 1.5690 Train Acc: 0.2652 Train Prec: 0.3563 Train Rcll: 0.3847 Train F1: 0.3595 \n",
            "Val Loss: 1.6402 Val Acc: 0.2298 Val Prec: 0.3075 Val Rcll: 0.3869 Val F1: 0.3086\n",
            "\n",
            "EPOCH: 8/30 \n",
            "Train Loss: 1.4675 Train Acc: 0.3399 Train Prec: 0.4135 Train Rcll: 0.4500 Train F1: 0.4224 \n",
            "Val Loss: 1.6782 Val Acc: 0.1049 Val Prec: 0.2106 Val Rcll: 0.2576 Val F1: 0.1615\n",
            "\n",
            "EPOCH: 9/30 \n",
            "Train Loss: 1.3588 Train Acc: 0.3760 Train Prec: 0.4477 Train Rcll: 0.4751 Train F1: 0.4520 \n",
            "Val Loss: 1.7324 Val Acc: 0.0859 Val Prec: 0.1393 Val Rcll: 0.2065 Val F1: 0.1248\n",
            "\n",
            "EPOCH: 10/30 \n",
            "Train Loss: 1.3010 Train Acc: 0.3829 Train Prec: 0.4568 Train Rcll: 0.4892 Train F1: 0.4611 \n",
            "Val Loss: 1.6944 Val Acc: 0.1221 Val Prec: 0.1576 Val Rcll: 0.2882 Val F1: 0.1851\n",
            "\n",
            "EPOCH: 11/30 \n",
            "Train Loss: 1.2550 Train Acc: 0.3919 Train Prec: 0.4597 Train Rcll: 0.4953 Train F1: 0.4675 \n",
            "Val Loss: 1.7352 Val Acc: 0.1180 Val Prec: 0.1598 Val Rcll: 0.2345 Val F1: 0.1629\n",
            "\n",
            "EPOCH: 12/30 \n",
            "Train Loss: 1.2105 Train Acc: 0.3984 Train Prec: 0.4683 Train Rcll: 0.5016 Train F1: 0.4760 \n",
            "Val Loss: 1.6768 Val Acc: 0.1135 Val Prec: 0.1464 Val Rcll: 0.2355 Val F1: 0.1583\n",
            "\n",
            "EPOCH: 13/30 \n",
            "Train Loss: 1.1700 Train Acc: 0.4135 Train Prec: 0.4860 Train Rcll: 0.5203 Train F1: 0.4949 \n",
            "Val Loss: 1.6612 Val Acc: 0.1179 Val Prec: 0.1574 Val Rcll: 0.2384 Val F1: 0.1647\n",
            "\n",
            "EPOCH: 14/30 \n",
            "Train Loss: 1.1317 Train Acc: 0.4248 Train Prec: 0.4923 Train Rcll: 0.5284 Train F1: 0.5037 \n",
            "Val Loss: 1.6297 Val Acc: 0.1173 Val Prec: 0.1676 Val Rcll: 0.2402 Val F1: 0.1663\n",
            "\n",
            "EPOCH: 15/30 \n",
            "Train Loss: 1.0886 Train Acc: 0.4408 Train Prec: 0.5100 Train Rcll: 0.5464 Train F1: 0.5219 \n",
            "Val Loss: 1.6570 Val Acc: 0.1277 Val Prec: 0.1817 Val Rcll: 0.2376 Val F1: 0.1687\n",
            "\n",
            "EPOCH: 16/30 \n",
            "Train Loss: 1.0669 Train Acc: 0.4501 Train Prec: 0.5210 Train Rcll: 0.5579 Train F1: 0.5329 \n",
            "Val Loss: 1.6177 Val Acc: 0.1275 Val Prec: 0.1943 Val Rcll: 0.2638 Val F1: 0.1896\n",
            "\n",
            "EPOCH: 17/30 \n",
            "Train Loss: 1.0329 Train Acc: 0.4501 Train Prec: 0.5156 Train Rcll: 0.5516 Train F1: 0.5287 \n",
            "Val Loss: 1.6423 Val Acc: 0.1242 Val Prec: 0.1821 Val Rcll: 0.2459 Val F1: 0.1749\n",
            "\n",
            "EPOCH: 18/30 \n",
            "Train Loss: 1.0179 Train Acc: 0.4573 Train Prec: 0.5220 Train Rcll: 0.5557 Train F1: 0.5354 \n",
            "Val Loss: 1.6464 Val Acc: 0.1116 Val Prec: 0.1439 Val Rcll: 0.2228 Val F1: 0.1467\n",
            "\n",
            "EPOCH: 19/30 \n",
            "Train Loss: 0.9817 Train Acc: 0.4651 Train Prec: 0.5309 Train Rcll: 0.5638 Train F1: 0.5433 \n",
            "Val Loss: 1.7233 Val Acc: 0.1151 Val Prec: 0.1481 Val Rcll: 0.2238 Val F1: 0.1505\n",
            "\n",
            "EPOCH: 20/30 \n",
            "Train Loss: 0.9819 Train Acc: 0.4603 Train Prec: 0.5320 Train Rcll: 0.5670 Train F1: 0.5418 \n",
            "Val Loss: 1.6331 Val Acc: 0.1330 Val Prec: 0.3376 Val Rcll: 0.2477 Val F1: 0.1847\n",
            "\n",
            "EPOCH: 21/30 \n",
            "Train Loss: 0.9899 Train Acc: 0.4466 Train Prec: 0.5140 Train Rcll: 0.5485 Train F1: 0.5251 \n",
            "Val Loss: 1.5849 Val Acc: 0.1357 Val Prec: 0.2280 Val Rcll: 0.2539 Val F1: 0.1834\n",
            "\n",
            "EPOCH: 22/30 \n",
            "Train Loss: 0.9313 Train Acc: 0.4699 Train Prec: 0.5319 Train Rcll: 0.5658 Train F1: 0.5456 \n",
            "Val Loss: 1.6743 Val Acc: 0.1214 Val Prec: 0.1847 Val Rcll: 0.2406 Val F1: 0.1694\n",
            "\n",
            "EPOCH: 23/30 \n",
            "Train Loss: 0.9009 Train Acc: 0.4778 Train Prec: 0.5450 Train Rcll: 0.5760 Train F1: 0.5575 \n",
            "Val Loss: 1.6448 Val Acc: 0.1193 Val Prec: 0.1500 Val Rcll: 0.2357 Val F1: 0.1599\n",
            "\n",
            "EPOCH: 24/30 \n",
            "Train Loss: 0.8629 Train Acc: 0.4894 Train Prec: 0.5582 Train Rcll: 0.5880 Train F1: 0.5701 \n",
            "Val Loss: 1.4606 Val Acc: 0.1542 Val Prec: 0.2076 Val Rcll: 0.2951 Val F1: 0.2144\n",
            "\n",
            "EPOCH: 25/30 \n",
            "Train Loss: 0.8551 Train Acc: 0.4850 Train Prec: 0.5531 Train Rcll: 0.5817 Train F1: 0.5647 \n",
            "Val Loss: 1.3770 Val Acc: 0.2794 Val Prec: 0.3462 Val Rcll: 0.4272 Val F1: 0.3633\n",
            "\n",
            "EPOCH: 26/30 \n",
            "Train Loss: 0.8405 Train Acc: 0.4880 Train Prec: 0.5578 Train Rcll: 0.5858 Train F1: 0.5684 \n",
            "Val Loss: 1.5022 Val Acc: 0.1715 Val Prec: 0.3078 Val Rcll: 0.2917 Val F1: 0.2421\n",
            "\n",
            "EPOCH: 27/30 \n",
            "Train Loss: 0.8099 Train Acc: 0.4965 Train Prec: 0.6473 Train Rcll: 0.5893 Train F1: 0.5760 \n",
            "Val Loss: 1.5171 Val Acc: 0.2259 Val Prec: 0.3468 Val Rcll: 0.3570 Val F1: 0.3145\n",
            "\n",
            "EPOCH: 28/30 \n",
            "Train Loss: 0.8557 Train Acc: 0.4776 Train Prec: 0.6574 Train Rcll: 0.5711 Train F1: 0.5601 \n",
            "Val Loss: 1.3691 Val Acc: 0.2127 Val Prec: 0.3716 Val Rcll: 0.3664 Val F1: 0.2878\n",
            "\n",
            "EPOCH: 29/30 \n",
            "Train Loss: 0.7875 Train Acc: 0.5031 Train Prec: 0.6764 Train Rcll: 0.5908 Train F1: 0.5804 \n",
            "Val Loss: 1.4826 Val Acc: 0.1910 Val Prec: 0.3504 Val Rcll: 0.3133 Val F1: 0.2680\n",
            "\n",
            "EPOCH: 30/30 \n",
            "Train Loss: 0.7573 Train Acc: 0.5266 Train Prec: 0.6970 Train Rcll: 0.6131 Train F1: 0.6139 \n",
            "Val Loss: 1.4976 Val Acc: 0.1896 Val Prec: 0.3333 Val Rcll: 0.3050 Val F1: 0.2568\n"
          ]
        }
      ]
    }
  ]
}