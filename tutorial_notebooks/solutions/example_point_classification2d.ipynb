{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's import the stuff we need in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.tri import Triangulation\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's define two data distributions in 2d\n",
    "These functions generate two distributions of points with 2d coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distrib1(n):\n",
    "    x = np.random.rand(n,2)-0.5\n",
    "    x[:,1] = 0.4 - 0.2*x[:,0] - 0.3*np.cos(6*np.pi*x[:,0]) + 0.1*np.random.randn(n) -(np.random.randn(n)<0)*0.8\n",
    "    return x\n",
    "\n",
    "def distrib2(n):\n",
    "    x = np.random.rand(n,2)-0.5\n",
    "    x[:,1] = - 0.2*x[:,0] - 0.3*np.cos(6*np.pi*x[:,0]) + 0.05*np.random.randn(n)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training samples and their visualization\n",
    "Lets sample n examples from the two distributions and visualize them. The task of our classification network will be to predict the membership to one of the classes for given 2d coordinates of a point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 110\n",
    "x1 = distrib1(n)\n",
    "x2 = distrib2(n)\n",
    "print(x1.shape, x2.shape)\n",
    "\n",
    "plt.plot(x1[:,0],x1[:,1], 'x', label='Class 0')\n",
    "plt.plot(x2[:,0],x2[:,1], 'o', label='Class 1')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate all points and generate class labels in order to have training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = np.concatenate((x1,x2),axis=0)\n",
    "Ytrain = np.ones((2*n))\n",
    "Ytrain[0:n] = 0\n",
    "\n",
    "\n",
    "Xtrain = torch.from_numpy(Xtrain).type(torch.FloatTensor)\n",
    "Ytrain = torch.from_numpy(Ytrain).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's define a simple (fully connected) network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define a network architecture\n",
    "class myNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        d_hidden = 40\n",
    "        self.fc1 = nn.Linear(2, d_hidden)\n",
    "        self.fc2 = nn.Linear(d_hidden,d_hidden)\n",
    "        self.fc3 = nn.Linear(d_hidden,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets create a network instance and train it\n",
    "model = myNet()\n",
    "\n",
    "# training\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(10001):\n",
    "    y_pred = model(Xtrain)\n",
    "    loss = criterion(y_pred, Ytrain)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if np.mod(epoch,1000)==0:\n",
    "            print(epoch, loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the result\n",
    "A nice way to visualize the predictions of a network in such a simple 2d example is to feed an entire meshgrid of coordinates (i.e. a dense sampling of the 2d plane) into the network, and visualize the predictions via a contour plot  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a meshgrid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-1,1,5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_y, grid_x = torch.meshgrid(x, x)\n",
    "print(grid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The visualization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeMyDecisionBoundary(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        c=100\n",
    "        x = torch.linspace(-0.7,0.7,c)\n",
    "        y = torch.linspace(-0.9,0.9,c)\n",
    "        grid_y, grid_x = torch.meshgrid(x, y)\n",
    "        wholeGrid = torch.cat((torch.reshape(grid_y, (-1,1)), torch.reshape(grid_x, (-1,1))), 1)\n",
    "\n",
    "        probs = F.softmax(model(wholeGrid),dim=1) # Predicted probabilities for each point on the grid\n",
    "        decisionBoundary = torch.reshape(probs[:,0], (c,c)) # Reshape into an image\n",
    "        plt.contourf(grid_y.detach().numpy() , grid_x.detach().numpy() , decisionBoundary.detach().numpy(),np.linspace(0,1,20),cmap=plt.cm.bone)\n",
    "\n",
    "        plt.plot(x1[:,0],x1[:,1], 'x', label='Class 0')\n",
    "        plt.plot(x2[:,0],x2[:,1], 'o', label='Class 1')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeMyDecisionBoundary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let us define the label smoothing loss ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, smoothing=0.0):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        assert 0 <= self.smoothing < 1\n",
    "        neglog_softmaxPrediction = -prediction.log_softmax(dim=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            smoothedLabels = self.smoothing / (prediction.size(1) - 1)* torch.ones_like(prediction)\n",
    "            smoothedLabels.scatter_(1, target.data.unsqueeze(1), 1-self.smoothing)\n",
    "        return torch.mean(torch.sum(smoothedLabels * neglog_softmaxPrediction, dim=1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... and train a network with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets create a network instance and train it\n",
    "modelLS = myNet()\n",
    "\n",
    "\n",
    "# training\n",
    "criterion = LabelSmoothingLoss(smoothing=0.1)   #<<<< This is the only line that changed!! \n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(modelLS.parameters(), lr=learning_rate)\n",
    "for epoch in range(10000):\n",
    "    y_pred = modelLS(Xtrain)\n",
    "    loss = criterion(y_pred, Ytrain)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if np.mod(epoch,1000)==0:\n",
    "            print(epoch, loss.item())\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualize the decision boundary of the network trained with label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeMyDecisionBoundary(modelLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reminder - this was the original decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeMyDecisionBoundary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play with Maxup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation inspired by https://github.com/JonasGeiping/data-poisoning/\n",
    "# see forest / data / mixing_data_augmentations.py\n",
    "\n",
    "class Maxup(torch.nn.Module):\n",
    "    \"\"\"A meta-augmentation, returning the worst result from a range of augmentations.\n",
    "    As in the orignal paper, https://arxiv.org/abs/2002.09024,\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, given_data_augmentation, ntrials=4):\n",
    "        \"\"\"Initialize with a given data augmentation module.\"\"\"\n",
    "        super().__init__()\n",
    "        self.augment = given_data_augmentation\n",
    "        self.ntrials = ntrials\n",
    "        self.max_criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        additional_x, additional_labels = [], []\n",
    "        for trial in range(self.ntrials):\n",
    "            x_out, y_out = self.augment(x, y)\n",
    "            additional_x.append(x_out)\n",
    "            additional_labels.append(y_out)\n",
    "\n",
    "        additional_x = torch.cat(additional_x, dim=0)\n",
    "        additional_labels = torch.cat(additional_labels, dim=0)\n",
    "        \n",
    "        return additional_x, additional_labels\n",
    "\n",
    "\n",
    "    def maxup_loss(self, outputs, extra_labels):\n",
    "        \"\"\"Compute loss. Here the loss is computed as worst-case estimate over the trials.\"\"\"\n",
    "        batch_size = outputs.shape[0] // self.ntrials\n",
    "        correct_preds = (torch.argmax(outputs.data, dim=1) == extra_labels).sum().item() / self.ntrials\n",
    "        stacked_loss = self.max_criterion(outputs, extra_labels).view(batch_size, self.ntrials, -1)\n",
    "        loss = stacked_loss.max(dim=1)[0].mean()\n",
    "        \n",
    "        return loss, correct_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myNoiseAdditionAugmenter(x,y):\n",
    "    sigma = 0.03\n",
    "    return x + sigma*torch.randn_like(x), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try our maxup implementation out in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets create a network instance and train it\n",
    "modelMaxup = myNet()\n",
    "\n",
    "\n",
    "# training\n",
    "maxup = Maxup(myNoiseAdditionAugmenter, ntrials=2)\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(modelMaxup.parameters(), lr=learning_rate)\n",
    "for epoch in range(10000):\n",
    "    \n",
    "    inputs,targets = maxup(Xtrain.detach().clone(),Ytrain.detach().clone())\n",
    "    y_pred = modelMaxup(inputs)\n",
    "    loss = maxup.maxup_loss(y_pred, targets.long())[0]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if np.mod(epoch,1000)==0:\n",
    "            print(epoch, loss.item())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeMyDecisionBoundary(modelMaxup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
