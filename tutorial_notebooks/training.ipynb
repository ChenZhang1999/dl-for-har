{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Network Architecture & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this notebook, you'll learn all you need to know about how to define your network architecture as well as train it. The notebook is structured into three main parts. \n",
    "\n",
    "First, we will go over a sample Deep Learning for HAR architecture called the DeepConvLSTM. You will learn how to define the architecture in PyTorch and create a trainable object. \n",
    "\n",
    "Second, we will go over essential steps which you need to apply on top of the data so that it is in the correct format to be fed into the network. This directly ties into what you have learned within the first notebook on preprocessing.\n",
    "\n",
    "Third, you'll learn how to embed your defined network object into a training loop and train it using the data of the first two subjects within the RWHAR dataset. You will also learn how to perfom a simple evaluation by applying your trained network on top of the data of the third subject within the RWHAR dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING FOR COLAB USERS:**  \n",
    "- Set use_colab to True if you are accessing this notebook\n",
    "- Change your runtime time to GPU by clicking: Runtime -> Change runtime type -> Dropdown -> GPU -> Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "use_colab = False\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if use_colab:\n",
    "    # clone package repository\n",
    "    !git clone https://github.com/mariusbock/dl-for-har.git\n",
    "\n",
    "    # navigate to dl-for-har directory\n",
    "    %cd dl-for-har/\n",
    "else:\n",
    "    os.chdir(module_path)\n",
    "    \n",
    "# this statement is needed so that we can use the methods of the DL-ARC pipeline\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Defining a Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will define a network which we can train using the data which we previously preprocessed. The architecture which we will used is called DeepConvLSTM. The architecture was introduced by Francisco Javier Ordonez and Daniel Roggen in 2016 and is to this date a state-of-the-art architecture for applying Deep Learning on Human Activity Recognition. The architecture combines both convolutional and recurrent layers.\n",
    "\n",
    "The architecture is made of three main parts:\n",
    "\n",
    "1. **Convolutional layers:** Within the original architecture Ordonez and Roggen apply 4 convolutional layers each with 64 filters of size 5x1. \n",
    "2. **LSTM layer(s):** After applying convolutional layers, Ordonez and Roggen make us of an LSTM in order to capture time dependencies on features extracted by convolutional operations. Originally, Ordonez and Roggen employed a 2-layered LSTM with 128 hidden units. Recently, we exhibited that a 1-layered LSTM might be a better suited option when dealing with raw sensor-data. We thus employ a 1-layered instead of 2-layered LSTM within this tutorial.\n",
    "3. **Classification layer:** The output of the LSTM is finally fed into a classifier which produces the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class DeepConvLSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DeepConvLSTM, self).__init__()\n",
    "        # parameters\n",
    "        self.window_size = config['window_size']\n",
    "        self.drop_prob = config['drop_prob']\n",
    "        self.nb_channels = config['nb_channels']\n",
    "        self.nb_classes = config['nb_classes']\n",
    "        self.seed = config['seed']\n",
    "        self.nb_filters = config['nb_filters']\n",
    "        self.filter_width = config['filter_width']\n",
    "        self.nb_units_lstm = config['nb_units_lstm']\n",
    "        self.nb_layers_lstm = config['nb_layers_lstm']\n",
    "\n",
    "        # define conv layers\n",
    "        self.conv1 = nn.Conv2d(1, self.nb_filters, (self.filter_width, 1))\n",
    "        self.conv2 = nn.Conv2d(self.nb_filters, self.nb_filters, (self.filter_width, 1))\n",
    "        self.conv3 = nn.Conv2d(self.nb_filters, self.nb_filters, (self.filter_width, 1))\n",
    "        self.conv4 = nn.Conv2d(self.nb_filters, self.nb_filters, (self.filter_width, 1))\n",
    "        \n",
    "        # define lstm layers\n",
    "        self.lstm = nn.LSTM(input_size=self.nb_filters * self.nb_channels, hidden_size=self.nb_units_lstm, num_layers=self.nb_layers_lstm)\n",
    "\n",
    "        # define dropout layer\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        \n",
    "        # define classifier\n",
    "        self.fc = nn.Linear(self.nb_units_lstm, self.nb_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # reshape data for convolutions\n",
    "        x = x.view(-1, 1, self.window_size, self.nb_channels)\n",
    "        \n",
    "        # apply convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        # set the final sequence length to be \n",
    "        final_seq_len = x.shape[2]\n",
    "        \n",
    "        # permute dimensions and reshape for LSTM\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = x.reshape(-1, final_seq_len, self.nb_filters * self.nb_channels)\n",
    "\n",
    "        # apply LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "            \n",
    "        # reshape data for classifier\n",
    "        x = x.view(-1, self.nb_units_lstm)\n",
    "        \n",
    "        # apply dropout and feed data through classifier\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # reshape data and return predicted label of last sample within final sequence (determines label of window)\n",
    "        out = x.view(-1, final_seq_len, self.nb_classes)\n",
    "        return out[:, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Preparing your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the network we just defined, we need to bring our data into the right format. The preprocessing consists of five essential parts.\n",
    "\n",
    "1. Split the data into a training and validation dataset. The validation dataset is used to gain feedback on the perfomance of the model and functions as unseen data. Results obtained on the validation dataset can be used as an indicator whether the changes you make to a network and/ or its training process are improving or worsening results.\n",
    "2. Apply the sliding window approach on top of the training and validation dataset. As you learned in the previous notebook, we do not classify a single record, but a window of records. The label of the last record within a window defines the label of the window and is our ultimate goal to predict.\n",
    "3. Omit the subject identifier column.\n",
    "4. Apply label encoding on top of the datasets, i.e. replace the string label names with integer values. \n",
    "5. Convert the two datasets into the correct data format so that they are compatible with the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts before label encoding: \n",
      "running          99204\n",
      "walking          96810\n",
      "sitting          95265\n",
      "standing         94106\n",
      "lying            94038\n",
      "climbing_up      87572\n",
      "climbing_down    78004\n",
      "jumping          14261\n",
      "Name: activity_label, dtype: int64\n",
      "\n",
      "Value counts after label encoding: \n",
      "4    99204\n",
      "7    96810\n",
      "5    95265\n",
      "6    94106\n",
      "3    94038\n",
      "1    87572\n",
      "0    78004\n",
      "2    14261\n",
      "Name: activity_label, dtype: int64\n",
      "\n",
      "Shape of the dataset before splitting and windowing: \n",
      "(659260, 5)\n",
      "\n",
      "Shape of the train and validation datasets after splitting and windowing: \n",
      "(17215, 50, 4) (17215,)\n",
      "(9151, 50, 4) (9151,)\n",
      "\n",
      "Shape of the train and validation feature dataset after splitting and windowing: \n",
      "(17215, 50, 3) (9151, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from data_processing.sliding_window import apply_sliding_window\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data loading; you already know this\n",
    "data_folder = 'data'\n",
    "dataset = 'rwhar_3sbjs_data.csv'\n",
    "data = pd.read_csv(os.path.join(data_folder, dataset), names=['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity_label'])\n",
    "print(\"\\nValue counts before label encoding: \")\n",
    "print(data['activity_label'].value_counts())\n",
    "\n",
    "# all activity names (you need them to define the label_dict!)\n",
    "class_names = ['climbing_down', 'climbing_up', 'jumping', 'lying', 'running', 'sitting', 'standing', 'walking']\n",
    "\n",
    "# apply label encoding: define a dict with all the activities and assign them integers going from zero \n",
    "# to (number of activites - 1); use the .replace() function of pandas.Series to replace the values within the dataset\n",
    "label_dict = {\n",
    "    'climbing_down': 0,\n",
    "    'climbing_up': 1,\n",
    "    'jumping': 2,\n",
    "    'lying': 3,\n",
    "    'running': 4,\n",
    "    'sitting': 5,\n",
    "    'standing': 6,\n",
    "    'walking': 7\n",
    "}\n",
    "\n",
    "data['activity_label'] = data['activity_label'].replace(label_dict) \n",
    "\n",
    "# if you did everything correctly then the label distribution should now be printed with integer instead of strings\n",
    "print(\"\\nValue counts after label encoding: \")\n",
    "print(data['activity_label'].value_counts())\n",
    "\n",
    "print(\"\\nShape of the dataset before splitting and windowing: \")\n",
    "print(data.shape)\n",
    "\n",
    "# define the train data to be all data belonging to the first two subjects\n",
    "train_data = data[data.subject_id <= 1]\n",
    "# define the validation data to be all data belonging to the third subject\n",
    "valid_data = data[data.subject_id == 2]\n",
    "\n",
    "# settings for the sliding window (change them if you want to!)\n",
    "sw_length = 50\n",
    "sw_unit = 'units'\n",
    "sw_overlap = 50\n",
    "\n",
    "# apply a sliding window on top of both the train and validation data; you can use our predefined method\n",
    "# you can import it via from preprocessing.sliding_window import apply_sliding_window\n",
    "X_train, y_train = apply_sliding_window(train_data.iloc[:, :-1], train_data.iloc[:, -1],\n",
    "                                        sliding_window_size=sw_length,\n",
    "                                        unit=sw_unit,\n",
    "                                        sampling_rate=50,\n",
    "                                        sliding_window_overlap=sw_overlap,\n",
    "                                        )\n",
    "\n",
    "\n",
    "X_valid, y_valid = apply_sliding_window(valid_data.iloc[:, :-1], valid_data.iloc[:, -1],\n",
    "                                        sliding_window_size=sw_length,\n",
    "                                        unit=sw_unit,\n",
    "                                        sampling_rate=50,\n",
    "                                        sliding_window_overlap=sw_overlap,\n",
    "                                        )\n",
    "\n",
    "print(\"\\nShape of the train and validation datasets after splitting and windowing: \")\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_valid.shape, y_valid.shape)\n",
    "\n",
    "# omit the first feature column (subject_identifier) from the train and validation dataset\n",
    "X_train, X_valid = X_train[:, :, 1:], X_valid[:, :, 1:]\n",
    "print(\"\\nShape of the train and validation feature dataset after splitting and windowing: \")\n",
    "print(X_train.shape, X_valid.shape)\n",
    "\n",
    "# convert the features of the train and validation to float32 and labels to uint8 for GPU compatibility \n",
    "X_train, y_train = X_train.astype(np.float32), y_train.astype(np.uint8)\n",
    "X_valid, y_valid = X_valid.astype(np.float32), y_valid.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Training Your Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have brought the data into the correct format, let's train our network with it!\n",
    "\n",
    "A typical training loop can be divided into three steps:\n",
    "\n",
    "1. **Definition:** You define your network, optimizer and loss\n",
    "2. **Training:** Iterating over the number of epochs: you chunk your training data into so-called batches and iteratively feed them through your network. After a batch has been fed through the network, you compute the loss said batch produced. Using the loss you backprogate it through the network using the optimizer which adjusts the weights accordingly. \n",
    "3. **Validation:** After you have processed your whole training dataset, you go on to validate the predictive performance of the network. To do so you again chunk your training and validation data into batches. Iterating over all batches of both all datasets, fed the batches through the trained network and obtain its predictions. **Note:** you only want to obtain predicitons and not backpropagate any loss. Using the predictions you can now use them to calculate your standard evaluation metrics which you learnt in previous notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Define your own train loop (ADVANCED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You'll see that we already defined a config object which you can use. Nevertheless, there are three values missing, i.e. the window size, number of feature channels and number of classes. Define them correctly.\n",
    "2. Define your network, optimizer and loss object\n",
    "3. Write your training loop: iterate over the number of epochs and define a DataLoader object using the train features and labels.\n",
    "4. Iterate over the DataLoader object; for each batch, compute the loss by passing it through the network; backprogate the computed loss using your optimizer object \n",
    "5. Obtain predictions for the train and validation dataset using the resulting trained network of the current epoch. To do so: define a DataLoader object for the validation dataset and fed both DataLoader objects batch-wise through the network. Obtain predicitons by applying softmax on top of the network output and compute the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH: 1/20 \n",
      "Train Loss: 2.0133 Train Acc: 0.1968 Train Prec: 0.3320 Train Rcll: 0.3394 Train F1: 0.2774 \n",
      "Val Loss: 2.0925 Val Acc: 0.1360 Val Prec: 0.1594 Val Rcll: 0.2392 Val F1: 0.1557\n",
      "\n",
      "EPOCH: 2/20 \n",
      "Train Loss: 1.9281 Train Acc: 0.2438 Train Prec: 0.2601 Train Rcll: 0.3578 Train F1: 0.2804 \n",
      "Val Loss: 1.8990 Val Acc: 0.2780 Val Prec: 0.3474 Val Rcll: 0.3718 Val F1: 0.3131\n",
      "\n",
      "EPOCH: 3/20 \n",
      "Train Loss: 1.8311 Train Acc: 0.3060 Train Prec: 0.3454 Train Rcll: 0.4229 Train F1: 0.3721 \n",
      "Val Loss: 1.8964 Val Acc: 0.2050 Val Prec: 0.2508 Val Rcll: 0.3350 Val F1: 0.2555\n",
      "\n",
      "EPOCH: 4/20 \n",
      "Train Loss: 1.8028 Train Acc: 0.3168 Train Prec: 0.3536 Train Rcll: 0.4470 Train F1: 0.3748 \n",
      "Val Loss: 1.9316 Val Acc: 0.1670 Val Prec: 0.1760 Val Rcll: 0.3194 Val F1: 0.2188\n",
      "\n",
      "EPOCH: 5/20 \n",
      "Train Loss: 1.6829 Train Acc: 0.3377 Train Prec: 0.4310 Train Rcll: 0.4789 Train F1: 0.4123 \n",
      "Val Loss: 1.9313 Val Acc: 0.0961 Val Prec: 0.1750 Val Rcll: 0.2397 Val F1: 0.1443\n",
      "\n",
      "EPOCH: 6/20 \n",
      "Train Loss: 1.5558 Train Acc: 0.3187 Train Prec: 0.3940 Train Rcll: 0.4518 Train F1: 0.3833 \n",
      "Val Loss: 1.9239 Val Acc: 0.1011 Val Prec: 0.1946 Val Rcll: 0.2501 Val F1: 0.1498\n",
      "\n",
      "EPOCH: 7/20 \n",
      "Train Loss: 1.5323 Train Acc: 0.2654 Train Prec: 0.3105 Train Rcll: 0.3833 Train F1: 0.3077 \n",
      "Val Loss: 1.9477 Val Acc: 0.0876 Val Prec: 0.1176 Val Rcll: 0.2409 Val F1: 0.1283\n",
      "\n",
      "EPOCH: 8/20 \n",
      "Train Loss: 1.4730 Train Acc: 0.3263 Train Prec: 0.4354 Train Rcll: 0.4798 Train F1: 0.3897 \n",
      "Val Loss: 1.9639 Val Acc: 0.0891 Val Prec: 0.1255 Val Rcll: 0.2461 Val F1: 0.1297\n",
      "\n",
      "EPOCH: 9/20 \n",
      "Train Loss: 1.4077 Train Acc: 0.3640 Train Prec: 0.6451 Train Rcll: 0.5087 Train F1: 0.4441 \n",
      "Val Loss: 1.8724 Val Acc: 0.0969 Val Prec: 0.1088 Val Rcll: 0.2409 Val F1: 0.1414\n",
      "\n",
      "EPOCH: 10/20 \n",
      "Train Loss: 1.3929 Train Acc: 0.3887 Train Prec: 0.6991 Train Rcll: 0.5244 Train F1: 0.4653 \n",
      "Val Loss: 1.9097 Val Acc: 0.0637 Val Prec: 0.1128 Val Rcll: 0.1976 Val F1: 0.1020\n",
      "\n",
      "EPOCH: 11/20 \n",
      "Train Loss: 1.2877 Train Acc: 0.3856 Train Prec: 0.6933 Train Rcll: 0.5069 Train F1: 0.4617 \n",
      "Val Loss: 1.9159 Val Acc: 0.0908 Val Prec: 0.1448 Val Rcll: 0.2466 Val F1: 0.1312\n",
      "\n",
      "EPOCH: 12/20 \n",
      "Train Loss: 1.2212 Train Acc: 0.4695 Train Prec: 0.6851 Train Rcll: 0.5925 Train F1: 0.5863 \n",
      "Val Loss: 1.8965 Val Acc: 0.0543 Val Prec: 0.1738 Val Rcll: 0.1672 Val F1: 0.0943\n",
      "\n",
      "EPOCH: 13/20 \n",
      "Train Loss: 1.1870 Train Acc: 0.4738 Train Prec: 0.7661 Train Rcll: 0.5898 Train F1: 0.5806 \n",
      "Val Loss: 1.9696 Val Acc: 0.0410 Val Prec: 0.1651 Val Rcll: 0.1448 Val F1: 0.0715\n",
      "\n",
      "EPOCH: 14/20 \n",
      "Train Loss: 1.1397 Train Acc: 0.6188 Train Prec: 0.7879 Train Rcll: 0.7268 Train F1: 0.7352 \n",
      "Val Loss: 1.9298 Val Acc: 0.0452 Val Prec: 0.2150 Val Rcll: 0.1467 Val F1: 0.0795\n",
      "\n",
      "EPOCH: 15/20 \n",
      "Train Loss: 1.0817 Train Acc: 0.6255 Train Prec: 0.7829 Train Rcll: 0.7329 Train F1: 0.7452 \n",
      "Val Loss: 1.8850 Val Acc: 0.0320 Val Prec: 0.1560 Val Rcll: 0.1338 Val F1: 0.0555\n",
      "\n",
      "EPOCH: 16/20 \n",
      "Train Loss: 1.0645 Train Acc: 0.6024 Train Prec: 0.7823 Train Rcll: 0.7167 Train F1: 0.7234 \n",
      "Val Loss: 1.8775 Val Acc: 0.0552 Val Prec: 0.2409 Val Rcll: 0.1644 Val F1: 0.0969\n",
      "\n",
      "EPOCH: 17/20 \n",
      "Train Loss: 1.0390 Train Acc: 0.6611 Train Prec: 0.8159 Train Rcll: 0.7590 Train F1: 0.7676 \n",
      "Val Loss: 1.9296 Val Acc: 0.0407 Val Prec: 0.2276 Val Rcll: 0.1419 Val F1: 0.0714\n",
      "\n",
      "EPOCH: 18/20 \n",
      "Train Loss: 1.0009 Train Acc: 0.6688 Train Prec: 0.8267 Train Rcll: 0.7629 Train F1: 0.7719 \n",
      "Val Loss: 1.8601 Val Acc: 0.0584 Val Prec: 0.2426 Val Rcll: 0.1710 Val F1: 0.1025\n",
      "\n",
      "EPOCH: 19/20 \n",
      "Train Loss: 0.9567 Train Acc: 0.6686 Train Prec: 0.8306 Train Rcll: 0.7623 Train F1: 0.7730 \n",
      "Val Loss: 1.8506 Val Acc: 0.1083 Val Prec: 0.2921 Val Rcll: 0.2575 Val F1: 0.1651\n",
      "\n",
      "EPOCH: 20/20 \n",
      "Train Loss: 0.9339 Train Acc: 0.6839 Train Prec: 0.8271 Train Rcll: 0.7806 Train F1: 0.7904 \n",
      "Val Loss: 1.8076 Val Acc: 0.1027 Val Prec: 0.2771 Val Rcll: 0.2567 Val F1: 0.1535\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
    "\n",
    "import time\n",
    "\n",
    "# this is the config object which contains all relevant settings. Feel free to change them and see how it influences\n",
    "# your results. Parameters which shouldn't be changed are marked.\n",
    "config = {\n",
    "    'nb_filters': 64,\n",
    "    'filter_width': 11,\n",
    "    'nb_units_lstm': 128,\n",
    "    'nb_layers_lstm': 1,\n",
    "    'drop_prob': 0.5,\n",
    "    'seed': 1,\n",
    "    'epochs': 20,\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-6,\n",
    "    'gpu_name': 'cuda:0',\n",
    "    'print_counts': False\n",
    "}\n",
    "\n",
    "# define the missing parameters within the config file. \n",
    "# window_size = size of the sliding window in units\n",
    "# nb_channels = number of feature channels\n",
    "# nb_classes = number of classes that can be predicted\n",
    "config['window_size'] = X_train.shape[1]\n",
    "config['nb_channels'] = X_train.shape[2]\n",
    "config['nb_classes'] = len(class_names)\n",
    "\n",
    "# initialize your DeepConvLSTM object \n",
    "network = DeepConvLSTM(config)\n",
    "\n",
    "# send network to the GPU and set it to training mode\n",
    "network.to(config['gpu_name'])\n",
    "network.train()\n",
    "\n",
    "# initialize your optimizer and loss; e.g. Adam optimizer and Cross-entropy loss\n",
    "# look up the PyTorch documentation for more options\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# define your training loop; iterates over the number of epochs\n",
    "for e in range(config['epochs']):\n",
    "    # helper objects needed for proper documentation\n",
    "    train_losses = []\n",
    "    start_time = time.time()\n",
    "    batch_num = 1\n",
    "\n",
    "    # initialize train dataset in Torch format\n",
    "    dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "    \n",
    "    # define your trainloader; use from torch.utils.data import DataLoader\n",
    "    trainloader = DataLoader(dataset,\n",
    "                             batch_size=config['batch_size'],\n",
    "                             num_workers=2,\n",
    "                             shuffle=False,\n",
    "                             )\n",
    "\n",
    "    # iterate over the trainloader object (it'll return batches which you can use)\n",
    "    for i, (x, y) in enumerate(trainloader):\n",
    "        # sends batch x and y to the GPU\n",
    "        inputs, targets = x.to(config['gpu_name']), y.to(config['gpu_name'])\n",
    "        # zero accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # send inputs through network to get predictions, calculate loss\n",
    "        output = network(inputs)\n",
    "        loss = criterion(output, targets.long())\n",
    "        # backprogates your computed loss through the network\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # appends the computed batch loss to list\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # prints out every 100 batches information about the current loss and time per batch\n",
    "        if batch_num % 100 == 0 and batch_num > 0:\n",
    "            cur_loss = np.mean(train_losses)\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d} batches | ms/batch {:5.2f} | train loss {:5.2f}'.format(e, batch_num, elapsed * 1000 / config['batch_size'], cur_loss))\n",
    "            start_time = time.time()\n",
    "            batch_num += 1\n",
    "\n",
    "            \n",
    "    # helper objects\n",
    "    val_preds = []\n",
    "    val_gt = []\n",
    "    val_losses = []\n",
    "    train_preds = []\n",
    "    train_gt = []\n",
    "\n",
    "    # initialize validation dataset in Torch format\n",
    "    dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_valid).float(), torch.from_numpy(y_valid))\n",
    "    \n",
    "    # define your valloader; use from torch.utils.data import DataLoader\n",
    "    valloader = DataLoader(dataset,\n",
    "                           batch_size=config['batch_size'],\n",
    "                           num_workers=2,\n",
    "                           shuffle=False,\n",
    "                           )\n",
    "\n",
    "    # sets network to eval mode and \n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        # iterate over the valloader object (it'll return batches which you can use)\n",
    "        for i, (x, y) in enumerate(valloader):\n",
    "            # sends batch x and y to the GPU\n",
    "            inputs, targets = x.to(config['gpu_name']), y.to(config['gpu_name'])\n",
    "\n",
    "            # send inputs through network to get predictions\n",
    "            val_output = network(inputs)\n",
    "            # calculate loss by passing criterion both predicitons and true labels \n",
    "            val_loss = criterion(val_output, targets.long())\n",
    "            # calculate actual predictions (i.e. softmax probabilites); use torch.nn.functional.softmax()\n",
    "            val_output = torch.nn.functional.softmax(val_output, dim=1)\n",
    "\n",
    "            # appends validation loss to list\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "            # creates predictions and true labels; appends them to the final lists\n",
    "            y_preds = np.argmax(val_output.cpu().numpy(), axis=-1)\n",
    "            y_true = targets.cpu().numpy().flatten()\n",
    "            val_preds = np.concatenate((np.array(val_preds, int), np.array(y_preds, int)))\n",
    "            val_gt = np.concatenate((np.array(val_gt, int), np.array(y_true, int)))\n",
    "\n",
    "        # iterate over the trainloader object (it'll return batches which you can use)\n",
    "        for i, (x, y) in enumerate(trainloader):\n",
    "            # sends batch x and y to the GPU\n",
    "            inputs, targets = x.to(config['gpu_name']), y.to(config['gpu_name'])\n",
    "\n",
    "            # send inputs through network to get predictions\n",
    "            train_output = network(inputs)\n",
    "            # calculate actual predictions (i.e. softmax probabilites); use torch.nn.functional.softmax()\n",
    "            train_output = torch.nn.functional.softmax(train_output, dim=1)\n",
    "\n",
    "            # creates predictions and true labels; appends them to the final lists\n",
    "            y_preds = np.argmax(train_output.cpu().numpy(), axis=-1)\n",
    "            y_true = targets.cpu().numpy().flatten()\n",
    "            train_preds = np.concatenate((np.array(train_preds, int), np.array(y_preds, int)))\n",
    "            train_gt = np.concatenate((np.array(train_gt, int), np.array(y_true, int)))\n",
    "\n",
    "        # print epoch evaluation results for train and validation dataset\n",
    "        print(\"\\nEPOCH: {}/{}\".format(e + 1, config['epochs']),\n",
    "                  \"\\nTrain Loss: {:.4f}\".format(np.mean(train_losses)),\n",
    "                  \"Train Acc: {:.4f}\".format(jaccard_score(train_gt, train_preds, average='macro')),\n",
    "                  \"Train Prec: {:.4f}\".format(precision_score(train_gt, train_preds, average='macro')),\n",
    "                  \"Train Rcll: {:.4f}\".format(recall_score(train_gt, train_preds, average='macro')),\n",
    "                  \"Train F1: {:.4f}\".format(f1_score(train_gt, train_preds, average='macro')),\n",
    "                  \"\\nVal Loss: {:.4f}\".format(np.mean(val_losses)),\n",
    "                  \"Val Acc: {:.4f}\".format(jaccard_score(val_gt, val_preds, average='macro')),\n",
    "                  \"Val Prec: {:.4f}\".format(precision_score(val_gt, val_preds, average='macro')),\n",
    "                  \"Val Rcll: {:.4f}\".format(recall_score(val_gt, val_preds, average='macro')),\n",
    "                  \"Val F1: {:.4f}\".format(f1_score(val_gt, val_preds, average='macro')))\n",
    "\n",
    "        # if chosen, print the value counts of the predicted labels for train and validation dataset\n",
    "        if config['print_counts']:\n",
    "            print('Predicted Train Labels: ')\n",
    "            print(np.vstack((np.nonzero(np.bincount(train_preds))[0], np.bincount(train_preds)[np.nonzero(np.bincount(train_preds))[0]])).T)\n",
    "            print('Predicted Val Labels: ')\n",
    "            print(np.vstack((np.nonzero(np.bincount(val_preds))[0], np.bincount(val_preds)[np.nonzero(np.bincount(val_preds))[0]])).T)\n",
    "\n",
    "\n",
    "    # set network to train mode again\n",
    "    network.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
