{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "lLy5PsIql9A5"
   },
   "source": [
    "# 5. Validation & Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6L-ynOvokKT"
   },
   "source": [
    "Welcome to the fifth and final notebook of our five part series part of our tutorial on Deep Learning for Human Activity Recognition. Within the last notebook you learned:\n",
    "\n",
    "- How do I define a sample neural network architecture in PyTorch? \n",
    "- What additional preprocessing do I need to apply to my data to fed it into my network?\n",
    "- How do I define a train loop which trains my neural network?\n",
    "\n",
    "This notebook will teach you everything you need to know about validation and testing. When building a predictive pipeline there are a lot of parameters which one needs to set before comencing the actual training. Coming up with a suitable set of hyperparameters is called hypertuning. In order to gain feedback whether the applied hyperparameters are a good choice, we check the predictive performance of our model on the validation set. This is called validation.\n",
    "\n",
    "Now you might ask yourself: Solely relying and tuning based on the validation scores would inherit that your trained model would end up being too well optimized on the validation set and thus not general anymore, right? If asked yourself that question, then you are 100% right in your assumption! This is what we call overfitting and is one of the major pitfalls in Machine Learning.Overfitting your model results in bad prediction performance on unseen data. \n",
    "\n",
    "We therefore need a third dataset, called the test dataset. The test dataset is a part of the initial dataset which you keep separate from all optimization steps. It is only used to gain insights on the predictive performance of the model and must not (!) be used as a reference for tuning hyperparameters. As we mentioned in during the theoretical parts of this tutorial, (supervised) Deep Learning, in our opinion, is just a fancy word for function approximation. If your model performs both well during validation and testing, it is a general function which properly approximates the underlying function.\n",
    "\n",
    "After completing this notebook you will be answer the following questions:\n",
    "- How do I split my initial dataset into a train, validation and test dataset?\n",
    "- What validation methods exist in Human Activity Recognition? How are they performed?\n",
    "- How is testing usually performed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4fWjW5V0_MT"
   },
   "source": [
    "## 5.1. Important Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkhCF6Pd1B1Z"
   },
   "source": [
    "If you are accessing this tutorial via Google Colab, first make sure to use Google Colab in English. This will help us to better assist you with issues that might arise during the tutorial. There are two ways to change the default language if it isn't English already:\n",
    "1. On Google Colab, go to 'Help' -> 'View in English'. \n",
    "2. Change the default language of your browser to English.\n",
    "\n",
    "In general, we strongly advise you to use Google Colab as it provides you with a working Python distribution as well as free GPU resources. To make Colab use GPUs, you need to change the current notebooks runtime type via:\n",
    "\n",
    "- Runtime -> Change runtime type -> Dropdown -> GPU -> Save\n",
    "\n",
    "For the live tutorial, we require all participants to use Colab. If you decide to rerun the tutorial at later points and rather want to have it run locally on your machine, feel free to clone our GitHub repository (mariusbock/dl-for-har).\n",
    "\n",
    "To get started with this notebook, you need to first run the code cell below. Please set `use_colab` to be `True` if you are accessing this notebook via Colab. If not, please set it to `False`. This code cell will make sure that imports from our GitHub repository will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "si3n5Sc51L-D",
    "outputId": "438a7c5c-b71a-4ec8-f5b5-d9c2179f898e"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "use_colab = True\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if use_colab:\n",
    "    # move to content directory and remove directory for a clean start \n",
    "    %cd /content/         \n",
    "    %rm -rf dl-for-har\n",
    "    # clone package repository (will throw error if already cloned)\n",
    "    !git clone https://github.com/mariusbock/dl-for-har.git\n",
    "    # navigate to dl-for-har directory\n",
    "    %cd dl-for-har/       \n",
    "else:\n",
    "    os.chdir(module_path)\n",
    "    \n",
    "# this statement is needed so that we can use the methods of the DL-ARC pipeline\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIjrK-KE1iDL"
   },
   "source": [
    "## 5.1. Splitting your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrLU2e9H1oAX"
   },
   "source": [
    "Within the first part of this notebook we will split our data in the above mentioned three datasets, namely the train, validation and test dataset. There are multiple ways how to split the data into the two respective datasets, for example:\n",
    "\n",
    "- **Subject-wise:** split according to participants within the dataset. This means that we are reserving certain subjects to be included in the train, validation and test set respectively. For example, given that there are a total of 10 subjects, you could use 6 subjects for trainig, 2 subjects for validation and 2 subjects for testing.\n",
    "- **Percentage-wise:** state how large percentage-wise your train, validation and test dataset should be compared to the full dataset. For example, you could use 60% of your data for training, 20% for validation and 20% for testing. The three splits can also be chosen to be stratified, meaning that the relative label distribution within each of the two dataset is kept the same as in the full dataset. Note that stratifiying your data would require the data to be shuffled.\n",
    "- **Record-wise:** state how many records should be in your train, validation and test dataset should be contained, i.e. define two cutoff points. For example, given that there are 1 million records in your full dataset, you could have the first 600 thousand records to be contained in the train dataset, the next 200 thousand in the validation dataset and the remaining 200 thousand records to be contained in the test dataset.\n",
    "\n",
    "**WARNING:** shuffling your dataset during splitting (which is e.g. needed for stratified splits) will destroy the time-dependencies among the data records. To minimize this effect, apply a sliding window on top of your data before splitting. This way, time-dependencies will at least be preserved within the windows. While working on this notebook, we will notify you when this is necessary.\n",
    "\n",
    "To keep things simple and fast, we will be splitting our data subject-wise. We will use the first data of the first subject for training, the data of the second subject for validation and the data of the third subject for testing. Your first task will be to perform said split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIAoSI0Ql9BC"
   },
   "source": [
    "### Task 1: Split the data into train, validation and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_QkR_bHl9BC"
   },
   "source": [
    "1. Define the `train` dataset to be the data of the first subject, i.e. with `subject_identifier = 0`.\n",
    "2. Define the `valid` dataset to be the data of the second subject, i.e. with `subject_identifier = 1`.\n",
    "3. Define the `test` dataset to be the data of the third subject, i.e. with `subject_identifier = 2`.\n",
    "4. Define a fourth dataset being a concatenated version of the `train` and `valid` dataset called `train_valid`. You will need this dataset for some of the validation methods. Use `pd.concat()` in order to concat the two Pandas dataframes along `axis=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "el2x8KMJl9BE",
    "outputId": "c3790e74-f347-4ee7-ed94-53a8964b8850"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "######################### YOU ALREADY KNOW THIS ################################\n",
    "\n",
    "# folder where the data is located and name of dataset\n",
    "data_folder = 'data'\n",
    "dataset = 'rwhar_3sbjs_data.csv'\n",
    "\n",
    "# read in the data using the pandas read_csv function; define the header as done previously\n",
    "data = pd.read_csv(os.path.join(data_folder, dataset), names=['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity_label'])\n",
    "print(data.head())\n",
    "\n",
    "# label dictionary needed for converting the string label names to integers \n",
    "label_dict = {\n",
    "    'climbing_down': 0,\n",
    "    'climbing_up': 1,\n",
    "    'jumping': 2,\n",
    "    'lying': 3,\n",
    "    'running': 4,\n",
    "    'sitting': 5,\n",
    "    'standing': 6,\n",
    "    'walking': 7\n",
    "}\n",
    "\n",
    "# all activity names\n",
    "class_names = ['climbing_down', 'climbing_up', 'jumping', 'lying', 'running', 'sitting', 'standing', 'walking']\n",
    "\n",
    "# replace values within the 'activity_label' column using the label_dict (use .replace())\n",
    "data['activity_label'] = data['activity_label'].replace(label_dict) \n",
    "\n",
    "######################### NOW COMES THE NEW PART ###############################\n",
    "\n",
    "# define the train data to be the data of the first subject\n",
    "train_data = data[data.subject_id == 0]\n",
    "# define the valid data to be the data of the second subject\n",
    "valid_data = data[data.subject_id == 1]\n",
    "# define the test data to be the data of the third subject\n",
    "test_data = data[data.subject_id == 2]\n",
    "\n",
    "# define the train_valid_data by concatenating the train and validation dataset \n",
    "train_valid_data = pd.concat((train,))\n",
    "\n",
    "print('\\nShape of the train, validation and test dataset:')\n",
    "print(train_data.shape, valid_data.shape, test_data.shape)\n",
    "print('\\nShape of the concatenated train_valid dataset:')\n",
    "print(train_valid_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzCPsMcd4koA"
   },
   "source": [
    "## 5.2. Define the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_q8TpPal9BE"
   },
   "source": [
    "Before we go over talking about how to perform validation in Human Activtiy Recognition, we need to define our hyperparameters again. As you know from the previous notebook, it is common practice to track all your settings and parameters in a compiled `config` object. Due to fact that we will be using pre-implemented methods of the feature stack of the DL-ARC GitHub, we will now need to define a more complex `config` object. \n",
    "\n",
    "Within the next code block we defined a sample `config` object for you. It contains some parameters which you already know from previous notebooks, but also lots which you don't know. We will not cover all of them during this tutorial, but encourage you to check out the complete implementation of the DL-ARC. We also separated the parameters into two groups for you, once which you can play around with and ones which you should handle with care and rather leave as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjZYXFX6l9BF"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    #### TRY AND CHANGE THESE PARAMETERS ####\n",
    "    # sliding window settings\n",
    "    'sw_length': 50,\n",
    "    'sw_unit': 'units',\n",
    "    'sampling_rate': 50,\n",
    "    'sw_overlap': 30,\n",
    "    # network settings\n",
    "    'nb_conv_blocks': 2,\n",
    "    'conv_block_type': 'normal',\n",
    "    'nb_filters': 64,\n",
    "    'filter_width': 11,\n",
    "    'nb_units_lstm': 128,\n",
    "    'nb_layers_lstm': 1,\n",
    "    'drop_prob': 0.5,\n",
    "    # training settings\n",
    "    'epochs': 10,\n",
    "    'batch_size': 100,\n",
    "    'loss': 'cross_entropy',\n",
    "    'use_weights': True,\n",
    "    'weights_init': 'xavier_uniform',\n",
    "    'optimizer': 'adam',\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-6,\n",
    "    ### UP FROM HERE YOU SHOULD RATHER NOT CHANGE THESE ####\n",
    "    'batch_norm': False,\n",
    "    'dilation': 1,\n",
    "    'pooling': False,\n",
    "    'pool_type': 'max',\n",
    "    'pool_kernel_width': 2,\n",
    "    'reduce_layer': False,\n",
    "    'reduce_layer_output': 10,\n",
    "    'nb_classes': 8,\n",
    "    'seed': 1,\n",
    "    'gpu': 'cuda:0',\n",
    "    'verbose': False,\n",
    "    'print_freq': 10,\n",
    "    'save_gradient_plot': False,\n",
    "    'print_counts': False,\n",
    "    'adj_lr': False,\n",
    "    'adj_lr_patience': 5,\n",
    "    'early_stopping': False,\n",
    "    'es_patience': 5,\n",
    "    'save_test_preds': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0WRQj1dl9BF"
   },
   "source": [
    "## 5.3. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhwdRETQ8t9D"
   },
   "source": [
    "Within the next segment we will explain the most prominent validation methods used in Human Activity Recognition. These are:\n",
    "\n",
    "- Train-Valid Split\n",
    "- k-Fold Cross-Validation\n",
    "- Per-Participant Cross-Validation\n",
    "- Cross-Participant Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQPUaRRE8AC9"
   },
   "source": [
    "### 5.3.1. Train-Valid Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8BS4Qf3l9BF"
   },
   "source": [
    "The train-valid split is one of the most basic validation method, which you  already did yourself. Instead of varying the validation set and getting a more holistic view, we define it to be a set part of the data. As mentioned above there are multiple ways how to do so. For simplicity purposes, we chose to use a subject-wise split. Within the next task you will be asked to train your network using the `train` data and obtain predictions on the `valid` data. We do not ask you to define the training loop again and allow you to use the built-in `train` function of the DL-ARC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOcMYLvTl9BF"
   },
   "source": [
    "#### Task 2: Implementing the train-valid split validation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsRp7hDNl9BF"
   },
   "source": [
    "1. As you already defined the train and valid dataset you can go ahead and apply a sliding window on top of both datasets. You can use the predefined method `apply_sliding_window()`, which is part of the DL-ARC pipeline, to do so. It is already be imported for you. We will give you hints on what to pass the method.\n",
    "2. Using the windowed features and labels of both the train and valid set to train a model and obtain validation results. You can use `train` function of the DL-ARC pipeline. It is already imported for you. As this is quite a complex task, we will give you hints along the way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61uZSoSdl9BG",
    "outputId": "3de72390-4372-46d3-f5ac-28ce1c71dfd7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
    "from model.train import train\n",
    "from model.DeepConvLSTM import DeepConvLSTM\n",
    "from data_processing.sliding_window import apply_sliding_window\n",
    "\n",
    "# needed for saving results\n",
    "log_date = time.strftime('%Y%m%d')\n",
    "log_timestamp = time.strftime('%H%M%S')\n",
    "\n",
    "# split the data into train and validation data\n",
    "train_data = train_valid_data[train_valid_data.subject_id == 0]\n",
    "valid_data = train_valid_data[train_valid_data.subject_id == 1]\n",
    "\n",
    "print(train_data.shape, valid_data.shape)\n",
    "\n",
    "# apply the sliding window on top of both the train and validation data; use the \"apply_sliding_window\" function\n",
    "# found in data_processing.sliding_window\n",
    "X_train, y_train = apply_sliding_window(train_data.iloc[:, :-1], train_data.iloc[:, -1],\n",
    "                                        sliding_window_size=config['sw_length'],\n",
    "                                        unit=config['sw_unit'],\n",
    "                                        sampling_rate=config['sampling_rate'],\n",
    "                                        sliding_window_overlap=config['sw_overlap'],\n",
    "                                        )\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "X_valid, y_valid = apply_sliding_window(valid_data.iloc[:, :-1], valid_data.iloc[:, -1],\n",
    "                                        sliding_window_size=config['sw_length'],\n",
    "                                        unit=config['sw_unit'],\n",
    "                                        sampling_rate=config['sampling_rate'],\n",
    "                                        sliding_window_overlap=config['sw_overlap'],\n",
    "                                        )\n",
    "\n",
    "print(X_valid.shape, y_valid.shape)\n",
    "\n",
    "# (optional) omit the first feature column (subject_identifier) from the train and validation dataset\n",
    "# you can do it if you want to as it is not a useful feature\n",
    "X_train, X_valid = X_train[:, :, 1:], X_valid[:, :, 1:]\n",
    "\n",
    "# within the config file, set the parameters 'window_size' and 'nb_channels' accordingly\n",
    "# window_size = size of the sliding window in units\n",
    "# nb_channels = number of feature channels\n",
    "config['window_size'] = X_train.shape[1]\n",
    "config['nb_channels'] = X_train.shape[2]\n",
    "\n",
    "# define the network to be a DeepConvLSTM object; can be imported from model.DeepConvLSTM\n",
    "# pass it the config object\n",
    "net = DeepConvLSTM(config=config)\n",
    "\n",
    "# convert the features of the train and validation to float32 and labels to uint8 for GPU compatibility \n",
    "X_train, y_train = X_train.astype(np.float32), y_train.astype(np.uint8)\n",
    "X_valid, y_valid = X_valid.astype(np.float32), y_valid.astype(np.uint8)\n",
    "\n",
    "# feed the datasets into the train function; can be imported from model.train\n",
    "train_valid_net, val_output, train_output = train(X_train, y_train, X_valid, y_valid,\n",
    "                                                  network=net, \n",
    "                                                  config=config, \n",
    "                                                  log_date=log_date,\n",
    "                                                  log_timestamp=log_timestamp)\n",
    "\n",
    "# the next bit prints out your results if you did everything correctly\n",
    "cls = np.array(range(config['nb_classes']))\n",
    "\n",
    "print('\\nVALIDATION RESULTS: ')\n",
    "print(\"\\nAvg. Accuracy: {0}\".format(jaccard_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "print(\"Avg. Precision: {0}\".format(precision_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "print(\"Avg. Recall: {0}\".format(recall_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "print(\"Avg. F1: {0}\".format(f1_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "\n",
    "print(\"\\nVALIDATION RESULTS (PER CLASS): \")\n",
    "print(\"\\nAccuracy:\")\n",
    "for i, rslt in enumerate(jaccard_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "print(\"\\nPrecision:\")\n",
    "for i, rslt in enumerate(precision_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "print(\"\\nRecall:\")\n",
    "for i, rslt in enumerate(recall_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "print(\"\\nF1:\")\n",
    "for i, rslt in enumerate(f1_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "\n",
    "print(\"\\nGENERALIZATION GAP ANALYSIS: \")\n",
    "print(\"\\nTrain-Val-Accuracy Difference: {0}\".format(jaccard_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                  jaccard_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "print(\"Train-Val-Precision Difference: {0}\".format(precision_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                   precision_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "print(\"Train-Val-Recall Difference: {0}\".format(recall_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                recall_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "print(\"Train-Val-F1 Difference: {0}\".format(f1_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                            f1_score(val_output[:, 1], val_output[:, 0], average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BepnQk8l9BH"
   },
   "source": [
    "### 5.3.2. K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OlOGi55l9BJ"
   },
   "source": [
    "The k-fold cross-validation is the most popular form of cross-validation. Instead of only splitting our data once into a train and validation dataset, like we did in the previous validation method, we take the average of k different train-valid splits. To do so we take our concatenated version of the train and validation set and split it into k equal-sized chunks of data. A so-called fold is now that we train our network using all but one of these chunks of data and validate it using the chunk we excluded (thus being unseen data). The process is repeated k-times, i.e. k-folds, so that each chunk of data is the validation dataset exactly once. Note that with each fold, the network needs to be reinitialized, i.e. trained from scratch, to ensure that it is not predicting already seen data.\n",
    "\n",
    "\n",
    "**Note:** It is recommended to use stratified k-fold cross-validation, i.e. each of the k chunks of data has the same distribution of labels as the original full dataset. This avoids the risk, especially for unbalanced datasets, of having certain labels missing within the train dataset, which would cause the validation process to break. Nevertheless, as also stated above, stratification requires shuffeling and thus one should always first apply the sliding window before applying the split.\n",
    "\n",
    "The next task will lead you through the implementation of the k-fold cross-validation loop. In order to chunk your data and also apply stratification, we recommend you to use the scikit-learn helper object for stratified k-fold cross-validation called `StratifiedKFold`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sK2SCNYl9BJ"
   },
   "source": [
    "#### Task 3: Implementing the k-fold CV loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iirq5yFll9BK"
   },
   "source": [
    "1. Define the scikit-learn helper object for stratified k-fold cross-validation called `StratifiedKFold`. It is already imported for you. We will also give you hints what to pass it as arguments\n",
    "2. Apply the `apply_sliding_window()` function on top of the `train_valid_data` object which you previously defined.\n",
    "3. Define the k-fold loop; use the `split()` function of the `StratifiedKFold` object to obtain indeces to split the `train_valid_data`\n",
    "4. Having split the data, run the train function with it and add up obtained results to the accumulated result objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wnh-tBGAl9BK",
    "outputId": "dc623f6d-60f1-443d-b51e-6808cf5c026b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# number of splits, i.e. folds\n",
    "config['splits_kfold'] = 10\n",
    "\n",
    "# needed for saving results\n",
    "log_date = time.strftime('%Y%m%d')\n",
    "log_timestamp = time.strftime('%H%M%S')\n",
    "\n",
    "# define the stratified k-fold object; it is already imported for you\n",
    "# pass it the number of splits, i.e. folds, and seed as well as set shuffling to true\n",
    "skf = StratifiedKFold(n_splits=config['splits_kfold'],\n",
    "                      random_state=config['seed']\n",
    "                      )\n",
    "    \n",
    "print(train_valid_data.shape)\n",
    "\n",
    "# apply the sliding window on top of both the train_valid_data; use the \"apply_sliding_window\" function\n",
    "# found in data_processing.sliding_window\n",
    "X_train_valid, y_train_valid = apply_sliding_window(train_valid_data.iloc[:, :-1], train_valid_data.iloc[:, -1],\n",
    "                                                    sliding_window_size=config['sw_length'],\n",
    "                                                    unit=config['sw_unit'],\n",
    "                                                    sampling_rate=config['sampling_rate'],\n",
    "                                                    sliding_window_overlap=config['sw_overlap'])\n",
    "\n",
    "print(X_train_valid.shape, y_train_valid.shape)\n",
    "\n",
    "# (optional) omit the first feature column (subject_identifier) from the train _valid_data\n",
    "# you can do it if you want to as it is not a useful feature\n",
    "X_train_valid = X_train_valid[:, :, 1:]\n",
    "\n",
    "# result objects used for accumulating the scores across folds; add each fold result to these objects so that they\n",
    "# are averaged at the end of the k-fold loop\n",
    "kfold_accuracy = np.zeros(config['nb_classes'])\n",
    "kfold_precision = np.zeros(config['nb_classes'])\n",
    "kfold_recall = np.zeros(config['nb_classes'])\n",
    "kfold_f1 = np.zeros(config['nb_classes'])\n",
    "    \n",
    "kfold_accuracy_gap = 0\n",
    "kfold_precision_gap = 0\n",
    "kfold_recall_gap = 0\n",
    "kfold_f1_gap = 0\n",
    "\n",
    "# k-fold validation loop; for each loop iteration return fold identifier and indeces which can be used to split\n",
    "# the train + valid data into train and validation data according to the current fold\n",
    "for j, (train_index, valid_index) in enumerate(skf.split(X_train_valid, y_train_valid)):\n",
    "    print('\\nFold {0}/{1}'.format(j + 1, config['splits_kfold']))\n",
    "    \n",
    "    # split the data into train and validation data; to do so, use the indeces produces by the split function\n",
    "    X_train, X_valid = X_train_valid[train_index], X_train_valid[valid_index]\n",
    "    y_train, y_valid = y_train_valid[train_index], y_train_valid[valid_index]\n",
    "    \n",
    "    # within the config file, set the parameters 'window_size' and 'nb_channels' accordingly\n",
    "    # window_size = size of the sliding window in units\n",
    "    # nb_channels = number of feature channels\n",
    "    config['window_size'] = X_train.shape[1]\n",
    "    config['nb_channels'] = X_train.shape[2]\n",
    "    \n",
    "    # define the network to be a DeepConvLSTM object; can be imported from model.DeepConvLSTM\n",
    "    # pass it the config object\n",
    "    net = DeepConvLSTM(config=config)\n",
    "    \n",
    "    # convert the features of the train and validation to float32 and labels to uint8 for GPU compatibility \n",
    "    X_train, y_train,  = X_train.astype(np.float32), y_train.astype(np.uint8)\n",
    "    X_valid, y_valid = X_valid.astype(np.float32), y_valid.astype(np.uint8)\n",
    "    \n",
    "    # feed the datasets into the train function; can be imported from model.train\n",
    "    kfold_net, val_output, train_output = train(X_train, y_train, X_valid, y_valid, network=net, config=config,\n",
    "                                                log_date=log_date, log_timestamp=log_timestamp)\n",
    "        \n",
    "    # in the following validation and train evaluation metrics are calculated\n",
    "    cls = np.array(range(config['nb_classes']))\n",
    "    val_accuracy = jaccard_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "    val_precision = precision_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "    val_recall = recall_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "    val_f1 = f1_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "    train_accuracy = jaccard_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "    train_precision = precision_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "    train_recall = recall_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "    train_f1 = f1_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "    \n",
    "    # add up the fold results\n",
    "    kfold_accuracy += val_accuracy\n",
    "    kfold_precision += val_precision\n",
    "    kfold_recall += val_recall\n",
    "    kfold_f1 += val_f1\n",
    "\n",
    "    # add up the generalization gap results\n",
    "    kfold_accuracy_gap += train_accuracy - val_accuracy\n",
    "    kfold_precision_gap += train_precision - val_precision\n",
    "    kfold_recall_gap += train_recall - val_recall\n",
    "    kfold_f1_gap += train_f1 - val_f1\n",
    "    \n",
    "# the next bit prints out the average results across folds if you did everything correctly\n",
    "print(\"\\nK-FOLD VALIDATION RESULTS: \")\n",
    "print(\"Accuracy: {0}\".format(np.mean(kfold_accuracy / config['splits_kfold'])))\n",
    "print(\"Precision: {0}\".format(np.mean(kfold_precision / config['splits_kfold'])))\n",
    "print(\"Recall: {0}\".format(np.mean(kfold_recall / config['splits_kfold'])))\n",
    "print(\"F1: {0}\".format(np.mean(kfold_f1 / config['splits_kfold'])))\n",
    "    \n",
    "print(\"\\nVALIDATION RESULTS (PER CLASS): \")\n",
    "print(\"\\nAccuracy:\")\n",
    "for i, rslt in enumerate(kfold_accuracy / config['splits_kfold']):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "print(\"\\nPrecision:\")\n",
    "for i, rslt in enumerate(kfold_precision / config['splits_kfold']):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "print(\"\\nRecall:\")\n",
    "for i, rslt in enumerate(kfold_recall / config['splits_kfold']):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "print(\"\\nF1:\")\n",
    "for i, rslt in enumerate(kfold_f1 / config['splits_kfold']):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    \n",
    "print(\"\\nGENERALIZATION GAP ANALYSIS: \")\n",
    "print(\"\\nAccuracy: {0}\".format(kfold_accuracy_gap / config['splits_kfold']))\n",
    "print(\"Precision: {0}\".format(kfold_precision_gap / config['splits_kfold']))\n",
    "print(\"Recall: {0}\".format(kfold_recall_gap / config['splits_kfold']))\n",
    "print(\"F1: {0}\".format(kfold_f1_gap / config['splits_kfold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ifmKA-Dl9BK"
   },
   "source": [
    "### 5.3.3. Per-participant Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3XIxq9rl9BK"
   },
   "source": [
    "Per-participant cross-validation validates the predictive perfomance of each subject individually. This means that for each subject contained in the dataset, one separate validation loop is run. Usually, this done by applying a stratified shuffle split, i.e. multiple stratified train-valid splits with each time randomly shuffled records, of multiple rounds per subject. The per-participant evaluation is a great validation method for judging whether your model is able to learn to predict the activities of a single participant. Unfortunately, it does not give you any clue whether it would be able to learn a general model which is able to correctly predict activities of multiple participants simultaneously\n",
    "\n",
    "**Note:** Using the stratified shuffle split, shuffeling is inherently required for the per-participant cross-validation. As mentioned above, when shuffeling, one must always first apply the sliding window before applying the split.\n",
    "\n",
    "The next task will lead you through the implementation of the per-participant cross-validation loop. In order to perform the stratified shuffle splits, we recommend you to use the scikit-learn helper object called `StratifiedShuffleSplit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poAFuqWyl9BL"
   },
   "source": [
    "#### Task 4: Implementing the per-participant CV loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_nG_WVcl9BL"
   },
   "source": [
    "1. Define a for loop which iterates over the identifiers of all subjects\n",
    "2. Within the loop define the `StratifiedShuffleSplit` object. It is already imported for you.\n",
    "3. Define the subject data by filtering the `train_valid_data` for the current subject.\n",
    "4. Apply the `apply_sliding_window()` function on top of the filtered subject data.\n",
    "5. Define the stratified shuffle split loop; use the `split` function of the `StratifiedShuffleSplit` object to obtain indeces to split the current subject data. \n",
    "6. Using the split data, run the train function and add up obtained results on the validation split to the accumulated result objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MtVQzmLl9BL",
    "outputId": "fd3cb1db-7eeb-4d81-c1cb-48ab29f8eb27"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "# size of the train portion within each split and number of splits per subject\n",
    "config['size_sss'] = 0.6\n",
    "config['splits_sss'] = 10\n",
    "\n",
    "# needed for saving results\n",
    "log_date = time.strftime('%Y%m%d')\n",
    "log_timestamp = time.strftime('%H%M%S')\n",
    "\n",
    "# iterate over all subjects\n",
    "for i, sbj in enumerate(np.unique(train_valid_data.iloc[:, 0])):\n",
    "    print('\\n VALIDATING FOR SUBJECT {0} OF {1}'.format(int(sbj) + 1, int(np.max(train_valid_data.iloc[:, 0])) + 1))\n",
    "    \n",
    "    # define the stratified shuffle split object for the current subject\n",
    "    # pass it the size of the train portion of the split, number of splits and seed\n",
    "    sss = StratifiedShuffleSplit(train_size=config['size_sss'],\n",
    "                                 n_splits=config['splits_sss'],\n",
    "                                 random_state=config['seed'])\n",
    "    \n",
    "    # define the subject data by filtering the train + valid dataset for the identifier of the current subject \n",
    "    subject_data = train_valid_data[train_valid_data.iloc[:, 0] == sbj]\n",
    "    \n",
    "    print(subject_data.shape)\n",
    "    \n",
    "    # apply the sliding window on top of both the subject data; use the \"apply_sliding_window\" function\n",
    "    # found in data_processing.sliding_window \n",
    "    X_subject, y_subject = apply_sliding_window(subject_data.iloc[:, :-1], subject_data.iloc[:, -1],\n",
    "                                                        sliding_window_size=config['sw_length'],\n",
    "                                                        unit=config['sw_unit'],\n",
    "                                                        sampling_rate=config['sampling_rate'],\n",
    "                                                        sliding_window_overlap=config['sw_overlap'])\n",
    "\n",
    "    print(X_subject.shape, y_subject.shape)\n",
    "    \n",
    "    # (optional) omit the first feature column (subject_identifier) from the subject data\n",
    "    # you can do it if you want to as it is not a useful feature\n",
    "    X_subject = X_subject[:, :, 1:]\n",
    "    \n",
    "    # result objects used for accumulating the scores across splits; add each split results to the objects so that\n",
    "    # they are averaged at the end of the stratified shuffle split loop\n",
    "    subject_accuracy = np.zeros(config['nb_classes'])\n",
    "    subject_precision = np.zeros(config['nb_classes'])\n",
    "    subject_recall = np.zeros(config['nb_classes'])\n",
    "    subject_f1 = np.zeros(config['nb_classes'])\n",
    "    \n",
    "    subject_accuracy_gap = 0\n",
    "    subject_precision_gap = 0\n",
    "    subject_recall_gap = 0\n",
    "    subject_f1_gap = 0\n",
    "    \n",
    "    # stratified shuffle split validation loop; for each loop iteration returns a split identifier and indeces \n",
    "    # which can be used to split the subject data into train and validation data according to the current split\n",
    "    for j, (train_index, test_index) in enumerate(sss.split(X_subject, y_subject)):\n",
    "        print('\\nSPLIT {0}/{1}'.format(j + 1, config['splits_sss']))\n",
    "\n",
    "        # split the data into train and validation data; to do so, use the indeces produces by the split function\n",
    "        X_train, X_valid = X_subject[train_index], X_subject[test_index]\n",
    "        y_train, y_valid = y_subject[train_index], y_subject[test_index]\n",
    "        \n",
    "        # within the config file, set the parameters 'window_size' and 'nb_channels' accordingly\n",
    "        # window_size = size of the sliding window in units\n",
    "        # nb_channels = number of feature channels\n",
    "        config['window_size'] = X_train.shape[1]\n",
    "        config['nb_channels'] = X_train.shape[2]\n",
    "        \n",
    "        # define the network to be a DeepConvLSTM object; can be imported from model.DeepConvLSTM\n",
    "        # pass it the config object\n",
    "        net = DeepConvLSTM(config=config)\n",
    "        \n",
    "        # convert the features of the train and validation to float32 and labels to uint8 for GPU compatibility \n",
    "        X_train, y_train,  = X_train.astype(np.float32), y_train.astype(np.uint8)\n",
    "        X_valid, y_valid = X_valid.astype(np.float32), y_valid.astype(np.uint8)\n",
    "        \n",
    "        # feed the datasets into the train function; can be imported from model.train\n",
    "        per_participant_net, val_output, train_output = train(X_train, y_train, X_valid, y_valid, network=net, \n",
    "                                                              config=config, log_date=log_date, \n",
    "                                                              log_timestamp=log_timestamp)\n",
    "        \n",
    "        # in the following validation and train evaluation metrics are calculated\n",
    "        cls = np.array(range(config['nb_classes']))\n",
    "        val_accuracy = jaccard_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "        val_precision = precision_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "        val_recall = recall_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "        val_f1 = f1_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "        train_accuracy = jaccard_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "        train_precision = precision_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "        train_recall = recall_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "        train_f1 = f1_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "        \n",
    "        # add up the fold results\n",
    "        subject_accuracy += val_accuracy\n",
    "        subject_precision += val_precision\n",
    "        subject_recall += val_recall\n",
    "        subject_f1 += val_f1\n",
    "\n",
    "        # add up train val gap evaluation\n",
    "        subject_accuracy_gap += train_accuracy - val_accuracy\n",
    "        subject_precision_gap += train_precision - val_precision\n",
    "        subject_recall_gap += train_recall - val_recall\n",
    "        subject_f1_gap += train_f1 - val_f1\n",
    "    \n",
    "    # the next bit prints out the average results per subject if you did everything correctly\n",
    "    print(\"\\nSUBJECT {0} VALIDATION RESULTS: \".format(int(sbj) + 1))\n",
    "    print(\"Accuracy: {0}\".format(np.mean(subject_accuracy / config['splits_sss'])))\n",
    "    print(\"Precision: {0}\".format(np.mean(subject_precision / config['splits_sss'])))\n",
    "    print(\"Recall: {0}\".format(np.mean(subject_recall / config['splits_sss'])))\n",
    "    print(\"F1: {0}\".format(np.mean(subject_f1 / config['splits_sss'])))\n",
    "    \n",
    "    print(\"\\nVALIDATION RESULTS (PER CLASS): \")\n",
    "    print(\"\\nAccuracy:\")\n",
    "    for i, rslt in enumerate(subject_accuracy / config['splits_sss']):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    print(\"\\nPrecision:\")\n",
    "    for i, rslt in enumerate(subject_precision / config['splits_sss']):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    print(\"\\nRecall:\")\n",
    "    for i, rslt in enumerate(subject_recall / config['splits_sss']):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    print(\"\\nF1:\")\n",
    "    for i, rslt in enumerate(subject_f1 / config['splits_sss']):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    \n",
    "    print(\"\\nGENERALIZATION GAP ANALYSIS: \")\n",
    "    print(\"\\nAccuracy: {0}\".format(subject_accuracy_gap / config['splits_sss']))\n",
    "    print(\"Precision: {0}\".format(subject_precision_gap / config['splits_sss']))\n",
    "    print(\"Recall: {0}\".format(subject_recall_gap / config['splits_sss']))\n",
    "    print(\"F1: {0}\".format(subject_f1_gap / config['splits_sss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI5ztrFyl9BL"
   },
   "source": [
    "### 5.3.4. Cross-Participant Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgc5fBYMl9BL"
   },
   "source": [
    "Cross-participant cross-validation, also known as Leave-One-Subject-Out (LOSO) cross-validation is the most complex, but also most expressive validation method one can apply when dealing with multi-subject data. In general, it can be seen as a variation of the k-fold cross-validation with k being the number of subjects. Within each fold, you train your network on the data of all but one subject and validate it on the left-out subject. The process is repeated as many times as there are subjects so that each subject becomes the validation set exaclty once. This way, each subject is treated as the unseen data at least once. \n",
    "\n",
    "Leaving one subject out each fold ensures that the overall evaluation of the algorithm does not overfit on subject-specific traits, i.e. how subjects performed the activities individually. It is therefore a great method to obtain a model which is good at predicting activities no matter which person performs them, i.e. a more general model!\n",
    "\n",
    "The next task will lead you through the implementation of the cross-participant cross-validation loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxmMLN71l9BM"
   },
   "source": [
    "#### Task 5: Implementing the cross-participant CV loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yaGaXanl9BM"
   },
   "source": [
    "1. Define a loop which iterates over the identifiers of all subjects\n",
    "2. Define the `train` data to be everything but the current subject's data and the `valid` data to be the current subject's data by filtering the `train_valid_data`.\n",
    "3. Apply the `apply_sliding_window()` function on top of the filtered datasets you just defined.\n",
    "4. Use both datasets to run the `train()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcUekkJal9BM",
    "outputId": "d15e14e8-d6c9-4230-96e6-1a1ea711b7a9"
   },
   "outputs": [],
   "source": [
    "# needed for saving results\n",
    "log_date = time.strftime('%Y%m%d')\n",
    "log_timestamp = time.strftime('%H%M%S')\n",
    "\n",
    "# iterate over all subjects\n",
    "for i, sbj in enumerate(np.unique(train_valid_data.iloc[:, 0])):\n",
    "    print('\\n VALIDATING FOR SUBJECT {0} OF {1}'.format(int(sbj) + 1, int(np.max(train_valid_data.iloc[:, 0])) + 1))\n",
    "    \n",
    "    # define the train data to be everything, but the data of the current subject\n",
    "    train_data = data[data.iloc[:, 0] != sbj]\n",
    "    # define the validation data to be the data of the current subject\n",
    "    valid_data = data[data.iloc[:, 0] == sbj]\n",
    "    \n",
    "    print(train_data.shape, valid_data.shape)\n",
    "    \n",
    "    # apply the sliding window on top of both the train and validation data; use the \"apply_sliding_window\" function\n",
    "    # found in data_processing.sliding_window\n",
    "    X_train, y_train = apply_sliding_window(train_data.iloc[:, :-1], train_data.iloc[:, -1],\n",
    "                                            sliding_window_size=config['sw_length'],\n",
    "                                            unit=config['sw_unit'],\n",
    "                                            sampling_rate=config['sampling_rate'],\n",
    "                                            sliding_window_overlap=config['sw_overlap'],\n",
    "                                            )\n",
    "\n",
    "    print(X_train.shape, y_train.shape)\n",
    "\n",
    "    X_valid, y_valid = apply_sliding_window(valid_data.iloc[:, :-1], valid_data.iloc[:, -1],\n",
    "                                            sliding_window_size=config['sw_length'],\n",
    "                                            unit=config['sw_unit'],\n",
    "                                            sampling_rate=config['sampling_rate'],\n",
    "                                            sliding_window_overlap=config['sw_overlap'],\n",
    "                                            )\n",
    "\n",
    "    print(X_valid.shape, y_valid.shape)\n",
    "\n",
    "    # (optional) omit the first feature column (subject_identifier) from the train and validation dataset\n",
    "    # you can do it if you want to as it is not a useful feature\n",
    "    X_train, X_valid = X_train[:, :, 1:], X_valid[:, :, 1:]\n",
    "    \n",
    "    # within the config file, set the parameters 'window_size' and 'nb_channels' accordingly\n",
    "    # window_size = size of the sliding window in units\n",
    "    # nb_channels = number of feature channels\n",
    "    config['window_size'] = X_train.shape[1]\n",
    "    config['nb_channels'] = X_train.shape[2]\n",
    "    \n",
    "    # define the network to be a DeepConvLSTM object; can be imported from model.DeepConvLSTM\n",
    "    # pass it the config object\n",
    "    net = DeepConvLSTM(config=config)\n",
    "\n",
    "    X_train, y_train,  = X_train.astype(np.float32), y_train.astype(np.uint8)\n",
    "    X_valid, y_valid = X_valid.astype(np.float32), y_valid.astype(np.uint8)\n",
    "\n",
    "    cross_participant_net, val_output, train_output = train(X_train, y_train, X_valid, y_valid,\n",
    "                                                            network=net, \n",
    "                                                            config=config, \n",
    "                                                            log_date=log_date,\n",
    "                                                            log_timestamp=log_timestamp)\n",
    "    \n",
    "    # the next bit prints out the average results per subject if you did everything correctly\n",
    "    cls = np.array(range(config['nb_classes']))\n",
    "    \n",
    "    print('\\nVALIDATION RESULTS FOR SUBJECT {0}: '.format(int(sbj) + 1))\n",
    "    print(\"\\nAvg. Accuracy: {0}\".format(jaccard_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "    print(\"Avg. Precision: {0}\".format(precision_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "    print(\"Avg. Recall: {0}\".format(recall_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "    print(\"Avg. F1: {0}\".format(f1_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "\n",
    "    print(\"\\nVALIDATION RESULTS (PER CLASS): \")\n",
    "    print(\"\\nAccuracy:\")\n",
    "    for i, rslt in enumerate(jaccard_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    print(\"\\nPrecision:\")\n",
    "    for i, rslt in enumerate(precision_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    print(\"\\nRecall:\")\n",
    "    for i, rslt in enumerate(recall_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    print(\"\\nF1:\")\n",
    "    for i, rslt in enumerate(f1_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "\n",
    "    print(\"\\nGENERALIZATION GAP ANALYSIS: \")\n",
    "    print(\"\\nTrain-Val-Accuracy Difference: {0}\".format(jaccard_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                      jaccard_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "    print(\"Train-Val-Precision Difference: {0}\".format(precision_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                       precision_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "    print(\"Train-Val-Recall Difference: {0}\".format(recall_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                    recall_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "    print(\"Train-Val-F1 Difference: {0}\".format(f1_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                f1_score(val_output[:, 1], val_output[:, 0], average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTFNUytOl9BM"
   },
   "source": [
    "## 4.5 Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXvP175Tl9BM"
   },
   "source": [
    "Now, after having implemented each of the validation techniques we want to get an unbiased view of how our trained algorithm perfoms on unseen data. To do so we use the testing set which we split off the original dataset within the first step of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4cmr2Tll9BM"
   },
   "source": [
    "### Task 6: Testing your trained networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHRkzAaul9BM"
   },
   "source": [
    "1. Apply the `apply_sliding_window()` function on top of the `test` data \n",
    "2. Using the `predict()` function of the DL-ARC GitHub to obtain results on the `test` data using each of the trained networks as input. The function is already imported for you.\n",
    "3. Which model does perform the best and why? Was this expected? Can you make out a reason why that is? \n",
    "4. What would you change about the pipeline we just created if your goal was to get the best predictions possible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9s6kwjul9BM",
    "outputId": "ab3d6e60-d953-44fc-8c1a-66643ea64774"
   },
   "outputs": [],
   "source": [
    "from model.train import predict\n",
    "\n",
    "\n",
    "X_test, y_test = apply_sliding_window(test_data.iloc[:, :-1], test_data.iloc[:, -1],\n",
    "                                      sliding_window_size=config['sw_length'],\n",
    "                                      unit=config['sw_unit'],\n",
    "                                      sampling_rate=config['sampling_rate'],\n",
    "                                      sliding_window_overlap=config['sw_overlap'],\n",
    "                                      )\n",
    "\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# (optional) omit the first feature column (subject_identifier) from the test dataset\n",
    "# you need to do it if you did so during training and validation!\n",
    "X_test = X_test[:, :, 1:]\n",
    "\n",
    "# the next lines will print out the test results for each of the trained networks\n",
    "print('COMPILED TEST RESULTS: ')\n",
    "print('\\nTest results (train-valid-split): ')\n",
    "predict(X_test, y_test, train_valid_net, config, log_date, log_timestamp)\n",
    "\n",
    "print('\\nTest results (k-fold): ')\n",
    "predict(X_test, y_test, kfold_net, config, log_date, log_timestamp)\n",
    "\n",
    "print('\\nTest results (per-participant): ')\n",
    "predict(X_test, y_test, per_participant_net, config, log_date, log_timestamp)\n",
    "\n",
    "print('\\nTest results (cross-participant): ')\n",
    "predict(X_test, y_test, cross_participant_net, config, log_date, log_timestamp)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of validation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}