{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Validation (& Testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a predicitive pipeline, there are a lot of hyperparameters which one can choose from. During validation we try to asses whether we have found a suitable of hyperparameters. The basic idea of any validation method is to split the training data into a two separate datasets, one of which is called the validation set. \n",
    "\n",
    "During training, the validation set is kept seperate from the training data and, once training is completed, is fed through the trained network in order to get predictions. This validation score will give you an intuition, whether the change in hyperparameters you have made has led to an improvement in predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this notebook we will introduce to you four popular methods on how to perform validation within the Human Activity Recognition community.\n",
    "\n",
    "These methods are:\n",
    "1. Train-valid split\n",
    "2. k-fold cross-validation\n",
    "3. Per-participant cross-validation\n",
    "4. Cross-participant cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solely relying and tuning based on the validation scores would inherit what is called overfitting. Overfitting means that your trained model would be too well optimized on the validation set and thus not general anymore resulting in bad prediction performance on unseen data. Assessing how your model would perform on completly unseen data is called testing. The test set is a separate dataset which is kept separate from the training and validation loop. It is only used to gain insights on the predictive performance of the model and must not (!) be used as a reference for tuning hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus begin by splitting the original dataset into two separate datasets, one used for the validation loop and one used for testing. There are multiple ways how to split the data into the two respective datasets, for example:\n",
    "\n",
    "- **Subject-wise:** split according to participants within the dataset. This means that we are reserving certain subjects to be included in the train + valid and test set respectively. For example, given that there are a total of 10 subjects, you could use 8 subjects for trainig + validation and 2 for testing.\n",
    "- **Percentage-wise:** state how large percentage-wise your train + valid and test dataset should be compared to the full dataset. For example, you could use 80% of your data for training and validation and 20% for testing. The two splits can then be chosen to be stratified, meaning that the relative label distribution within each of the two dataset is kept the same as in the full dataset.\n",
    "- **Record-wise:** state how many records should be in your train + valid and test dataset should be contained, i.e. define a cutoff point. For example, given that there are 1 million records in your full dataset, you could have the first 800 thousand records to be contained in train + valid dataset and the remaining 200 thousand records to be contained in the test dataset.\n",
    "\n",
    "For this notebook we will be using the data of three subjects instead of just one. For simplicity purposes we will use the first two subjects for training + validation and the remaining subject for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING:** splitting your dataset record-wise, percentage-wise and/ or applying shuffling during splitting (which is needed for stratified splits) will destroy your time-dependencies among the data records. To minimize this effect, apply a sliding window on top of your data before splitting. This way, time-dependencies will at least be preserved within the windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING FOR COLAB USERS:**  \n",
    "- Set use_colab to True if you are accessing this notebook\n",
    "- Change your runtime time to GPU by clicking: Runtime -> Change runtime type -> Dropdown -> GPU -> Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "use_colab = False\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if use_colab:\n",
    "    # clone package repository\n",
    "    !git clone https://github.com/mariusbock/dl-for-har.git\n",
    "\n",
    "    # navigate to dl-for-har directory\n",
    "    %cd dl-for-har/\n",
    "\n",
    "    # get modifications made on the repo\n",
    "    !git pull origin master\n",
    "else:\n",
    "    os.chdir(module_path)\n",
    "    \n",
    "# this statement is needed so that we can use the methods of the DL-ARC pipeline\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Loading the dataset and splitting off the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the 'rwhar_3sbjs_data.csv' as done in previous notebooks.\n",
    "2. Split the loaded dataset into a train_valid and test dataset based on the split criteria defined above, i.e. subjects with identifiers 0 and 1 to be in the train_valid and the subject with identifier 2 to be in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subject_id     acc_x      acc_y     acc_z activity_label\n",
      "0           0  0.378284  10.168175  0.847547    climbing_up\n",
      "1           0  0.383671  10.172364  0.849942    climbing_up\n",
      "2           0  0.372298  10.181941  0.859518    climbing_up\n",
      "3           0  0.342969  10.170568  0.834379    climbing_up\n",
      "4           0  0.319626  10.159795  0.818817    climbing_up\n",
      "(430456, 5) (228804, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# folder where the data is located and name of dataset\n",
    "data_folder = 'data'\n",
    "dataset = 'rwhar_3sbjs_data.csv'\n",
    "\n",
    "# read in the data using the pandas read_csv function; define the header as done previously\n",
    "data = pd.read_csv(os.path.join(data_folder, dataset), names=['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity_label'])\n",
    "print(data.head())\n",
    "\n",
    "# label dictionary needed for converting the string label names to integers \n",
    "label_dict = {\n",
    "    'climbing_down': 0,\n",
    "    'climbing_up': 1,\n",
    "    'jumping': 2,\n",
    "    'lying': 3,\n",
    "    'running': 4,\n",
    "    'sitting': 5,\n",
    "    'standing': 6,\n",
    "    'walking': 7\n",
    "}\n",
    "\n",
    "# all activity names\n",
    "class_names = ['climbing_down', 'climbing_up', 'jumping', 'lying', 'running', 'sitting', 'standing', 'walking']\n",
    "\n",
    "# replace values within the 'activity_label' column using the label_dict (use .replace())\n",
    "data['activity_label'] = data['activity_label'].replace(label_dict) \n",
    "\n",
    "# define the train + valid data to be the data of the first two subjects\n",
    "train_valid_data = data[data.subject_id <= 1]\n",
    "# define the test data to be the data of the third subject\n",
    "test_data = data[data.subject_id == 2]\n",
    "\n",
    "print(train_valid_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the next part we will go step by step explaining all the different validation methods there are when dealing with Human Activity Recognition. Note that all these methods are featured in the DL-ARC and can be interchanged within the main script. Throughout all experiments, we will use the configurations as listed below. As you already know some of these parameters from the training notebook, feel free to adjust the configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # sliding window settings\n",
    "    'sw_length': 50,\n",
    "    'sw_unit': 'units',\n",
    "    'sampling_rate': 50,\n",
    "    'sw_overlap': 30,\n",
    "    # network settings\n",
    "    'nb_conv_blocks': 2,\n",
    "    'conv_block_type': 'normal',\n",
    "    'nb_filters': 64,\n",
    "    'filter_width': 11,\n",
    "    'nb_units_lstm': 128,\n",
    "    'nb_layers_lstm': 1,\n",
    "    'drop_prob': 0.5,\n",
    "    # training settings\n",
    "    'epochs': 10,\n",
    "    'batch_size': 100,\n",
    "    'loss': 'cross_entropy',\n",
    "    'use_weights': True,\n",
    "    'weights_init': 'xavier_uniform',\n",
    "    'optimizer': 'adam',\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-6,\n",
    "    # general settings (do not alter!)\n",
    "    'batch_norm': False,\n",
    "    'dilation': 1,\n",
    "    'pooling': False,\n",
    "    'pool_type': 'max',\n",
    "    'pool_kernel_width': 2,\n",
    "    'reduce_layer': False,\n",
    "    'reduce_layer_output': 10,\n",
    "    'nb_classes': 8,\n",
    "    'seed': 1,\n",
    "    'gpu': 'cuda:0',\n",
    "    'verbose': False,\n",
    "    'print_freq': 10,\n",
    "    'save_gradient_plot': False,\n",
    "    'print_counts': False,\n",
    "    'adj_lr': False,\n",
    "    'adj_lr_patience': 5,\n",
    "    'early_stopping': False,\n",
    "    'es_patience': 5,\n",
    "    'save_test_preds': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Train-Valid Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic validation method is to split your train + validation data, like for the test set, into two separate datasets. As mentioned above there are multiple ways how to do so. For simplicity purposes, we will use a subject-wise split within this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Implementing the train-valid validation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Split the train_valid dataset into the train and valid dataset. The former shall contain all data related to the subject with the identifier 0 and the latter shall contain all data related to the subject with the identifier 1.\n",
    "2. Apply a sliding window on top of both datasets. You can use the predefined method 'apply_sliding_window', which is part of the DL-ARC pipeline, to do so. The funtion needs both features and labels as input and will return you a windowed version of both.\n",
    "3. Using the windowed features and labels of both the train and valid set to train a model and obtain validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214659, 5) (168565, 5)\n",
      "(6132, 50, 4) (6132,)\n",
      "(4815, 50, 4) (4815,)\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.06163435 0.99674902 6.33471074 0.85930493 0.88103448 0.81716418\n",
      " 0.84046053 0.844163  ]\n",
      "EPOCH: 1/10 Train Loss: 2.2140 Train Acc: 0.0332 Train Prec: 0.0612 Train Rcll: 0.1427 Train F1: 0.0593 Val Loss: 2.0291 Val Acc: 0.0272 Val Prec: 0.0793 Val Rcll: 0.1571 Val F1: 0.0497\n",
      "EPOCH: 2/10 Train Loss: 1.9672 Train Acc: 0.1804 Train Prec: 0.2147 Train Rcll: 0.3633 Train F1: 0.2338 Val Loss: 1.9261 Val Acc: 0.0697 Val Prec: 0.2430 Val Rcll: 0.2800 Val F1: 0.1132\n",
      "EPOCH: 3/10 Train Loss: 1.8544 Train Acc: 0.1780 Train Prec: 0.2395 Train Rcll: 0.3610 Train F1: 0.2351 Val Loss: 1.9152 Val Acc: 0.0606 Val Prec: 0.2080 Val Rcll: 0.1902 Val F1: 0.1007\n",
      "EPOCH: 4/10 Train Loss: 1.6585 Train Acc: 0.1805 Train Prec: 0.1977 Train Rcll: 0.3588 Train F1: 0.2365 Val Loss: 1.9969 Val Acc: 0.0677 Val Prec: 0.2455 Val Rcll: 0.1730 Val F1: 0.1183\n",
      "EPOCH: 5/10 Train Loss: 1.4593 Train Acc: 0.3044 Train Prec: 0.5150 Train Rcll: 0.4700 Train F1: 0.4038 Val Loss: 1.9790 Val Acc: 0.2537 Val Prec: 0.4354 Val Rcll: 0.4117 Val F1: 0.3268\n",
      "EPOCH: 6/10 Train Loss: 1.3210 Train Acc: 0.3688 Train Prec: 0.5165 Train Rcll: 0.4946 Train F1: 0.4285 Val Loss: 1.9783 Val Acc: 0.1738 Val Prec: 0.3201 Val Rcll: 0.3418 Val F1: 0.2355\n",
      "EPOCH: 7/10 Train Loss: 1.2478 Train Acc: 0.4336 Train Prec: 0.6093 Train Rcll: 0.5284 Train F1: 0.5125 Val Loss: 1.8809 Val Acc: 0.3724 Val Prec: 0.4558 Val Rcll: 0.5400 Val F1: 0.4287\n",
      "EPOCH: 8/10 Train Loss: 1.1567 Train Acc: 0.4593 Train Prec: 0.6476 Train Rcll: 0.5649 Train F1: 0.5475 Val Loss: 1.7966 Val Acc: 0.3845 Val Prec: 0.5013 Val Rcll: 0.5455 Val F1: 0.4395\n",
      "EPOCH: 9/10 Train Loss: 1.0904 Train Acc: 0.5173 Train Prec: 0.7109 Train Rcll: 0.6310 Train F1: 0.5994 Val Loss: 1.7497 Val Acc: 0.3375 Val Prec: 0.4255 Val Rcll: 0.4662 Val F1: 0.3862\n",
      "EPOCH: 10/10 Train Loss: 1.0505 Train Acc: 0.5163 Train Prec: 0.7049 Train Rcll: 0.6292 Train F1: 0.5936 Val Loss: 1.6539 Val Acc: 0.3385 Val Prec: 0.4037 Val Rcll: 0.4600 Val F1: 0.3904\n",
      "\n",
      "VALIDATION RESULTS: \n",
      "\n",
      "Avg. Accuracy: 0.3384883145098192\n",
      "Avg. Precision: 0.403678243631984\n",
      "Avg. Recall: 0.46000598228049444\n",
      "Avg. F1: 0.39035049724245896\n",
      "\n",
      "VALIDATION RESULTS (PER CLASS): \n",
      "\n",
      "Accuracy:\n",
      "   climbing_down: 0.19761029411764705\n",
      "   climbing_up: 0.0\n",
      "   jumping: 0.9705882352941176\n",
      "   lying: 0.0\n",
      "   running: 0.9683544303797469\n",
      "   sitting: 0.47637362637362635\n",
      "   standing: 0.018743109151047408\n",
      "   walking: 0.07623682076236821\n",
      "\n",
      "Precision:\n",
      "   climbing_down: 0.22619673855865335\n",
      "   climbing_up: 0.0\n",
      "   jumping: 0.9924812030075187\n",
      "   lying: 0.0\n",
      "   running: 0.9913606911447084\n",
      "   sitting: 0.4790055248618785\n",
      "   standing: 0.3333333333333333\n",
      "   walking: 0.20704845814977973\n",
      "\n",
      "Recall:\n",
      "   climbing_down: 0.6099290780141844\n",
      "   climbing_up: 0.0\n",
      "   jumping: 0.9777777777777777\n",
      "   lying: 0.0\n",
      "   running: 0.9765957446808511\n",
      "   sitting: 0.9885974914481186\n",
      "   standing: 0.019473081328751432\n",
      "   walking: 0.10767468499427263\n",
      "\n",
      "F1:\n",
      "   climbing_down: 0.33000767459708363\n",
      "   climbing_up: 0.0\n",
      "   jumping: 0.9850746268656717\n",
      "   lying: 0.0\n",
      "   running: 0.9839228295819936\n",
      "   sitting: 0.6453293636025307\n",
      "   standing: 0.0367965367965368\n",
      "   walking: 0.14167294649585532\n",
      "\n",
      "GENERALIZATION GAP ANALYSIS: \n",
      "\n",
      "Train-Val-Accuracy Difference: 0.17781635998742595\n",
      "Train-Val-Precision Difference: 0.30124253507348164\n",
      "Train-Val-Recall Difference: 0.16922295887193234\n",
      "Train-Val-F1 Difference: 0.20326263094247554\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
    "from model.train import train\n",
    "from model.DeepConvLSTM import DeepConvLSTM\n",
    "from data_processing.sliding_window import apply_sliding_window\n",
    "\n",
    "# needed for saving results\n",
    "log_date = time.strftime('%Y%m%d')\n",
    "log_timestamp = time.strftime('%H%M%S')\n",
    "\n",
    "# split the data into train and validation data\n",
    "train_data = train_valid_data[train_valid_data.subject_id == 0]\n",
    "valid_data = train_valid_data[train_valid_data.subject_id == 1]\n",
    "\n",
    "print(train_data.shape, valid_data.shape)\n",
    "\n",
    "# apply the sliding window on top of both the train and validation data; use the \"apply_sliding_window\" function\n",
    "# found in data_processing.sliding_window\n",
    "X_train, y_train = apply_sliding_window(train_data.iloc[:, :-1], train_data.iloc[:, -1],\n",
    "                                        sliding_window_size=config['sw_length'],\n",
    "                                        unit=config['sw_unit'],\n",
    "                                        sampling_rate=config['sampling_rate'],\n",
    "                                        sliding_window_overlap=config['sw_overlap'],\n",
    "                                        )\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "X_valid, y_valid = apply_sliding_window(valid_data.iloc[:, :-1], valid_data.iloc[:, -1],\n",
    "                                        sliding_window_size=config['sw_length'],\n",
    "                                        unit=config['sw_unit'],\n",
    "                                        sampling_rate=config['sampling_rate'],\n",
    "                                        sliding_window_overlap=config['sw_overlap'],\n",
    "                                        )\n",
    "\n",
    "print(X_valid.shape, y_valid.shape)\n",
    "\n",
    "# omit the first feature column (subject_identifier) from the train and validation dataset\n",
    "X_train, X_valid = X_train[:, :, 1:], X_valid[:, :, 1:]\n",
    "\n",
    "# within the config file, set the parameters 'window_size' and 'nb_channels' accordingly\n",
    "# window_size = size of the sliding window in units\n",
    "# nb_channels = number of feature channels\n",
    "config['window_size'] = X_train.shape[1]\n",
    "config['nb_channels'] = X_train.shape[2]\n",
    "\n",
    "# define the network to be a DeepConvLSTM object; can be imported from model.DeepConvLSTM\n",
    "# pass it the config object\n",
    "net = DeepConvLSTM(config=config)\n",
    "\n",
    "# convert the features of the train and validation to float32 and labels to uint8 for GPU compatibility \n",
    "X_train, y_train = X_train.astype(np.float32), y_train.astype(np.uint8)\n",
    "X_valid, y_valid = X_valid.astype(np.float32), y_valid.astype(np.uint8)\n",
    "\n",
    "# feed the datasets into the train function; can be imported from model.train\n",
    "train_valid_net, val_output, train_output = train(X_train, y_train, X_valid, y_valid,\n",
    "                                                  network=net, \n",
    "                                                  config=config, \n",
    "                                                  log_date=log_date,\n",
    "                                                  log_timestamp=log_timestamp)\n",
    "\n",
    "# the next bit prints out your results if you did everything correctly\n",
    "cls = np.array(range(config['nb_classes']))\n",
    "\n",
    "print('\\nVALIDATION RESULTS: ')\n",
    "print(\"\\nAvg. Accuracy: {0}\".format(jaccard_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "print(\"Avg. Precision: {0}\".format(precision_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "print(\"Avg. Recall: {0}\".format(recall_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "print(\"Avg. F1: {0}\".format(f1_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "\n",
    "print(\"\\nVALIDATION RESULTS (PER CLASS): \")\n",
    "print(\"\\nAccuracy:\")\n",
    "for i, rslt in enumerate(jaccard_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "print(\"\\nPrecision:\")\n",
    "for i, rslt in enumerate(precision_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "print(\"\\nRecall:\")\n",
    "for i, rslt in enumerate(recall_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "print(\"\\nF1:\")\n",
    "for i, rslt in enumerate(f1_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "\n",
    "print(\"\\nGENERALIZATION GAP ANALYSIS: \")\n",
    "print(\"\\nTrain-Val-Accuracy Difference: {0}\".format(jaccard_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                  jaccard_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "print(\"Train-Val-Precision Difference: {0}\".format(precision_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                   precision_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "print(\"Train-Val-Recall Difference: {0}\".format(recall_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                recall_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "print(\"Train-Val-F1 Difference: {0}\".format(f1_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                            f1_score(val_output[:, 1], val_output[:, 0], average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-fold cross-validation is the most popular form of cross-validation. In it the train + valid dataset is split into k folds, i.e. k chunks of data. Having the data split the training process is repeated k times with each time having one fold as being the validation set and all other folds being the training set. Results are then averaged across folds.\n",
    "\n",
    "**Note:** It is recommended to use stratified folds, i.e. each fold has the same distribution of labels as the original full dataset. This avoids the risk, especially for unbalanced datasets, of having certain labels missing within the train dataset, which would cause the validation process to break as it would see unknown labels during prediction time. Nevertheless, as also stated above, when using stratifed splits, which inheritly require shuffling, one must always first apply the sliding window before applying the split. Doing so one can at least minimize the destroyed time-dependencies among records by at least maintaining them within each window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Implementing the k-fold CV loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define the stratified k-fold object.\n",
    "2. Apply the sliding window on top of the train + valid data and omit the subject identifier column\n",
    "3. Define the k-fold loop; use the split function of the stratified k-fold object to obtain indeces to split the train + valid data\n",
    "4. Run the train function and add up obtained results to the accumulated result objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(383224, 5)\n",
      "(10947, 50, 4) (10947,)\n",
      "\n",
      "Fold 1/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[0.95911215 1.77962428 5.35434783 0.77113338 1.02114428 0.75367197\n",
      " 0.76681196 0.76824704]\n",
      "EPOCH: 1/10 Train Loss: 2.1479 Train Acc: 0.1554 Train Prec: 0.2015 Train Rcll: 0.3085 Train F1: 0.2165 Val Loss: 1.8416 Val Acc: 0.1620 Val Prec: 0.2093 Val Rcll: 0.3145 Val F1: 0.2223\n",
      "EPOCH: 2/10 Train Loss: 1.8761 Train Acc: 0.2778 Train Prec: 0.4107 Train Rcll: 0.4185 Train F1: 0.3756 Val Loss: 1.5966 Val Acc: 0.2746 Val Prec: 0.4110 Val Rcll: 0.4090 Val F1: 0.3690\n",
      "EPOCH: 3/10 Train Loss: 1.6859 Train Acc: 0.4415 Train Prec: 0.5731 Train Rcll: 0.5648 Train F1: 0.5112 Val Loss: 1.3139 Val Acc: 0.4394 Val Prec: 0.5132 Val Rcll: 0.5544 Val F1: 0.5047\n",
      "EPOCH: 4/10 Train Loss: 1.4801 Train Acc: 0.4335 Train Prec: 0.5558 Train Rcll: 0.5480 Train F1: 0.5043 Val Loss: 1.1508 Val Acc: 0.4026 Val Prec: 0.5002 Val Rcll: 0.5019 Val F1: 0.4706\n",
      "EPOCH: 5/10 Train Loss: 1.2204 Train Acc: 0.4817 Train Prec: 0.5941 Train Rcll: 0.6160 Train F1: 0.5678 Val Loss: 0.9595 Val Acc: 0.4397 Val Prec: 0.5597 Val Rcll: 0.5756 Val F1: 0.5274\n",
      "EPOCH: 6/10 Train Loss: 1.0759 Train Acc: 0.5737 Train Prec: 0.6705 Train Rcll: 0.7014 Train F1: 0.6675 Val Loss: 0.8532 Val Acc: 0.5398 Val Prec: 0.6424 Val Rcll: 0.6718 Val F1: 0.6425\n",
      "EPOCH: 7/10 Train Loss: 0.9874 Train Acc: 0.4377 Train Prec: 0.5279 Train Rcll: 0.6049 Train F1: 0.5233 Val Loss: 0.9574 Val Acc: 0.4371 Val Prec: 0.5280 Val Rcll: 0.5934 Val F1: 0.5193\n",
      "EPOCH: 8/10 Train Loss: 0.9455 Train Acc: 0.4793 Train Prec: 0.5774 Train Rcll: 0.6463 Train F1: 0.5754 Val Loss: 0.8472 Val Acc: 0.4824 Val Prec: 0.5762 Val Rcll: 0.6370 Val F1: 0.5711\n",
      "EPOCH: 9/10 Train Loss: 0.8537 Train Acc: 0.6729 Train Prec: 0.7422 Train Rcll: 0.7709 Train F1: 0.7511 Val Loss: 0.6967 Val Acc: 0.6283 Val Prec: 0.7093 Val Rcll: 0.7369 Val F1: 0.7199\n",
      "EPOCH: 10/10 Train Loss: 0.7687 Train Acc: 0.6265 Train Prec: 0.7219 Train Rcll: 0.7192 Train F1: 0.7094 Val Loss: 0.6784 Val Acc: 0.6152 Val Prec: 0.7201 Val Rcll: 0.7158 Val F1: 0.7099\n",
      "\n",
      "Fold 2/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[0.95911215 1.77962428 5.35434783 0.77113338 1.02114428 0.75367197\n",
      " 0.76681196 0.76824704]\n",
      "EPOCH: 1/10 Train Loss: 2.2255 Train Acc: 0.2052 Train Prec: 0.2416 Train Rcll: 0.3525 Train F1: 0.2640 Val Loss: 1.9876 Val Acc: 0.2032 Val Prec: 0.2411 Val Rcll: 0.3499 Val F1: 0.2671\n",
      "EPOCH: 2/10 Train Loss: 2.0035 Train Acc: 0.2464 Train Prec: 0.2831 Train Rcll: 0.3964 Train F1: 0.3196 Val Loss: 1.7474 Val Acc: 0.2421 Val Prec: 0.2886 Val Rcll: 0.3976 Val F1: 0.3208\n",
      "EPOCH: 3/10 Train Loss: 1.8317 Train Acc: 0.3047 Train Prec: 0.4194 Train Rcll: 0.4452 Train F1: 0.3701 Val Loss: 1.5923 Val Acc: 0.3066 Val Prec: 0.3936 Val Rcll: 0.4405 Val F1: 0.3747\n",
      "EPOCH: 4/10 Train Loss: 1.6107 Train Acc: 0.3796 Train Prec: 0.4704 Train Rcll: 0.5065 Train F1: 0.4530 Val Loss: 1.3128 Val Acc: 0.3927 Val Prec: 0.5586 Val Rcll: 0.5228 Val F1: 0.4716\n",
      "EPOCH: 5/10 Train Loss: 1.4177 Train Acc: 0.3970 Train Prec: 0.5867 Train Rcll: 0.4914 Train F1: 0.4728 Val Loss: 1.2518 Val Acc: 0.3545 Val Prec: 0.4263 Val Rcll: 0.4560 Val F1: 0.4371\n",
      "EPOCH: 6/10 Train Loss: 1.1633 Train Acc: 0.5055 Train Prec: 0.6721 Train Rcll: 0.6116 Train F1: 0.6051 Val Loss: 1.0837 Val Acc: 0.4618 Val Prec: 0.6565 Val Rcll: 0.5746 Val F1: 0.5678\n",
      "EPOCH: 7/10 Train Loss: 1.1360 Train Acc: 0.4991 Train Prec: 0.6785 Train Rcll: 0.6017 Train F1: 0.5911 Val Loss: 1.0347 Val Acc: 0.4513 Val Prec: 0.6584 Val Rcll: 0.5648 Val F1: 0.5486\n",
      "EPOCH: 8/10 Train Loss: 1.0012 Train Acc: 0.6354 Train Prec: 0.7198 Train Rcll: 0.7331 Train F1: 0.7206 Val Loss: 0.9025 Val Acc: 0.5570 Val Prec: 0.6906 Val Rcll: 0.6653 Val F1: 0.6631\n",
      "EPOCH: 9/10 Train Loss: 0.9412 Train Acc: 0.5938 Train Prec: 0.7197 Train Rcll: 0.6906 Train F1: 0.6855 Val Loss: 0.9012 Val Acc: 0.5261 Val Prec: 0.6919 Val Rcll: 0.6302 Val F1: 0.6290\n",
      "EPOCH: 10/10 Train Loss: 0.9122 Train Acc: 0.5251 Train Prec: 0.6297 Train Rcll: 0.6333 Train F1: 0.5924 Val Loss: 0.9163 Val Acc: 0.5026 Val Prec: 0.6375 Val Rcll: 0.6159 Val F1: 0.5977\n",
      "\n",
      "Fold 3/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[0.95911215 1.77962428 5.35434783 0.77113338 1.02114428 0.75367197\n",
      " 0.76681196 0.76824704]\n",
      "EPOCH: 1/10 Train Loss: 2.1361 Train Acc: 0.1132 Train Prec: 0.2375 Train Rcll: 0.2726 Train F1: 0.1725 Val Loss: 1.9236 Val Acc: 0.1178 Val Prec: 0.2712 Val Rcll: 0.2726 Val F1: 0.1811\n",
      "EPOCH: 2/10 Train Loss: 1.8810 Train Acc: 0.2982 Train Prec: 0.6235 Train Rcll: 0.4454 Train F1: 0.3809 Val Loss: 1.4536 Val Acc: 0.3003 Val Prec: 0.4618 Val Rcll: 0.4457 Val F1: 0.3872\n",
      "EPOCH: 3/10 Train Loss: 1.5228 Train Acc: 0.4025 Train Prec: 0.5251 Train Rcll: 0.5294 Train F1: 0.4853 Val Loss: 1.0978 Val Acc: 0.4236 Val Prec: 0.5666 Val Rcll: 0.5376 Val F1: 0.4951\n",
      "EPOCH: 4/10 Train Loss: 1.3053 Train Acc: 0.4599 Train Prec: 0.6214 Train Rcll: 0.5672 Train F1: 0.5332 Val Loss: 1.0319 Val Acc: 0.4702 Val Prec: 0.6830 Val Rcll: 0.5712 Val F1: 0.5503\n",
      "EPOCH: 5/10 Train Loss: 1.1422 Train Acc: 0.5122 Train Prec: 0.6608 Train Rcll: 0.6215 Train F1: 0.5960 Val Loss: 0.9442 Val Acc: 0.5165 Val Prec: 0.6549 Val Rcll: 0.6159 Val F1: 0.5993\n",
      "EPOCH: 6/10 Train Loss: 1.0259 Train Acc: 0.5055 Train Prec: 0.6637 Train Rcll: 0.5998 Train F1: 0.5988 Val Loss: 0.8959 Val Acc: 0.5114 Val Prec: 0.6712 Val Rcll: 0.6078 Val F1: 0.6101\n",
      "EPOCH: 7/10 Train Loss: 0.9634 Train Acc: 0.6429 Train Prec: 0.7161 Train Rcll: 0.7482 Train F1: 0.7244 Val Loss: 0.7454 Val Acc: 0.6278 Val Prec: 0.7146 Val Rcll: 0.7295 Val F1: 0.7109\n",
      "EPOCH: 8/10 Train Loss: 0.8691 Train Acc: 0.6212 Train Prec: 0.6902 Train Rcll: 0.7479 Train F1: 0.7090 Val Loss: 0.7007 Val Acc: 0.5909 Val Prec: 0.6682 Val Rcll: 0.7213 Val F1: 0.6820\n",
      "EPOCH: 9/10 Train Loss: 0.7827 Train Acc: 0.6717 Train Prec: 0.7286 Train Rcll: 0.7826 Train F1: 0.7511 Val Loss: 0.6076 Val Acc: 0.6586 Val Prec: 0.7236 Val Rcll: 0.7705 Val F1: 0.7434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/10 Train Loss: 0.6770 Train Acc: 0.7498 Train Prec: 0.8829 Train Rcll: 0.8386 Train F1: 0.8418 Val Loss: 0.5508 Val Acc: 0.7152 Val Prec: 0.8524 Val Rcll: 0.8185 Val F1: 0.8231\n",
      "\n",
      "Fold 4/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[0.95911215 1.77962428 5.33116883 0.77161654 1.02114428 0.75367197\n",
      " 0.76681196 0.76824704]\n",
      "EPOCH: 1/10 Train Loss: 2.1116 Train Acc: 0.1729 Train Prec: 0.2938 Train Rcll: 0.3334 Train F1: 0.2430 Val Loss: 1.6741 Val Acc: 0.1585 Val Prec: 0.2180 Val Rcll: 0.3110 Val F1: 0.2250\n",
      "EPOCH: 2/10 Train Loss: 1.7682 Train Acc: 0.3095 Train Prec: 0.3862 Train Rcll: 0.4457 Train F1: 0.3953 Val Loss: 1.4994 Val Acc: 0.2936 Val Prec: 0.3701 Val Rcll: 0.4346 Val F1: 0.3812\n",
      "EPOCH: 3/10 Train Loss: 1.5370 Train Acc: 0.4354 Train Prec: 0.5910 Train Rcll: 0.5480 Train F1: 0.5145 Val Loss: 1.2762 Val Acc: 0.4083 Val Prec: 0.5714 Val Rcll: 0.5314 Val F1: 0.4995\n",
      "EPOCH: 4/10 Train Loss: 1.3434 Train Acc: 0.4459 Train Prec: 0.5493 Train Rcll: 0.5459 Train F1: 0.5259 Val Loss: 1.1987 Val Acc: 0.4229 Val Prec: 0.5441 Val Rcll: 0.5321 Val F1: 0.5110\n",
      "EPOCH: 5/10 Train Loss: 1.2465 Train Acc: 0.4739 Train Prec: 0.6279 Train Rcll: 0.5665 Train F1: 0.5562 Val Loss: 1.0970 Val Acc: 0.4607 Val Prec: 0.6127 Val Rcll: 0.5557 Val F1: 0.5531\n",
      "EPOCH: 6/10 Train Loss: 1.1469 Train Acc: 0.5516 Train Prec: 0.6541 Train Rcll: 0.6568 Train F1: 0.6435 Val Loss: 1.0084 Val Acc: 0.4904 Val Prec: 0.6388 Val Rcll: 0.6039 Val F1: 0.5979\n",
      "EPOCH: 7/10 Train Loss: 1.0316 Train Acc: 0.6475 Train Prec: 0.7349 Train Rcll: 0.7461 Train F1: 0.7304 Val Loss: 0.8968 Val Acc: 0.6192 Val Prec: 0.7288 Val Rcll: 0.7235 Val F1: 0.7174\n",
      "EPOCH: 8/10 Train Loss: 0.9883 Train Acc: 0.5111 Train Prec: 0.6148 Train Rcll: 0.6181 Train F1: 0.5647 Val Loss: 0.9101 Val Acc: 0.4565 Val Prec: 0.5607 Val Rcll: 0.5840 Val F1: 0.5417\n",
      "EPOCH: 9/10 Train Loss: 0.9889 Train Acc: 0.6638 Train Prec: 0.7465 Train Rcll: 0.7587 Train F1: 0.7426 Val Loss: 0.8498 Val Acc: 0.6060 Val Prec: 0.7417 Val Rcll: 0.7074 Val F1: 0.7031\n",
      "EPOCH: 10/10 Train Loss: 0.9064 Train Acc: 0.6058 Train Prec: 0.8052 Train Rcll: 0.7406 Train F1: 0.7000 Val Loss: 0.7969 Val Acc: 0.5462 Val Prec: 0.6254 Val Rcll: 0.6853 Val F1: 0.6430\n",
      "\n",
      "Fold 5/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[0.95911215 1.77962428 5.33116883 0.77161654 1.02114428 0.75367197\n",
      " 0.76633479 0.76872659]\n",
      "EPOCH: 1/10 Train Loss: 2.1104 Train Acc: 0.1756 Train Prec: 0.2283 Train Rcll: 0.3236 Train F1: 0.2358 Val Loss: 1.8404 Val Acc: 0.1870 Val Prec: 0.2546 Val Rcll: 0.3337 Val F1: 0.2515\n",
      "EPOCH: 2/10 Train Loss: 1.8591 Train Acc: 0.3594 Train Prec: 0.4542 Train Rcll: 0.4807 Train F1: 0.4032 Val Loss: 1.4594 Val Acc: 0.3732 Val Prec: 0.4503 Val Rcll: 0.4857 Val F1: 0.4146\n",
      "EPOCH: 3/10 Train Loss: 1.6029 Train Acc: 0.4511 Train Prec: 0.6310 Train Rcll: 0.5661 Train F1: 0.5355 Val Loss: 1.2219 Val Acc: 0.4564 Val Prec: 0.6417 Val Rcll: 0.5636 Val F1: 0.5382\n",
      "EPOCH: 4/10 Train Loss: 1.3892 Train Acc: 0.4091 Train Prec: 0.5614 Train Rcll: 0.5280 Train F1: 0.4773 Val Loss: 1.0573 Val Acc: 0.4308 Val Prec: 0.6018 Val Rcll: 0.5386 Val F1: 0.4978\n",
      "EPOCH: 5/10 Train Loss: 1.2462 Train Acc: 0.4786 Train Prec: 0.6832 Train Rcll: 0.5913 Train F1: 0.5631 Val Loss: 0.9430 Val Acc: 0.4973 Val Prec: 0.6804 Val Rcll: 0.6033 Val F1: 0.5842\n",
      "EPOCH: 6/10 Train Loss: 1.0456 Train Acc: 0.6361 Train Prec: 0.7426 Train Rcll: 0.7352 Train F1: 0.7281 Val Loss: 0.7968 Val Acc: 0.6374 Val Prec: 0.7479 Val Rcll: 0.7307 Val F1: 0.7280\n",
      "EPOCH: 7/10 Train Loss: 0.9728 Train Acc: 0.5587 Train Prec: 0.6417 Train Rcll: 0.6663 Train F1: 0.6275 Val Loss: 0.8344 Val Acc: 0.5507 Val Prec: 0.6553 Val Rcll: 0.6554 Val F1: 0.6271\n",
      "EPOCH: 8/10 Train Loss: 0.8837 Train Acc: 0.5548 Train Prec: 0.7494 Train Rcll: 0.6839 Train F1: 0.6320 Val Loss: 0.7512 Val Acc: 0.5566 Val Prec: 0.6442 Val Rcll: 0.6761 Val F1: 0.6346\n",
      "EPOCH: 9/10 Train Loss: 0.8184 Train Acc: 0.6790 Train Prec: 0.7376 Train Rcll: 0.7716 Train F1: 0.7533 Val Loss: 0.7132 Val Acc: 0.6301 Val Prec: 0.7162 Val Rcll: 0.7271 Val F1: 0.7160\n",
      "EPOCH: 10/10 Train Loss: 0.7489 Train Acc: 0.6660 Train Prec: 0.8291 Train Rcll: 0.7585 Train F1: 0.7428 Val Loss: 0.6338 Val Acc: 0.6143 Val Prec: 0.7067 Val Rcll: 0.7153 Val F1: 0.7063\n",
      "\n",
      "Fold 6/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[0.95911215 1.77962428 5.33116883 0.77161654 1.02114428 0.7541335\n",
      " 0.76633479 0.76824704]\n",
      "EPOCH: 1/10 Train Loss: 2.1411 Train Acc: 0.1520 Train Prec: 0.3446 Train Rcll: 0.3167 Train F1: 0.2272 Val Loss: 1.9333 Val Acc: 0.1573 Val Prec: 0.2285 Val Rcll: 0.3277 Val F1: 0.2361\n",
      "EPOCH: 2/10 Train Loss: 1.9109 Train Acc: 0.3443 Train Prec: 0.5074 Train Rcll: 0.4754 Train F1: 0.3935 Val Loss: 1.6213 Val Acc: 0.3259 Val Prec: 0.3661 Val Rcll: 0.4602 Val F1: 0.3831\n",
      "EPOCH: 3/10 Train Loss: 1.6405 Train Acc: 0.5335 Train Prec: 0.6333 Train Rcll: 0.6448 Train F1: 0.6035 Val Loss: 1.2743 Val Acc: 0.5318 Val Prec: 0.6585 Val Rcll: 0.6431 Val F1: 0.6085\n",
      "EPOCH: 4/10 Train Loss: 1.3146 Train Acc: 0.5085 Train Prec: 0.6257 Train Rcll: 0.6209 Train F1: 0.5734 Val Loss: 1.0497 Val Acc: 0.4781 Val Prec: 0.6703 Val Rcll: 0.5919 Val F1: 0.5636\n",
      "EPOCH: 5/10 Train Loss: 1.1460 Train Acc: 0.6157 Train Prec: 0.7219 Train Rcll: 0.7166 Train F1: 0.6996 Val Loss: 0.9428 Val Acc: 0.5948 Val Prec: 0.7296 Val Rcll: 0.6940 Val F1: 0.6915\n",
      "EPOCH: 6/10 Train Loss: 1.0265 Train Acc: 0.5682 Train Prec: 0.6502 Train Rcll: 0.7172 Train F1: 0.6676 Val Loss: 0.8386 Val Acc: 0.5673 Val Prec: 0.6525 Val Rcll: 0.7115 Val F1: 0.6709\n",
      "EPOCH: 7/10 Train Loss: 0.9391 Train Acc: 0.5415 Train Prec: 0.6291 Train Rcll: 0.7001 Train F1: 0.6419 Val Loss: 0.7879 Val Acc: 0.5531 Val Prec: 0.6367 Val Rcll: 0.7044 Val F1: 0.6554\n",
      "EPOCH: 8/10 Train Loss: 0.8735 Train Acc: 0.6128 Train Prec: 0.6996 Train Rcll: 0.7035 Train F1: 0.6898 Val Loss: 0.7759 Val Acc: 0.5662 Val Prec: 0.6809 Val Rcll: 0.6605 Val F1: 0.6569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 9/10 Train Loss: 0.7514 Train Acc: 0.6200 Train Prec: 0.6908 Train Rcll: 0.7467 Train F1: 0.7087 Val Loss: 0.6621 Val Acc: 0.5958 Val Prec: 0.6767 Val Rcll: 0.7277 Val F1: 0.6922\n",
      "EPOCH: 10/10 Train Loss: 0.6669 Train Acc: 0.5333 Train Prec: 0.7071 Train Rcll: 0.6873 Train F1: 0.6245 Val Loss: 0.7759 Val Acc: 0.5303 Val Prec: 0.6649 Val Rcll: 0.6662 Val F1: 0.6229\n",
      "\n",
      "Fold 7/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[0.95911215 1.77962428 5.33116883 0.77161654 1.02114428 0.7541335\n",
      " 0.76633479 0.76824704]\n",
      "EPOCH: 1/10 Train Loss: 2.1719 Train Acc: 0.1769 Train Prec: 0.2094 Train Rcll: 0.3296 Train F1: 0.2344 Val Loss: 1.8660 Val Acc: 0.1827 Val Prec: 0.2273 Val Rcll: 0.3335 Val F1: 0.2423\n",
      "EPOCH: 2/10 Train Loss: 1.8686 Train Acc: 0.3131 Train Prec: 0.4333 Train Rcll: 0.4238 Train F1: 0.3883 Val Loss: 1.5508 Val Acc: 0.3023 Val Prec: 0.3786 Val Rcll: 0.4070 Val F1: 0.3749\n",
      "EPOCH: 3/10 Train Loss: 1.6334 Train Acc: 0.4410 Train Prec: 0.6015 Train Rcll: 0.5577 Train F1: 0.5237 Val Loss: 1.2205 Val Acc: 0.4372 Val Prec: 0.6154 Val Rcll: 0.5488 Val F1: 0.5121\n",
      "EPOCH: 4/10 Train Loss: 1.3677 Train Acc: 0.5008 Train Prec: 0.6037 Train Rcll: 0.6208 Train F1: 0.5810 Val Loss: 1.0174 Val Acc: 0.4859 Val Prec: 0.6374 Val Rcll: 0.5978 Val F1: 0.5662\n",
      "EPOCH: 5/10 Train Loss: 1.1719 Train Acc: 0.4367 Train Prec: 0.5722 Train Rcll: 0.5514 Train F1: 0.5041 Val Loss: 0.9849 Val Acc: 0.4500 Val Prec: 0.6465 Val Rcll: 0.5601 Val F1: 0.5195\n",
      "EPOCH: 6/10 Train Loss: 1.1077 Train Acc: 0.5372 Train Prec: 0.6357 Train Rcll: 0.6303 Train F1: 0.6284 Val Loss: 0.8904 Val Acc: 0.5397 Val Prec: 0.6322 Val Rcll: 0.6310 Val F1: 0.6282\n",
      "EPOCH: 7/10 Train Loss: 1.0084 Train Acc: 0.5485 Train Prec: 0.6693 Train Rcll: 0.6486 Train F1: 0.6291 Val Loss: 0.8517 Val Acc: 0.5455 Val Prec: 0.6776 Val Rcll: 0.6398 Val F1: 0.6290\n",
      "EPOCH: 8/10 Train Loss: 0.9174 Train Acc: 0.5845 Train Prec: 0.6843 Train Rcll: 0.6879 Train F1: 0.6645 Val Loss: 0.7346 Val Acc: 0.5757 Val Prec: 0.6780 Val Rcll: 0.6787 Val F1: 0.6605\n",
      "EPOCH: 9/10 Train Loss: 0.8506 Train Acc: 0.5248 Train Prec: 0.6010 Train Rcll: 0.6595 Train F1: 0.6029 Val Loss: 0.8592 Val Acc: 0.4950 Val Prec: 0.5716 Val Rcll: 0.6287 Val F1: 0.5733\n",
      "EPOCH: 10/10 Train Loss: 0.7891 Train Acc: 0.4633 Train Prec: 0.6735 Train Rcll: 0.6243 Train F1: 0.5478 Val Loss: 0.8522 Val Acc: 0.4670 Val Prec: 0.5321 Val Rcll: 0.6127 Val F1: 0.5368\n",
      "\n",
      "Fold 8/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[0.95846304 1.77980491 5.3548913  0.77121165 1.02124793 0.75421004\n",
      " 0.76641257 0.76832502]\n",
      "EPOCH: 1/10 Train Loss: 2.1467 Train Acc: 0.1487 Train Prec: 0.3260 Train Rcll: 0.3025 Train F1: 0.2125 Val Loss: 1.8061 Val Acc: 0.1524 Val Prec: 0.2902 Val Rcll: 0.3063 Val F1: 0.2169\n",
      "EPOCH: 2/10 Train Loss: 1.8750 Train Acc: 0.2560 Train Prec: 0.3233 Train Rcll: 0.4049 Train F1: 0.3233 Val Loss: 1.5333 Val Acc: 0.2600 Val Prec: 0.3403 Val Rcll: 0.4082 Val F1: 0.3277\n",
      "EPOCH: 3/10 Train Loss: 1.6961 Train Acc: 0.3403 Train Prec: 0.5715 Train Rcll: 0.4771 Train F1: 0.4067 Val Loss: 1.3643 Val Acc: 0.3373 Val Prec: 0.3653 Val Rcll: 0.4667 Val F1: 0.3884\n",
      "EPOCH: 4/10 Train Loss: 1.4215 Train Acc: 0.4800 Train Prec: 0.5926 Train Rcll: 0.5943 Train F1: 0.5628 Val Loss: 1.1037 Val Acc: 0.4493 Val Prec: 0.6007 Val Rcll: 0.5603 Val F1: 0.5349\n",
      "EPOCH: 5/10 Train Loss: 1.2667 Train Acc: 0.4934 Train Prec: 0.6508 Train Rcll: 0.6073 Train F1: 0.5706 Val Loss: 0.9303 Val Acc: 0.4881 Val Prec: 0.6529 Val Rcll: 0.6100 Val F1: 0.5716\n",
      "EPOCH: 6/10 Train Loss: 1.1369 Train Acc: 0.5643 Train Prec: 0.6781 Train Rcll: 0.6672 Train F1: 0.6643 Val Loss: 0.8814 Val Acc: 0.5789 Val Prec: 0.6817 Val Rcll: 0.6888 Val F1: 0.6772\n",
      "EPOCH: 7/10 Train Loss: 1.0266 Train Acc: 0.6431 Train Prec: 0.7228 Train Rcll: 0.7386 Train F1: 0.7277 Val Loss: 0.7913 Val Acc: 0.6358 Val Prec: 0.7242 Val Rcll: 0.7402 Val F1: 0.7270\n",
      "EPOCH: 8/10 Train Loss: 0.9515 Train Acc: 0.6411 Train Prec: 0.7368 Train Rcll: 0.7344 Train F1: 0.7253 Val Loss: 0.7339 Val Acc: 0.6116 Val Prec: 0.7239 Val Rcll: 0.7139 Val F1: 0.7049\n",
      "EPOCH: 9/10 Train Loss: 0.8700 Train Acc: 0.6007 Train Prec: 0.6955 Train Rcll: 0.7158 Train F1: 0.6821 Val Loss: 0.7781 Val Acc: 0.5814 Val Prec: 0.6913 Val Rcll: 0.6964 Val F1: 0.6689\n",
      "EPOCH: 10/10 Train Loss: 0.8955 Train Acc: 0.6370 Train Prec: 0.7140 Train Rcll: 0.7496 Train F1: 0.7212 Val Loss: 0.6807 Val Acc: 0.6181 Val Prec: 0.7030 Val Rcll: 0.7335 Val F1: 0.7091\n",
      "\n",
      "Fold 9/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[0.95846304 1.77980491 5.3548913  0.77121165 1.02124793 0.75421004\n",
      " 0.76641257 0.76832502]\n",
      "EPOCH: 1/10 Train Loss: 2.0996 Train Acc: 0.2115 Train Prec: 0.2279 Train Rcll: 0.3577 Train F1: 0.2639 Val Loss: 1.7944 Val Acc: 0.2195 Val Prec: 0.2431 Val Rcll: 0.3638 Val F1: 0.2720\n",
      "EPOCH: 2/10 Train Loss: 1.8508 Train Acc: 0.2195 Train Prec: 0.3840 Train Rcll: 0.3574 Train F1: 0.3204 Val Loss: 1.6282 Val Acc: 0.2344 Val Prec: 0.3962 Val Rcll: 0.3737 Val F1: 0.3368\n",
      "EPOCH: 3/10 Train Loss: 1.6881 Train Acc: 0.3255 Train Prec: 0.4024 Train Rcll: 0.4609 Train F1: 0.3831 Val Loss: 1.4997 Val Acc: 0.3222 Val Prec: 0.4136 Val Rcll: 0.4556 Val F1: 0.3821\n",
      "EPOCH: 4/10 Train Loss: 1.4907 Train Acc: 0.4286 Train Prec: 0.6034 Train Rcll: 0.5417 Train F1: 0.5148 Val Loss: 1.2531 Val Acc: 0.3963 Val Prec: 0.4883 Val Rcll: 0.5142 Val F1: 0.4726\n",
      "EPOCH: 5/10 Train Loss: 1.2759 Train Acc: 0.4657 Train Prec: 0.5832 Train Rcll: 0.5742 Train F1: 0.5491 Val Loss: 1.1172 Val Acc: 0.4194 Val Prec: 0.5974 Val Rcll: 0.5343 Val F1: 0.5109\n",
      "EPOCH: 6/10 Train Loss: 1.2079 Train Acc: 0.4681 Train Prec: 0.6088 Train Rcll: 0.5838 Train F1: 0.5525 Val Loss: 1.1349 Val Acc: 0.3991 Val Prec: 0.5916 Val Rcll: 0.5017 Val F1: 0.4863\n",
      "EPOCH: 7/10 Train Loss: 1.1132 Train Acc: 0.5617 Train Prec: 0.6718 Train Rcll: 0.6685 Train F1: 0.6615 Val Loss: 1.0164 Val Acc: 0.4769 Val Prec: 0.6475 Val Rcll: 0.5917 Val F1: 0.5818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 8/10 Train Loss: 1.0157 Train Acc: 0.5721 Train Prec: 0.6837 Train Rcll: 0.6889 Train F1: 0.6619 Val Loss: 0.9280 Val Acc: 0.5320 Val Prec: 0.6701 Val Rcll: 0.6473 Val F1: 0.6357\n",
      "EPOCH: 9/10 Train Loss: 0.9360 Train Acc: 0.5924 Train Prec: 0.6919 Train Rcll: 0.7048 Train F1: 0.6814 Val Loss: 0.8379 Val Acc: 0.5591 Val Prec: 0.6763 Val Rcll: 0.6761 Val F1: 0.6608\n",
      "EPOCH: 10/10 Train Loss: 0.8660 Train Acc: 0.6318 Train Prec: 0.7171 Train Rcll: 0.7311 Train F1: 0.7131 Val Loss: 0.7649 Val Acc: 0.5985 Val Prec: 0.6997 Val Rcll: 0.7062 Val F1: 0.6951\n",
      "\n",
      "Fold 10/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[0.95846304 1.77723665 5.3548913  0.77121165 1.02124793 0.75421004\n",
      " 0.76688979 0.76832502]\n",
      "EPOCH: 1/10 Train Loss: 2.1201 Train Acc: 0.1560 Train Prec: 0.2057 Train Rcll: 0.3090 Train F1: 0.2171 Val Loss: 1.8014 Val Acc: 0.1526 Val Prec: 0.2063 Val Rcll: 0.3061 Val F1: 0.2139\n",
      "EPOCH: 2/10 Train Loss: 1.8495 Train Acc: 0.2566 Train Prec: 0.5190 Train Rcll: 0.4223 Train F1: 0.3583 Val Loss: 1.5034 Val Acc: 0.2581 Val Prec: 0.4936 Val Rcll: 0.4171 Val F1: 0.3524\n",
      "EPOCH: 3/10 Train Loss: 1.5960 Train Acc: 0.4218 Train Prec: 0.5421 Train Rcll: 0.5346 Train F1: 0.4962 Val Loss: 1.2818 Val Acc: 0.3801 Val Prec: 0.5365 Val Rcll: 0.4963 Val F1: 0.4429\n",
      "EPOCH: 4/10 Train Loss: 1.3185 Train Acc: 0.4496 Train Prec: 0.5544 Train Rcll: 0.5590 Train F1: 0.5323 Val Loss: 1.0999 Val Acc: 0.3992 Val Prec: 0.6048 Val Rcll: 0.5121 Val F1: 0.4752\n",
      "EPOCH: 5/10 Train Loss: 1.1635 Train Acc: 0.4770 Train Prec: 0.6143 Train Rcll: 0.5883 Train F1: 0.5593 Val Loss: 0.9909 Val Acc: 0.4366 Val Prec: 0.6348 Val Rcll: 0.5531 Val F1: 0.5212\n",
      "EPOCH: 6/10 Train Loss: 1.0193 Train Acc: 0.5138 Train Prec: 0.6531 Train Rcll: 0.6325 Train F1: 0.6132 Val Loss: 0.8971 Val Acc: 0.4876 Val Prec: 0.6282 Val Rcll: 0.5948 Val F1: 0.5836\n",
      "EPOCH: 7/10 Train Loss: 0.9214 Train Acc: 0.5478 Train Prec: 0.6555 Train Rcll: 0.6618 Train F1: 0.6347 Val Loss: 0.8176 Val Acc: 0.5140 Val Prec: 0.6379 Val Rcll: 0.6157 Val F1: 0.5995\n",
      "EPOCH: 8/10 Train Loss: 0.8479 Train Acc: 0.4026 Train Prec: 0.5472 Train Rcll: 0.5686 Train F1: 0.4707 Val Loss: 1.1444 Val Acc: 0.3962 Val Prec: 0.5155 Val Rcll: 0.5226 Val F1: 0.4616\n",
      "EPOCH: 9/10 Train Loss: 0.7969 Train Acc: 0.5160 Train Prec: 0.5848 Train Rcll: 0.6579 Train F1: 0.5914 Val Loss: 0.8414 Val Acc: 0.4689 Val Prec: 0.5398 Val Rcll: 0.5825 Val F1: 0.5415\n",
      "EPOCH: 10/10 Train Loss: 0.7042 Train Acc: 0.6812 Train Prec: 0.7562 Train Rcll: 0.7724 Train F1: 0.7575 Val Loss: 0.6977 Val Acc: 0.6089 Val Prec: 0.7171 Val Rcll: 0.7069 Val F1: 0.7003\n",
      "\n",
      "K-FOLD VALIDATION RESULTS: \n",
      "Accuracy: 0.581627806204673\n",
      "Precision: 0.6858866553202413\n",
      "Recall: 0.6976297955757149\n",
      "F1: 0.6744084249933886\n",
      "\n",
      "VALIDATION RESULTS (PER CLASS): \n",
      "\n",
      "Accuracy:\n",
      "   climbing_down: 0.42712862265897933\n",
      "   climbing_up: 0.05061647213545948\n",
      "   jumping: 0.49981353144397056\n",
      "   lying: 0.9221750625004719\n",
      "   running: 0.8516542915453937\n",
      "   sitting: 0.8742332072125507\n",
      "   standing: 0.7031251934992925\n",
      "   walking: 0.32427606864126646\n",
      "\n",
      "Precision:\n",
      "   climbing_down: 0.5094336277608464\n",
      "   climbing_up: 0.13333333333333333\n",
      "   jumping: 0.6852265036127532\n",
      "   lying: 0.9638614035426937\n",
      "   running: 0.8821236664243873\n",
      "   sitting: 0.955171077828162\n",
      "   standing: 0.8668199526694413\n",
      "   walking: 0.4911236773903127\n",
      "\n",
      "Recall:\n",
      "   climbing_down: 0.7234019501625135\n",
      "   climbing_up: 0.050649350649350645\n",
      "   jumping: 0.7078461538461538\n",
      "   lying: 0.9554656255951247\n",
      "   running: 0.9619402985074628\n",
      "   sitting: 0.9118147046323843\n",
      "   standing: 0.8044818278827442\n",
      "   walking: 0.4654384533299855\n",
      "\n",
      "F1:\n",
      "   climbing_down: 0.5808647615815954\n",
      "   climbing_up: 0.06858695652173914\n",
      "   jumping: 0.6469819369682052\n",
      "   lying: 0.9593875513774712\n",
      "   running: 0.9196242701693527\n",
      "   sitting: 0.9326737820590679\n",
      "   standing: 0.819325895988148\n",
      "   walking: 0.46782224528152927\n",
      "\n",
      "GENERALIZATION GAP ANALYSIS: \n",
      "\n",
      "Accuracy: [ 0.0336762  -0.00514065  0.24899991  0.00558615  0.10339268 -0.01528652\n",
      " -0.11613928 -0.01224512]\n",
      "Precision: [ 0.04768916  0.33165456  0.10469173 -0.0161418   0.08551006 -0.03226136\n",
      " -0.01761475 -0.04131109]\n",
      "Recall: [ 0.04695818 -0.00484344  0.23917624  0.02236189  0.02446103  0.01484343\n",
      " -0.12673743  0.00659078]\n",
      "F1: [ 0.0363289   0.00020791  0.18730617  0.00279571  0.05718143 -0.00858954\n",
      " -0.09101018 -0.01914072]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# number of splits, i.e. folds\n",
    "config['splits_kfold'] = 10\n",
    "\n",
    "# needed for saving results\n",
    "log_date = time.strftime('%Y%m%d')\n",
    "log_timestamp = time.strftime('%H%M%S')\n",
    "\n",
    "# define the stratified k-fold object; it is already imported for you\n",
    "# pass it the number of splits, i.e. folds, and seed as well as set shuffling to true\n",
    "skf = StratifiedKFold(n_splits=config['splits_kfold'],\n",
    "                      shuffle=True,\n",
    "                      random_state=config['seed'])\n",
    "    \n",
    "    \n",
    "print(train_valid_data.shape)\n",
    "\n",
    "# apply the sliding window on top of both the train + valid data; use the \"apply_sliding_window\" function\n",
    "# found in data_processing.sliding_window\n",
    "X_train_valid, y_train_valid = apply_sliding_window(train_valid_data.iloc[:, :-1], train_valid_data.iloc[:, -1],\n",
    "                                                    sliding_window_size=config['sw_length'],\n",
    "                                                    unit=config['sw_unit'],\n",
    "                                                    sampling_rate=config['sampling_rate'],\n",
    "                                                    sliding_window_overlap=config['sw_overlap'])\n",
    "\n",
    "print(X_train_valid.shape, y_train_valid.shape)\n",
    "\n",
    "# omit the first feature column (subject_identifier) from the train + valid dataset\n",
    "X_train_valid = X_train_valid[:, :, 1:]\n",
    "\n",
    "# result objects used for accumulating the scores across folds; add each fold result to these objects so that they\n",
    "# are averaged at the end of the k-fold loop\n",
    "kfold_accuracy = np.zeros(config['nb_classes'])\n",
    "kfold_precision = np.zeros(config['nb_classes'])\n",
    "kfold_recall = np.zeros(config['nb_classes'])\n",
    "kfold_f1 = np.zeros(config['nb_classes'])\n",
    "    \n",
    "kfold_accuracy_gap = 0\n",
    "kfold_precision_gap = 0\n",
    "kfold_recall_gap = 0\n",
    "kfold_f1_gap = 0\n",
    "\n",
    "# k-fold validation loop; for each loop iteration return fold identifier and indeces which can be used to split\n",
    "# the train + valid data into train and validation data according to the current fold\n",
    "for j, (train_index, valid_index) in enumerate(skf.split(X_train_valid, y_train_valid)):\n",
    "    print('\\nFold {0}/{1}'.format(j + 1, config['splits_kfold']))\n",
    "    \n",
    "    # split the data into train and validation data; to do so, use the indeces produces by the split function\n",
    "    X_train, X_valid = X_train_valid[train_index], X_train_valid[valid_index]\n",
    "    y_train, y_valid = y_train_valid[train_index], y_train_valid[valid_index]\n",
    "    \n",
    "    # within the config file, set the parameters 'window_size' and 'nb_channels' accordingly\n",
    "    # window_size = size of the sliding window in units\n",
    "    # nb_channels = number of feature channels\n",
    "    config['window_size'] = X_train.shape[1]\n",
    "    config['nb_channels'] = X_train.shape[2]\n",
    "    \n",
    "    # define the network to be a DeepConvLSTM object; can be imported from model.DeepConvLSTM\n",
    "    # pass it the config object\n",
    "    net = DeepConvLSTM(config=config)\n",
    "    \n",
    "    # convert the features of the train and validation to float32 and labels to uint8 for GPU compatibility \n",
    "    X_train, y_train,  = X_train.astype(np.float32), y_train.astype(np.uint8)\n",
    "    X_valid, y_valid = X_valid.astype(np.float32), y_valid.astype(np.uint8)\n",
    "    \n",
    "    # feed the datasets into the train function; can be imported from model.train\n",
    "    kfold_net, val_output, train_output = train(X_train, y_train, X_valid, y_valid, network=net, config=config,\n",
    "                                                log_date=log_date, log_timestamp=log_timestamp)\n",
    "        \n",
    "    # in the following validation and train evaluation metrics are calculated\n",
    "    cls = np.array(range(config['nb_classes']))\n",
    "    val_accuracy = jaccard_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "    val_precision = precision_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "    val_recall = recall_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "    val_f1 = f1_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "    train_accuracy = jaccard_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "    train_precision = precision_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "    train_recall = recall_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "    train_f1 = f1_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "    \n",
    "    # add up the fold results\n",
    "    kfold_accuracy += val_accuracy\n",
    "    kfold_precision += val_precision\n",
    "    kfold_recall += val_recall\n",
    "    kfold_f1 += val_f1\n",
    "\n",
    "    # add up the generalization gap results\n",
    "    kfold_accuracy_gap += train_accuracy - val_accuracy\n",
    "    kfold_precision_gap += train_precision - val_precision\n",
    "    kfold_recall_gap += train_recall - val_recall\n",
    "    kfold_f1_gap += train_f1 - val_f1\n",
    "    \n",
    "# the next bit prints out the average results across folds if you did everything correctly\n",
    "print(\"\\nK-FOLD VALIDATION RESULTS: \")\n",
    "print(\"Accuracy: {0}\".format(np.mean(kfold_accuracy / config['splits_kfold'])))\n",
    "print(\"Precision: {0}\".format(np.mean(kfold_precision / config['splits_kfold'])))\n",
    "print(\"Recall: {0}\".format(np.mean(kfold_recall / config['splits_kfold'])))\n",
    "print(\"F1: {0}\".format(np.mean(kfold_f1 / config['splits_kfold'])))\n",
    "    \n",
    "print(\"\\nVALIDATION RESULTS (PER CLASS): \")\n",
    "print(\"\\nAccuracy:\")\n",
    "for i, rslt in enumerate(kfold_accuracy / config['splits_kfold']):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "print(\"\\nPrecision:\")\n",
    "for i, rslt in enumerate(kfold_precision / config['splits_kfold']):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "print(\"\\nRecall:\")\n",
    "for i, rslt in enumerate(kfold_recall / config['splits_kfold']):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "print(\"\\nF1:\")\n",
    "for i, rslt in enumerate(kfold_f1 / config['splits_kfold']):\n",
    "    print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    \n",
    "print(\"\\nGENERALIZATION GAP ANALYSIS: \")\n",
    "print(\"\\nAccuracy: {0}\".format(kfold_accuracy_gap / config['splits_kfold']))\n",
    "print(\"Precision: {0}\".format(kfold_precision_gap / config['splits_kfold']))\n",
    "print(\"Recall: {0}\".format(kfold_recall_gap / config['splits_kfold']))\n",
    "print(\"F1: {0}\".format(kfold_f1_gap / config['splits_kfold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Per-Participant Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per-Participant cross-validation validates the predictive perfomance of each subject individually. This means that for each subject contained in the dataset, one one validation loop is run. Usually, this done by applying a stratified shuffle split, i.e. multiple stratified train-valid splits with each time randomly shuffled records, of multiple rounds per subject. \n",
    "\n",
    "**Note:** as mentioned above when dealing with stratified splits, which inheritly require shuffling, and shuffling in general, one must always first apply the sliding window before applying the split. Doing so one can at least minimize the destroyed time-dependencies among records by at least maintaining them within each window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Implementing the per-participant CV loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define a for loop which iterates over all subjects\n",
    "2. Within the loop define a stratified shuffle split object\n",
    "3. Define the subject data by filtering the train + valid data for the current subject\n",
    "4. Apply the sliding window on top of the filtered subject data and omit the 'subject_identifier' column\n",
    "5. Define the stratified shuffle split loop; use the split function of the stratified shuffle split object to obtain indeces to split the current subject data\n",
    "6. Run the train function and add up obtained results to the accumulated result objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " VALIDATING FOR SUBJECT 1 OF 2\n",
      "(221621, 5)\n",
      "(6331, 50, 4) (6331,)\n",
      "\n",
      "SPLIT 1/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.09137931 0.84325044 6.41554054 0.88079777 0.90428571 0.84325044\n",
      " 0.86161525 0.86633212]\n",
      "EPOCH: 1/10 Train Loss: 2.0401 Train Acc: 0.2220 Train Prec: 0.3398 Train Rcll: 0.3893 Train F1: 0.3184 Val Loss: 1.9403 Val Acc: 0.2188 Val Prec: 0.3482 Val Rcll: 0.3870 Val F1: 0.3176\n",
      "EPOCH: 2/10 Train Loss: 1.6894 Train Acc: 0.4076 Train Prec: 0.6010 Train Rcll: 0.5480 Train F1: 0.5086 Val Loss: 1.2970 Val Acc: 0.3954 Val Prec: 0.5948 Val Rcll: 0.5328 Val F1: 0.4979\n",
      "EPOCH: 3/10 Train Loss: 1.1355 Train Acc: 0.6260 Train Prec: 0.7693 Train Rcll: 0.7334 Train F1: 0.7149 Val Loss: 0.8946 Val Acc: 0.6028 Val Prec: 0.7290 Val Rcll: 0.7117 Val F1: 0.6949\n",
      "EPOCH: 4/10 Train Loss: 0.9109 Train Acc: 0.6730 Train Prec: 0.7955 Train Rcll: 0.7867 Train F1: 0.7712 Val Loss: 0.7368 Val Acc: 0.6524 Val Prec: 0.7750 Val Rcll: 0.7769 Val F1: 0.7530\n",
      "EPOCH: 5/10 Train Loss: 0.7626 Train Acc: 0.7366 Train Prec: 0.8522 Train Rcll: 0.8307 Train F1: 0.8292 Val Loss: 0.6309 Val Acc: 0.7326 Val Prec: 0.8399 Val Rcll: 0.8339 Val F1: 0.8278\n",
      "EPOCH: 6/10 Train Loss: 0.6752 Train Acc: 0.7403 Train Prec: 0.8532 Train Rcll: 0.8374 Train F1: 0.8323 Val Loss: 0.5665 Val Acc: 0.7363 Val Prec: 0.8442 Val Rcll: 0.8420 Val F1: 0.8320\n",
      "EPOCH: 7/10 Train Loss: 0.6071 Train Acc: 0.7547 Train Prec: 0.8726 Train Rcll: 0.8448 Train F1: 0.8459 Val Loss: 0.5276 Val Acc: 0.7504 Val Prec: 0.8610 Val Rcll: 0.8504 Val F1: 0.8454\n",
      "EPOCH: 8/10 Train Loss: 0.5614 Train Acc: 0.7765 Train Prec: 0.8829 Train Rcll: 0.8612 Train F1: 0.8639 Val Loss: 0.4900 Val Acc: 0.7652 Val Prec: 0.8681 Val Rcll: 0.8611 Val F1: 0.8575\n",
      "EPOCH: 9/10 Train Loss: 0.5269 Train Acc: 0.7763 Train Prec: 0.8805 Train Rcll: 0.8611 Train F1: 0.8634 Val Loss: 0.4693 Val Acc: 0.7708 Val Prec: 0.8722 Val Rcll: 0.8623 Val F1: 0.8604\n",
      "EPOCH: 10/10 Train Loss: 0.5088 Train Acc: 0.7834 Train Prec: 0.8896 Train Rcll: 0.8648 Train F1: 0.8689 Val Loss: 0.4586 Val Acc: 0.7760 Val Prec: 0.8771 Val Rcll: 0.8657 Val F1: 0.8644\n",
      "\n",
      "SPLIT 2/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.09137931 0.84325044 6.41554054 0.88079777 0.90428571 0.84325044\n",
      " 0.86161525 0.86633212]\n",
      "EPOCH: 1/10 Train Loss: 2.0401 Train Acc: 0.1885 Train Prec: 0.3477 Train Rcll: 0.3951 Train F1: 0.2919 Val Loss: 1.9374 Val Acc: 0.1852 Val Prec: 0.3502 Val Rcll: 0.3854 Val F1: 0.2893\n",
      "EPOCH: 2/10 Train Loss: 1.7116 Train Acc: 0.3949 Train Prec: 0.5569 Train Rcll: 0.5674 Train F1: 0.4972 Val Loss: 1.3358 Val Acc: 0.3985 Val Prec: 0.5919 Val Rcll: 0.5675 Val F1: 0.5052\n",
      "EPOCH: 3/10 Train Loss: 1.1705 Train Acc: 0.5870 Train Prec: 0.7137 Train Rcll: 0.7334 Train F1: 0.7023 Val Loss: 0.9352 Val Acc: 0.5805 Val Prec: 0.7017 Val Rcll: 0.7240 Val F1: 0.6946\n",
      "EPOCH: 4/10 Train Loss: 0.9105 Train Acc: 0.6786 Train Prec: 0.7946 Train Rcll: 0.7912 Train F1: 0.7807 Val Loss: 0.7669 Val Acc: 0.6633 Val Prec: 0.7827 Val Rcll: 0.7791 Val F1: 0.7687\n",
      "EPOCH: 5/10 Train Loss: 0.7674 Train Acc: 0.7291 Train Prec: 0.8249 Train Rcll: 0.8284 Train F1: 0.8246 Val Loss: 0.6574 Val Acc: 0.7099 Val Prec: 0.8108 Val Rcll: 0.8173 Val F1: 0.8116\n",
      "EPOCH: 6/10 Train Loss: 0.6722 Train Acc: 0.7220 Train Prec: 0.8217 Train Rcll: 0.8233 Train F1: 0.8176 Val Loss: 0.5957 Val Acc: 0.7132 Val Prec: 0.8165 Val Rcll: 0.8196 Val F1: 0.8140\n",
      "EPOCH: 7/10 Train Loss: 0.6126 Train Acc: 0.7710 Train Prec: 0.8636 Train Rcll: 0.8596 Train F1: 0.8591 Val Loss: 0.5507 Val Acc: 0.7596 Val Prec: 0.8584 Val Rcll: 0.8520 Val F1: 0.8523\n",
      "EPOCH: 8/10 Train Loss: 0.5770 Train Acc: 0.7785 Train Prec: 0.8678 Train Rcll: 0.8638 Train F1: 0.8652 Val Loss: 0.5187 Val Acc: 0.7655 Val Prec: 0.8616 Val Rcll: 0.8549 Val F1: 0.8572\n",
      "EPOCH: 9/10 Train Loss: 0.5377 Train Acc: 0.7865 Train Prec: 0.8748 Train Rcll: 0.8702 Train F1: 0.8713 Val Loss: 0.4936 Val Acc: 0.7786 Val Prec: 0.8720 Val Rcll: 0.8637 Val F1: 0.8666\n",
      "EPOCH: 10/10 Train Loss: 0.5119 Train Acc: 0.7921 Train Prec: 0.8782 Train Rcll: 0.8742 Train F1: 0.8754 Val Loss: 0.4742 Val Acc: 0.7808 Val Prec: 0.8720 Val Rcll: 0.8682 Val F1: 0.8690\n",
      "\n",
      "SPLIT 3/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.09137931 0.84325044 6.41554054 0.88079777 0.90428571 0.84325044\n",
      " 0.86161525 0.86633212]\n",
      "EPOCH: 1/10 Train Loss: 2.0282 Train Acc: 0.1653 Train Prec: 0.5316 Train Rcll: 0.3340 Train F1: 0.2523 Val Loss: 1.9241 Val Acc: 0.1679 Val Prec: 0.4039 Val Rcll: 0.3382 Val F1: 0.2510\n",
      "EPOCH: 2/10 Train Loss: 1.7191 Train Acc: 0.4145 Train Prec: 0.5562 Train Rcll: 0.5992 Train F1: 0.5280 Val Loss: 1.3551 Val Acc: 0.4166 Val Prec: 0.5436 Val Rcll: 0.6038 Val F1: 0.5279\n",
      "EPOCH: 3/10 Train Loss: 1.2107 Train Acc: 0.5878 Train Prec: 0.7436 Train Rcll: 0.7298 Train F1: 0.7136 Val Loss: 0.9553 Val Acc: 0.5821 Val Prec: 0.7331 Val Rcll: 0.7210 Val F1: 0.7081\n",
      "EPOCH: 4/10 Train Loss: 0.9488 Train Acc: 0.7011 Train Prec: 0.8022 Train Rcll: 0.8240 Train F1: 0.8105 Val Loss: 0.7599 Val Acc: 0.6949 Val Prec: 0.7981 Val Rcll: 0.8196 Val F1: 0.8051\n",
      "EPOCH: 5/10 Train Loss: 0.7981 Train Acc: 0.7167 Train Prec: 0.8253 Train Rcll: 0.8339 Train F1: 0.8183 Val Loss: 0.6476 Val Acc: 0.7210 Val Prec: 0.8254 Val Rcll: 0.8325 Val F1: 0.8193\n",
      "EPOCH: 6/10 Train Loss: 0.6803 Train Acc: 0.7514 Train Prec: 0.8480 Train Rcll: 0.8542 Train F1: 0.8473 Val Loss: 0.5646 Val Acc: 0.7534 Val Prec: 0.8477 Val Rcll: 0.8552 Val F1: 0.8484\n",
      "EPOCH: 7/10 Train Loss: 0.6275 Train Acc: 0.7590 Train Prec: 0.8618 Train Rcll: 0.8556 Train F1: 0.8518 Val Loss: 0.5226 Val Acc: 0.7542 Val Prec: 0.8554 Val Rcll: 0.8540 Val F1: 0.8477\n",
      "EPOCH: 8/10 Train Loss: 0.5677 Train Acc: 0.7796 Train Prec: 0.8738 Train Rcll: 0.8661 Train F1: 0.8671 Val Loss: 0.4907 Val Acc: 0.7765 Val Prec: 0.8689 Val Rcll: 0.8668 Val F1: 0.8648\n",
      "EPOCH: 9/10 Train Loss: 0.5604 Train Acc: 0.7772 Train Prec: 0.8873 Train Rcll: 0.8620 Train F1: 0.8650 Val Loss: 0.4904 Val Acc: 0.7752 Val Prec: 0.8823 Val Rcll: 0.8615 Val F1: 0.8625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/10 Train Loss: 0.5311 Train Acc: 0.7940 Train Prec: 0.8867 Train Rcll: 0.8754 Train F1: 0.8776 Val Loss: 0.4563 Val Acc: 0.7918 Val Prec: 0.8818 Val Rcll: 0.8748 Val F1: 0.8755\n",
      "\n",
      "SPLIT 4/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.09137931 0.84325044 6.41554054 0.88079777 0.90428571 0.84325044\n",
      " 0.86161525 0.86633212]\n",
      "EPOCH: 1/10 Train Loss: 2.0386 Train Acc: 0.1600 Train Prec: 0.3898 Train Rcll: 0.3871 Train F1: 0.2479 Val Loss: 1.9548 Val Acc: 0.1671 Val Prec: 0.2608 Val Rcll: 0.3998 Val F1: 0.2574\n",
      "EPOCH: 2/10 Train Loss: 1.7425 Train Acc: 0.3671 Train Prec: 0.5150 Train Rcll: 0.5860 Train F1: 0.4763 Val Loss: 1.3906 Val Acc: 0.3733 Val Prec: 0.5037 Val Rcll: 0.5822 Val F1: 0.4793\n",
      "EPOCH: 3/10 Train Loss: 1.2131 Train Acc: 0.5955 Train Prec: 0.7283 Train Rcll: 0.7073 Train F1: 0.7017 Val Loss: 0.9766 Val Acc: 0.5790 Val Prec: 0.7157 Val Rcll: 0.6916 Val F1: 0.6902\n",
      "EPOCH: 4/10 Train Loss: 0.9385 Train Acc: 0.7099 Train Prec: 0.8197 Train Rcll: 0.8077 Train F1: 0.8068 Val Loss: 0.7805 Val Acc: 0.7052 Val Prec: 0.8132 Val Rcll: 0.8032 Val F1: 0.8022\n",
      "EPOCH: 5/10 Train Loss: 0.7739 Train Acc: 0.7293 Train Prec: 0.8533 Train Rcll: 0.8257 Train F1: 0.8234 Val Loss: 0.6563 Val Acc: 0.7281 Val Prec: 0.8483 Val Rcll: 0.8254 Val F1: 0.8228\n",
      "EPOCH: 6/10 Train Loss: 0.6792 Train Acc: 0.7434 Train Prec: 0.8475 Train Rcll: 0.8403 Train F1: 0.8346 Val Loss: 0.5810 Val Acc: 0.7375 Val Prec: 0.8421 Val Rcll: 0.8355 Val F1: 0.8304\n",
      "EPOCH: 7/10 Train Loss: 0.6241 Train Acc: 0.7723 Train Prec: 0.8747 Train Rcll: 0.8585 Train F1: 0.8605 Val Loss: 0.5351 Val Acc: 0.7639 Val Prec: 0.8665 Val Rcll: 0.8521 Val F1: 0.8543\n",
      "EPOCH: 8/10 Train Loss: 0.5681 Train Acc: 0.7782 Train Prec: 0.8823 Train Rcll: 0.8622 Train F1: 0.8650 Val Loss: 0.5110 Val Acc: 0.7683 Val Prec: 0.8696 Val Rcll: 0.8553 Val F1: 0.8570\n",
      "EPOCH: 9/10 Train Loss: 0.5441 Train Acc: 0.7798 Train Prec: 0.8771 Train Rcll: 0.8651 Train F1: 0.8656 Val Loss: 0.4832 Val Acc: 0.7683 Val Prec: 0.8670 Val Rcll: 0.8583 Val F1: 0.8578\n",
      "EPOCH: 10/10 Train Loss: 0.5179 Train Acc: 0.7905 Train Prec: 0.8903 Train Rcll: 0.8710 Train F1: 0.8745 Val Loss: 0.4684 Val Acc: 0.7836 Val Prec: 0.8866 Val Rcll: 0.8639 Val F1: 0.8693\n",
      "\n",
      "SPLIT 5/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.09137931 0.84325044 6.41554054 0.88079777 0.90428571 0.84325044\n",
      " 0.86161525 0.86633212]\n",
      "EPOCH: 1/10 Train Loss: 2.0612 Train Acc: 0.1307 Train Prec: 0.2639 Train Rcll: 0.2692 Train F1: 0.1917 Val Loss: 2.0012 Val Acc: 0.1370 Val Prec: 0.3753 Val Rcll: 0.2810 Val F1: 0.2040\n",
      "EPOCH: 2/10 Train Loss: 1.8500 Train Acc: 0.3180 Train Prec: 0.4544 Train Rcll: 0.5079 Train F1: 0.4119 Val Loss: 1.5560 Val Acc: 0.3090 Val Prec: 0.4467 Val Rcll: 0.4951 Val F1: 0.4006\n",
      "EPOCH: 3/10 Train Loss: 1.3556 Train Acc: 0.5055 Train Prec: 0.6248 Train Rcll: 0.6656 Train F1: 0.6188 Val Loss: 1.0576 Val Acc: 0.5098 Val Prec: 0.6295 Val Rcll: 0.6702 Val F1: 0.6230\n",
      "EPOCH: 4/10 Train Loss: 1.0353 Train Acc: 0.6006 Train Prec: 0.7165 Train Rcll: 0.7338 Train F1: 0.7093 Val Loss: 0.8468 Val Acc: 0.5990 Val Prec: 0.7119 Val Rcll: 0.7380 Val F1: 0.7070\n",
      "EPOCH: 5/10 Train Loss: 0.8558 Train Acc: 0.6585 Train Prec: 0.7648 Train Rcll: 0.7720 Train F1: 0.7571 Val Loss: 0.7187 Val Acc: 0.6600 Val Prec: 0.7653 Val Rcll: 0.7803 Val F1: 0.7589\n",
      "EPOCH: 6/10 Train Loss: 0.7542 Train Acc: 0.6772 Train Prec: 0.7850 Train Rcll: 0.7817 Train F1: 0.7727 Val Loss: 0.6360 Val Acc: 0.6811 Val Prec: 0.7872 Val Rcll: 0.7896 Val F1: 0.7767\n",
      "EPOCH: 7/10 Train Loss: 0.6651 Train Acc: 0.7480 Train Prec: 0.8449 Train Rcll: 0.8408 Train F1: 0.8414 Val Loss: 0.5578 Val Acc: 0.7511 Val Prec: 0.8435 Val Rcll: 0.8455 Val F1: 0.8437\n",
      "EPOCH: 8/10 Train Loss: 0.6223 Train Acc: 0.7656 Train Prec: 0.8639 Train Rcll: 0.8537 Train F1: 0.8555 Val Loss: 0.5182 Val Acc: 0.7621 Val Prec: 0.8549 Val Rcll: 0.8559 Val F1: 0.8534\n",
      "EPOCH: 9/10 Train Loss: 0.5923 Train Acc: 0.7730 Train Prec: 0.8715 Train Rcll: 0.8586 Train F1: 0.8613 Val Loss: 0.4936 Val Acc: 0.7799 Val Prec: 0.8715 Val Rcll: 0.8655 Val F1: 0.8666\n",
      "EPOCH: 10/10 Train Loss: 0.5603 Train Acc: 0.7833 Train Prec: 0.8856 Train Rcll: 0.8648 Train F1: 0.8688 Val Loss: 0.4727 Val Acc: 0.7845 Val Prec: 0.8811 Val Rcll: 0.8683 Val F1: 0.8703\n",
      "\n",
      "SPLIT 6/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.09137931 0.84325044 6.41554054 0.88079777 0.90428571 0.84325044\n",
      " 0.86161525 0.86633212]\n",
      "EPOCH: 1/10 Train Loss: 2.0435 Train Acc: 0.3028 Train Prec: 0.4402 Train Rcll: 0.4715 Train F1: 0.4115 Val Loss: 1.9660 Val Acc: 0.2837 Val Prec: 0.4363 Val Rcll: 0.4462 Val F1: 0.3886\n",
      "EPOCH: 2/10 Train Loss: 1.7314 Train Acc: 0.4323 Train Prec: 0.5383 Train Rcll: 0.5862 Train F1: 0.5348 Val Loss: 1.3337 Val Acc: 0.4242 Val Prec: 0.5356 Val Rcll: 0.5794 Val F1: 0.5271\n",
      "EPOCH: 3/10 Train Loss: 1.1719 Train Acc: 0.5248 Train Prec: 0.6752 Train Rcll: 0.6990 Train F1: 0.6327 Val Loss: 0.9505 Val Acc: 0.5254 Val Prec: 0.6691 Val Rcll: 0.7015 Val F1: 0.6359\n",
      "EPOCH: 4/10 Train Loss: 0.9475 Train Acc: 0.6417 Train Prec: 0.7652 Train Rcll: 0.7654 Train F1: 0.7448 Val Loss: 0.7907 Val Acc: 0.6439 Val Prec: 0.7646 Val Rcll: 0.7653 Val F1: 0.7442\n",
      "EPOCH: 5/10 Train Loss: 0.8193 Train Acc: 0.7021 Train Prec: 0.8079 Train Rcll: 0.8064 Train F1: 0.8037 Val Loss: 0.6876 Val Acc: 0.6987 Val Prec: 0.8061 Val Rcll: 0.7993 Val F1: 0.7991\n",
      "EPOCH: 6/10 Train Loss: 0.7176 Train Acc: 0.7301 Train Prec: 0.8294 Train Rcll: 0.8289 Train F1: 0.8278 Val Loss: 0.6037 Val Acc: 0.7326 Val Prec: 0.8313 Val Rcll: 0.8274 Val F1: 0.8280\n",
      "EPOCH: 7/10 Train Loss: 0.6624 Train Acc: 0.7481 Train Prec: 0.8426 Train Rcll: 0.8416 Train F1: 0.8407 Val Loss: 0.5474 Val Acc: 0.7434 Val Prec: 0.8419 Val Rcll: 0.8355 Val F1: 0.8372\n",
      "EPOCH: 8/10 Train Loss: 0.6071 Train Acc: 0.7699 Train Prec: 0.8633 Train Rcll: 0.8594 Train F1: 0.8598 Val Loss: 0.5105 Val Acc: 0.7695 Val Prec: 0.8619 Val Rcll: 0.8569 Val F1: 0.8589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 9/10 Train Loss: 0.5731 Train Acc: 0.7770 Train Prec: 0.8679 Train Rcll: 0.8639 Train F1: 0.8636 Val Loss: 0.4805 Val Acc: 0.7771 Val Prec: 0.8699 Val Rcll: 0.8618 Val F1: 0.8643\n",
      "EPOCH: 10/10 Train Loss: 0.5475 Train Acc: 0.7816 Train Prec: 0.8708 Train Rcll: 0.8683 Train F1: 0.8680 Val Loss: 0.4553 Val Acc: 0.7868 Val Prec: 0.8737 Val Rcll: 0.8698 Val F1: 0.8706\n",
      "\n",
      "SPLIT 7/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.09137931 0.84325044 6.41554054 0.88079777 0.90428571 0.84325044\n",
      " 0.86161525 0.86633212]\n",
      "EPOCH: 1/10 Train Loss: 2.0355 Train Acc: 0.0846 Train Prec: 0.1738 Train Rcll: 0.2208 Train F1: 0.1394 Val Loss: 1.9465 Val Acc: 0.0834 Val Prec: 0.1591 Val Rcll: 0.2220 Val F1: 0.1373\n",
      "EPOCH: 2/10 Train Loss: 1.7453 Train Acc: 0.3166 Train Prec: 0.4133 Train Rcll: 0.4557 Train F1: 0.3997 Val Loss: 1.4850 Val Acc: 0.3084 Val Prec: 0.3960 Val Rcll: 0.4494 Val F1: 0.3927\n",
      "EPOCH: 3/10 Train Loss: 1.3244 Train Acc: 0.4784 Train Prec: 0.6533 Train Rcll: 0.5922 Train F1: 0.5760 Val Loss: 1.1104 Val Acc: 0.4420 Val Prec: 0.6450 Val Rcll: 0.5585 Val F1: 0.5449\n",
      "EPOCH: 4/10 Train Loss: 1.0377 Train Acc: 0.6575 Train Prec: 0.7686 Train Rcll: 0.7646 Train F1: 0.7593 Val Loss: 0.8991 Val Acc: 0.6264 Val Prec: 0.7448 Val Rcll: 0.7366 Val F1: 0.7344\n",
      "EPOCH: 5/10 Train Loss: 0.8646 Train Acc: 0.7122 Train Prec: 0.8255 Train Rcll: 0.8149 Train F1: 0.8091 Val Loss: 0.7620 Val Acc: 0.6840 Val Prec: 0.8044 Val Rcll: 0.7931 Val F1: 0.7866\n",
      "EPOCH: 6/10 Train Loss: 0.7348 Train Acc: 0.7372 Train Prec: 0.8439 Train Rcll: 0.8355 Train F1: 0.8285 Val Loss: 0.6665 Val Acc: 0.7009 Val Prec: 0.8115 Val Rcll: 0.8102 Val F1: 0.8003\n",
      "EPOCH: 7/10 Train Loss: 0.6488 Train Acc: 0.7589 Train Prec: 0.8670 Train Rcll: 0.8491 Train F1: 0.8479 Val Loss: 0.6092 Val Acc: 0.7259 Val Prec: 0.8386 Val Rcll: 0.8275 Val F1: 0.8235\n",
      "EPOCH: 8/10 Train Loss: 0.5939 Train Acc: 0.7793 Train Prec: 0.8721 Train Rcll: 0.8644 Train F1: 0.8647 Val Loss: 0.5703 Val Acc: 0.7405 Val Prec: 0.8423 Val Rcll: 0.8384 Val F1: 0.8369\n",
      "EPOCH: 9/10 Train Loss: 0.5371 Train Acc: 0.7900 Train Prec: 0.8788 Train Rcll: 0.8718 Train F1: 0.8731 Val Loss: 0.5408 Val Acc: 0.7530 Val Prec: 0.8516 Val Rcll: 0.8480 Val F1: 0.8475\n",
      "EPOCH: 10/10 Train Loss: 0.5151 Train Acc: 0.7971 Train Prec: 0.8861 Train Rcll: 0.8762 Train F1: 0.8786 Val Loss: 0.5254 Val Acc: 0.7644 Val Prec: 0.8654 Val Rcll: 0.8561 Val F1: 0.8569\n",
      "\n",
      "SPLIT 8/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.09137931 0.84325044 6.41554054 0.88079777 0.90428571 0.84325044\n",
      " 0.86161525 0.86633212]\n",
      "EPOCH: 1/10 Train Loss: 2.0469 Train Acc: 0.1380 Train Prec: 0.3837 Train Rcll: 0.3047 Train F1: 0.2132 Val Loss: 1.9631 Val Acc: 0.1322 Val Prec: 0.4338 Val Rcll: 0.2954 Val F1: 0.2041\n",
      "EPOCH: 2/10 Train Loss: 1.7627 Train Acc: 0.3761 Train Prec: 0.5318 Train Rcll: 0.5280 Train F1: 0.4944 Val Loss: 1.4164 Val Acc: 0.3649 Val Prec: 0.5242 Val Rcll: 0.5098 Val F1: 0.4775\n",
      "EPOCH: 3/10 Train Loss: 1.2367 Train Acc: 0.6233 Train Prec: 0.7384 Train Rcll: 0.7286 Train F1: 0.7321 Val Loss: 0.9733 Val Acc: 0.6003 Val Prec: 0.7267 Val Rcll: 0.7069 Val F1: 0.7145\n",
      "EPOCH: 4/10 Train Loss: 0.9094 Train Acc: 0.7037 Train Prec: 0.8316 Train Rcll: 0.8033 Train F1: 0.8023 Val Loss: 0.7749 Val Acc: 0.6750 Val Prec: 0.8114 Val Rcll: 0.7781 Val F1: 0.7784\n",
      "EPOCH: 5/10 Train Loss: 0.7416 Train Acc: 0.7520 Train Prec: 0.8585 Train Rcll: 0.8448 Train F1: 0.8434 Val Loss: 0.6510 Val Acc: 0.7256 Val Prec: 0.8434 Val Rcll: 0.8228 Val F1: 0.8239\n",
      "EPOCH: 6/10 Train Loss: 0.6583 Train Acc: 0.7627 Train Prec: 0.8722 Train Rcll: 0.8502 Train F1: 0.8517 Val Loss: 0.5971 Val Acc: 0.7361 Val Prec: 0.8572 Val Rcll: 0.8305 Val F1: 0.8331\n",
      "EPOCH: 7/10 Train Loss: 0.6002 Train Acc: 0.7684 Train Prec: 0.8735 Train Rcll: 0.8559 Train F1: 0.8569 Val Loss: 0.5509 Val Acc: 0.7509 Val Prec: 0.8658 Val Rcll: 0.8422 Val F1: 0.8446\n",
      "EPOCH: 8/10 Train Loss: 0.5641 Train Acc: 0.7793 Train Prec: 0.8789 Train Rcll: 0.8636 Train F1: 0.8655 Val Loss: 0.5191 Val Acc: 0.7635 Val Prec: 0.8712 Val Rcll: 0.8519 Val F1: 0.8547\n",
      "EPOCH: 9/10 Train Loss: 0.5380 Train Acc: 0.7833 Train Prec: 0.8791 Train Rcll: 0.8671 Train F1: 0.8685 Val Loss: 0.4966 Val Acc: 0.7676 Val Prec: 0.8701 Val Rcll: 0.8570 Val F1: 0.8586\n",
      "EPOCH: 10/10 Train Loss: 0.5144 Train Acc: 0.7849 Train Prec: 0.8892 Train Rcll: 0.8675 Train F1: 0.8700 Val Loss: 0.4946 Val Acc: 0.7729 Val Prec: 0.8820 Val Rcll: 0.8584 Val F1: 0.8616\n",
      "\n",
      "SPLIT 9/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.09137931 0.84325044 6.41554054 0.88079777 0.90428571 0.84325044\n",
      " 0.86161525 0.86633212]\n",
      "EPOCH: 1/10 Train Loss: 2.0560 Train Acc: 0.1709 Train Prec: 0.4624 Train Rcll: 0.3500 Train F1: 0.2592 Val Loss: 1.9982 Val Acc: 0.1627 Val Prec: 0.4393 Val Rcll: 0.3416 Val F1: 0.2462\n",
      "EPOCH: 2/10 Train Loss: 1.8295 Train Acc: 0.3970 Train Prec: 0.5948 Train Rcll: 0.5541 Train F1: 0.5359 Val Loss: 1.4546 Val Acc: 0.3604 Val Prec: 0.5444 Val Rcll: 0.5211 Val F1: 0.4978\n",
      "EPOCH: 3/10 Train Loss: 1.2424 Train Acc: 0.6319 Train Prec: 0.7606 Train Rcll: 0.7468 Train F1: 0.7361 Val Loss: 0.8942 Val Acc: 0.6244 Val Prec: 0.7483 Val Rcll: 0.7417 Val F1: 0.7249\n",
      "EPOCH: 4/10 Train Loss: 0.8973 Train Acc: 0.6862 Train Prec: 0.8059 Train Rcll: 0.8002 Train F1: 0.7881 Val Loss: 0.6930 Val Acc: 0.7043 Val Prec: 0.8135 Val Rcll: 0.8084 Val F1: 0.7970\n",
      "EPOCH: 5/10 Train Loss: 0.7502 Train Acc: 0.7456 Train Prec: 0.8440 Train Rcll: 0.8409 Train F1: 0.8404 Val Loss: 0.5856 Val Acc: 0.7511 Val Prec: 0.8437 Val Rcll: 0.8447 Val F1: 0.8415\n",
      "EPOCH: 6/10 Train Loss: 0.6711 Train Acc: 0.7449 Train Prec: 0.8497 Train Rcll: 0.8418 Train F1: 0.8378 Val Loss: 0.5315 Val Acc: 0.7501 Val Prec: 0.8464 Val Rcll: 0.8459 Val F1: 0.8395\n",
      "EPOCH: 7/10 Train Loss: 0.6131 Train Acc: 0.7544 Train Prec: 0.8558 Train Rcll: 0.8493 Train F1: 0.8462 Val Loss: 0.5039 Val Acc: 0.7532 Val Prec: 0.8489 Val Rcll: 0.8458 Val F1: 0.8416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 8/10 Train Loss: 0.5788 Train Acc: 0.7667 Train Prec: 0.8634 Train Rcll: 0.8578 Train F1: 0.8568 Val Loss: 0.4785 Val Acc: 0.7657 Val Prec: 0.8582 Val Rcll: 0.8549 Val F1: 0.8523\n",
      "EPOCH: 9/10 Train Loss: 0.5564 Train Acc: 0.7734 Train Prec: 0.8669 Train Rcll: 0.8627 Train F1: 0.8622 Val Loss: 0.4658 Val Acc: 0.7663 Val Prec: 0.8561 Val Rcll: 0.8551 Val F1: 0.8528\n",
      "EPOCH: 10/10 Train Loss: 0.5171 Train Acc: 0.7662 Train Prec: 0.8618 Train Rcll: 0.8581 Train F1: 0.8563 Val Loss: 0.4643 Val Acc: 0.7598 Val Prec: 0.8505 Val Rcll: 0.8498 Val F1: 0.8464\n",
      "\n",
      "SPLIT 10/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.09137931 0.84325044 6.41554054 0.88079777 0.90428571 0.84325044\n",
      " 0.86161525 0.86633212]\n",
      "EPOCH: 1/10 Train Loss: 2.0155 Train Acc: 0.1296 Train Prec: 0.3618 Train Rcll: 0.3034 Train F1: 0.1974 Val Loss: 1.8803 Val Acc: 0.1287 Val Prec: 0.2886 Val Rcll: 0.3038 Val F1: 0.1972\n",
      "EPOCH: 2/10 Train Loss: 1.6198 Train Acc: 0.4207 Train Prec: 0.5409 Train Rcll: 0.5510 Train F1: 0.5250 Val Loss: 1.2495 Val Acc: 0.4059 Val Prec: 0.5510 Val Rcll: 0.5334 Val F1: 0.5121\n",
      "EPOCH: 3/10 Train Loss: 1.1598 Train Acc: 0.6033 Train Prec: 0.7062 Train Rcll: 0.7115 Train F1: 0.6986 Val Loss: 0.9641 Val Acc: 0.5852 Val Prec: 0.6858 Val Rcll: 0.6959 Val F1: 0.6798\n",
      "EPOCH: 4/10 Train Loss: 0.9469 Train Acc: 0.6608 Train Prec: 0.7658 Train Rcll: 0.7701 Train F1: 0.7578 Val Loss: 0.8063 Val Acc: 0.6482 Val Prec: 0.7628 Val Rcll: 0.7639 Val F1: 0.7476\n",
      "EPOCH: 5/10 Train Loss: 0.8177 Train Acc: 0.6862 Train Prec: 0.7862 Train Rcll: 0.7933 Train F1: 0.7789 Val Loss: 0.6990 Val Acc: 0.6732 Val Prec: 0.7729 Val Rcll: 0.7862 Val F1: 0.7677\n",
      "EPOCH: 6/10 Train Loss: 0.7208 Train Acc: 0.7305 Train Prec: 0.8440 Train Rcll: 0.8273 Train F1: 0.8228 Val Loss: 0.6263 Val Acc: 0.7198 Val Prec: 0.8370 Val Rcll: 0.8231 Val F1: 0.8206\n",
      "EPOCH: 7/10 Train Loss: 0.6471 Train Acc: 0.7376 Train Prec: 0.8495 Train Rcll: 0.8343 Train F1: 0.8285 Val Loss: 0.5772 Val Acc: 0.7312 Val Prec: 0.8442 Val Rcll: 0.8340 Val F1: 0.8292\n",
      "EPOCH: 8/10 Train Loss: 0.6176 Train Acc: 0.7599 Train Prec: 0.8607 Train Rcll: 0.8493 Train F1: 0.8488 Val Loss: 0.5359 Val Acc: 0.7558 Val Prec: 0.8543 Val Rcll: 0.8515 Val F1: 0.8489\n",
      "EPOCH: 9/10 Train Loss: 0.5655 Train Acc: 0.7645 Train Prec: 0.8693 Train Rcll: 0.8522 Train F1: 0.8535 Val Loss: 0.5096 Val Acc: 0.7629 Val Prec: 0.8636 Val Rcll: 0.8555 Val F1: 0.8550\n",
      "EPOCH: 10/10 Train Loss: 0.5508 Train Acc: 0.7677 Train Prec: 0.8720 Train Rcll: 0.8550 Train F1: 0.8562 Val Loss: 0.4925 Val Acc: 0.7686 Val Prec: 0.8671 Val Rcll: 0.8582 Val F1: 0.8590\n",
      "SUBJECT 1 VALIDATION RESULTS: \n",
      "Accuracy: 0.7769071198722272\n",
      "Precision: 0.8737303279053674\n",
      "Recall: 0.8633179020208176\n",
      "F1: 0.8643143022510158\n",
      "\n",
      "VALIDATION RESULTS (PER CLASS): \n",
      "\n",
      "Accuracy:\n",
      "   climbing_down: 0.5690321136112335\n",
      "   climbing_up: 0.5543818836493382\n",
      "   jumping: 0.9380306781638635\n",
      "   lying: 0.9631874724825187\n",
      "   running: 0.9733279949257805\n",
      "   sitting: 0.8376285306534443\n",
      "   standing: 0.7415229223491129\n",
      "   walking: 0.6381453631425258\n",
      "\n",
      "Precision:\n",
      "   climbing_down: 0.8212919128725966\n",
      "   climbing_up: 0.7763143653420921\n",
      "   jumping: 0.9740296964939821\n",
      "   lying: 0.9948789159713994\n",
      "   running: 0.994198388508473\n",
      "   sitting: 0.8938647408475653\n",
      "   standing: 0.820413282167945\n",
      "   walking: 0.7148513210388866\n",
      "\n",
      "Recall:\n",
      "   climbing_down: 0.6618556701030928\n",
      "   climbing_up: 0.6608000000000002\n",
      "   jumping: 0.9620000000000001\n",
      "   lying: 0.967966573816156\n",
      "   running: 0.9788571428571429\n",
      "   sitting: 0.9300531914893616\n",
      "   standing: 0.8855585831062671\n",
      "   walking: 0.8594520547945205\n",
      "\n",
      "F1:\n",
      "   climbing_down: 0.7247564352371514\n",
      "   climbing_up: 0.7124756402901224\n",
      "   jumping: 0.9677936801712249\n",
      "   lying: 0.9812257957886041\n",
      "   running: 0.9864555030431432\n",
      "   sitting: 0.9115461387975904\n",
      "   standing: 0.8514157191755427\n",
      "   walking: 0.7788455055047472\n",
      "\n",
      "GENERALIZATION GAP ANALYSIS: \n",
      "\n",
      "Accuracy: [ 0.0139145   0.00908165  0.00364704 -0.00089807  0.00065833  0.01711924\n",
      "  0.00944592  0.00428858]\n",
      "Precision: [ 0.0184615   0.01547267  0.00937811  0.0014929   0.00270148  0.00976393\n",
      "  0.00170035 -0.00056916]\n",
      "Recall: [ 0.00802939  0.00136696 -0.00524324 -0.00228939 -0.00190476  0.01044414\n",
      "  0.01153761  0.01171583]\n",
      "F1: [ 0.01144788  0.00772729  0.00205846 -0.00044737  0.00035806  0.0101204\n",
      "  0.00628735  0.00324851]\n",
      "\n",
      " VALIDATING FOR SUBJECT 2 OF 2\n",
      "(208835, 5)\n",
      "(5966, 50, 4) (5966,)\n",
      "\n",
      "SPLIT 1/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.05264706 1.03799304 5.45579268 0.84093045 0.85052281 0.84569943\n",
      " 0.84890892 0.84890892]\n",
      "EPOCH: 1/10 Train Loss: 1.9570 Train Acc: 0.2550 Train Prec: 0.3741 Train Rcll: 0.4177 Train F1: 0.3571 Val Loss: 1.7379 Val Acc: 0.2485 Val Prec: 0.3638 Val Rcll: 0.4054 Val F1: 0.3460\n",
      "EPOCH: 2/10 Train Loss: 1.4840 Train Acc: 0.4668 Train Prec: 0.6395 Train Rcll: 0.6039 Train F1: 0.5594 Val Loss: 1.1284 Val Acc: 0.4542 Val Prec: 0.6187 Val Rcll: 0.5982 Val F1: 0.5497\n",
      "EPOCH: 3/10 Train Loss: 1.0560 Train Acc: 0.5669 Train Prec: 0.7472 Train Rcll: 0.7124 Train F1: 0.6661 Val Loss: 0.8645 Val Acc: 0.5358 Val Prec: 0.7032 Val Rcll: 0.6817 Val F1: 0.6393\n",
      "EPOCH: 4/10 Train Loss: 0.8593 Train Acc: 0.6816 Train Prec: 0.7896 Train Rcll: 0.7878 Train F1: 0.7780 Val Loss: 0.7068 Val Acc: 0.6496 Val Prec: 0.7579 Val Rcll: 0.7657 Val F1: 0.7523\n",
      "EPOCH: 5/10 Train Loss: 0.7257 Train Acc: 0.7065 Train Prec: 0.8116 Train Rcll: 0.8103 Train F1: 0.7986 Val Loss: 0.6237 Val Acc: 0.6711 Val Prec: 0.7801 Val Rcll: 0.7859 Val F1: 0.7709\n",
      "EPOCH: 6/10 Train Loss: 0.6510 Train Acc: 0.7165 Train Prec: 0.8280 Train Rcll: 0.8158 Train F1: 0.8085 Val Loss: 0.5863 Val Acc: 0.6767 Val Prec: 0.7888 Val Rcll: 0.7931 Val F1: 0.7764\n",
      "EPOCH: 7/10 Train Loss: 0.5944 Train Acc: 0.7336 Train Prec: 0.8354 Train Rcll: 0.8268 Train F1: 0.8208 Val Loss: 0.5387 Val Acc: 0.7056 Val Prec: 0.8109 Val Rcll: 0.8130 Val F1: 0.8007\n",
      "EPOCH: 8/10 Train Loss: 0.5642 Train Acc: 0.7517 Train Prec: 0.8500 Train Rcll: 0.8383 Train F1: 0.8355 Val Loss: 0.5088 Val Acc: 0.7221 Val Prec: 0.8267 Val Rcll: 0.8196 Val F1: 0.8133\n",
      "EPOCH: 9/10 Train Loss: 0.5329 Train Acc: 0.7555 Train Prec: 0.8610 Train Rcll: 0.8381 Train F1: 0.8367 Val Loss: 0.5016 Val Acc: 0.7243 Val Prec: 0.8326 Val Rcll: 0.8179 Val F1: 0.8128\n",
      "EPOCH: 10/10 Train Loss: 0.5142 Train Acc: 0.7706 Train Prec: 0.8635 Train Rcll: 0.8499 Train F1: 0.8503 Val Loss: 0.4772 Val Acc: 0.7450 Val Prec: 0.8434 Val Rcll: 0.8363 Val F1: 0.8318\n",
      "\n",
      "SPLIT 2/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.05264706 1.03799304 5.45579268 0.84093045 0.85052281 0.84569943\n",
      " 0.84890892 0.84890892]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1/10 Train Loss: 1.9898 Train Acc: 0.2362 Train Prec: 0.4065 Train Rcll: 0.4054 Train F1: 0.3431 Val Loss: 1.7879 Val Acc: 0.2470 Val Prec: 0.4502 Val Rcll: 0.4191 Val F1: 0.3607\n",
      "EPOCH: 2/10 Train Loss: 1.5166 Train Acc: 0.4667 Train Prec: 0.6045 Train Rcll: 0.5709 Train F1: 0.5608 Val Loss: 1.1623 Val Acc: 0.4469 Val Prec: 0.5814 Val Rcll: 0.5471 Val F1: 0.5375\n",
      "EPOCH: 3/10 Train Loss: 1.0495 Train Acc: 0.6277 Train Prec: 0.7519 Train Rcll: 0.7422 Train F1: 0.7328 Val Loss: 0.8619 Val Acc: 0.5928 Val Prec: 0.7118 Val Rcll: 0.7070 Val F1: 0.6985\n",
      "EPOCH: 4/10 Train Loss: 0.8305 Train Acc: 0.6864 Train Prec: 0.7852 Train Rcll: 0.7938 Train F1: 0.7825 Val Loss: 0.6990 Val Acc: 0.6613 Val Prec: 0.7658 Val Rcll: 0.7737 Val F1: 0.7625\n",
      "EPOCH: 5/10 Train Loss: 0.6918 Train Acc: 0.7097 Train Prec: 0.8020 Train Rcll: 0.8118 Train F1: 0.8029 Val Loss: 0.6011 Val Acc: 0.6913 Val Prec: 0.7900 Val Rcll: 0.7979 Val F1: 0.7896\n",
      "EPOCH: 6/10 Train Loss: 0.6086 Train Acc: 0.7216 Train Prec: 0.8289 Train Rcll: 0.8215 Train F1: 0.8117 Val Loss: 0.5626 Val Acc: 0.6992 Val Prec: 0.8119 Val Rcll: 0.8041 Val F1: 0.7940\n",
      "EPOCH: 7/10 Train Loss: 0.5684 Train Acc: 0.7304 Train Prec: 0.8227 Train Rcll: 0.8274 Train F1: 0.8201 Val Loss: 0.5172 Val Acc: 0.7204 Val Prec: 0.8177 Val Rcll: 0.8221 Val F1: 0.8145\n",
      "EPOCH: 8/10 Train Loss: 0.5453 Train Acc: 0.7431 Train Prec: 0.8452 Train Rcll: 0.8412 Train F1: 0.8320 Val Loss: 0.5105 Val Acc: 0.7216 Val Prec: 0.8321 Val Rcll: 0.8243 Val F1: 0.8151\n",
      "EPOCH: 9/10 Train Loss: 0.5091 Train Acc: 0.7436 Train Prec: 0.8314 Train Rcll: 0.8422 Train F1: 0.8330 Val Loss: 0.4758 Val Acc: 0.7318 Val Prec: 0.8239 Val Rcll: 0.8326 Val F1: 0.8248\n",
      "EPOCH: 10/10 Train Loss: 0.4939 Train Acc: 0.7591 Train Prec: 0.8560 Train Rcll: 0.8512 Train F1: 0.8449 Val Loss: 0.4655 Val Acc: 0.7390 Val Prec: 0.8378 Val Rcll: 0.8397 Val F1: 0.8311\n",
      "\n",
      "SPLIT 3/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.05264706 1.03799304 5.45579268 0.84093045 0.85052281 0.84569943\n",
      " 0.84890892 0.84890892]\n",
      "EPOCH: 1/10 Train Loss: 1.9688 Train Acc: 0.2249 Train Prec: 0.3594 Train Rcll: 0.4037 Train F1: 0.3459 Val Loss: 1.7843 Val Acc: 0.2154 Val Prec: 0.3661 Val Rcll: 0.3911 Val F1: 0.3354\n",
      "EPOCH: 2/10 Train Loss: 1.5295 Train Acc: 0.4682 Train Prec: 0.5949 Train Rcll: 0.6388 Train F1: 0.5815 Val Loss: 1.1498 Val Acc: 0.4669 Val Prec: 0.5855 Val Rcll: 0.6311 Val F1: 0.5756\n",
      "EPOCH: 3/10 Train Loss: 1.0342 Train Acc: 0.6741 Train Prec: 0.7636 Train Rcll: 0.7801 Train F1: 0.7705 Val Loss: 0.8059 Val Acc: 0.6766 Val Prec: 0.7669 Val Rcll: 0.7741 Val F1: 0.7696\n",
      "EPOCH: 4/10 Train Loss: 0.7947 Train Acc: 0.7029 Train Prec: 0.7894 Train Rcll: 0.8012 Train F1: 0.7944 Val Loss: 0.6459 Val Acc: 0.7114 Val Prec: 0.7984 Val Rcll: 0.8004 Val F1: 0.7982\n",
      "EPOCH: 5/10 Train Loss: 0.6647 Train Acc: 0.7143 Train Prec: 0.7971 Train Rcll: 0.8118 Train F1: 0.8035 Val Loss: 0.5677 Val Acc: 0.7169 Val Prec: 0.8009 Val Rcll: 0.8088 Val F1: 0.8043\n",
      "EPOCH: 6/10 Train Loss: 0.5984 Train Acc: 0.7295 Train Prec: 0.8132 Train Rcll: 0.8235 Train F1: 0.8174 Val Loss: 0.5258 Val Acc: 0.7268 Val Prec: 0.8099 Val Rcll: 0.8212 Val F1: 0.8149\n",
      "EPOCH: 7/10 Train Loss: 0.5697 Train Acc: 0.7233 Train Prec: 0.8086 Train Rcll: 0.8198 Train F1: 0.8117 Val Loss: 0.5100 Val Acc: 0.7224 Val Prec: 0.8056 Val Rcll: 0.8109 Val F1: 0.8071\n",
      "EPOCH: 8/10 Train Loss: 0.5407 Train Acc: 0.7536 Train Prec: 0.8357 Train Rcll: 0.8402 Train F1: 0.8372 Val Loss: 0.4833 Val Acc: 0.7479 Val Prec: 0.8320 Val Rcll: 0.8309 Val F1: 0.8307\n",
      "EPOCH: 9/10 Train Loss: 0.5207 Train Acc: 0.7597 Train Prec: 0.8430 Train Rcll: 0.8455 Train F1: 0.8429 Val Loss: 0.4675 Val Acc: 0.7571 Val Prec: 0.8402 Val Rcll: 0.8426 Val F1: 0.8403\n",
      "EPOCH: 10/10 Train Loss: 0.5062 Train Acc: 0.7579 Train Prec: 0.8391 Train Rcll: 0.8453 Train F1: 0.8416 Val Loss: 0.4619 Val Acc: 0.7463 Val Prec: 0.8287 Val Rcll: 0.8390 Val F1: 0.8330\n",
      "\n",
      "SPLIT 4/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.05264706 1.03799304 5.45579268 0.84093045 0.85052281 0.84569943\n",
      " 0.84890892 0.84890892]\n",
      "EPOCH: 1/10 Train Loss: 1.9719 Train Acc: 0.2919 Train Prec: 0.4329 Train Rcll: 0.4559 Train F1: 0.3842 Val Loss: 1.7642 Val Acc: 0.2976 Val Prec: 0.4736 Val Rcll: 0.4637 Val F1: 0.3941\n",
      "EPOCH: 2/10 Train Loss: 1.4938 Train Acc: 0.4981 Train Prec: 0.6502 Train Rcll: 0.6608 Train F1: 0.6083 Val Loss: 1.1337 Val Acc: 0.4905 Val Prec: 0.6325 Val Rcll: 0.6483 Val F1: 0.5983\n",
      "EPOCH: 3/10 Train Loss: 1.0335 Train Acc: 0.6024 Train Prec: 0.7092 Train Rcll: 0.7497 Train F1: 0.7108 Val Loss: 0.8103 Val Acc: 0.6030 Val Prec: 0.7144 Val Rcll: 0.7489 Val F1: 0.7094\n",
      "EPOCH: 4/10 Train Loss: 0.7935 Train Acc: 0.6880 Train Prec: 0.7765 Train Rcll: 0.7923 Train F1: 0.7820 Val Loss: 0.6569 Val Acc: 0.6742 Val Prec: 0.7662 Val Rcll: 0.7805 Val F1: 0.7698\n",
      "EPOCH: 5/10 Train Loss: 0.6922 Train Acc: 0.7113 Train Prec: 0.8002 Train Rcll: 0.8123 Train F1: 0.8033 Val Loss: 0.5790 Val Acc: 0.6974 Val Prec: 0.7883 Val Rcll: 0.7981 Val F1: 0.7902\n",
      "EPOCH: 6/10 Train Loss: 0.6252 Train Acc: 0.7105 Train Prec: 0.7972 Train Rcll: 0.8103 Train F1: 0.8023 Val Loss: 0.5513 Val Acc: 0.7047 Val Prec: 0.7933 Val Rcll: 0.7991 Val F1: 0.7947\n",
      "EPOCH: 7/10 Train Loss: 0.5814 Train Acc: 0.7262 Train Prec: 0.8201 Train Rcll: 0.8225 Train F1: 0.8159 Val Loss: 0.5196 Val Acc: 0.7082 Val Prec: 0.8061 Val Rcll: 0.8019 Val F1: 0.7987\n",
      "EPOCH: 8/10 Train Loss: 0.5499 Train Acc: 0.7499 Train Prec: 0.8543 Train Rcll: 0.8376 Train F1: 0.8334 Val Loss: 0.4885 Val Acc: 0.7266 Val Prec: 0.8335 Val Rcll: 0.8143 Val F1: 0.8128\n",
      "EPOCH: 9/10 Train Loss: 0.5185 Train Acc: 0.7536 Train Prec: 0.8454 Train Rcll: 0.8413 Train F1: 0.8381 Val Loss: 0.4647 Val Acc: 0.7485 Val Prec: 0.8368 Val Rcll: 0.8316 Val F1: 0.8295\n",
      "EPOCH: 10/10 Train Loss: 0.4977 Train Acc: 0.7714 Train Prec: 0.8600 Train Rcll: 0.8524 Train F1: 0.8517 Val Loss: 0.4497 Val Acc: 0.7601 Val Prec: 0.8481 Val Rcll: 0.8421 Val F1: 0.8413\n",
      "\n",
      "SPLIT 5/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.05264706 1.03799304 5.45579268 0.84093045 0.85052281 0.84569943\n",
      " 0.84890892 0.84890892]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1/10 Train Loss: 2.0209 Train Acc: 0.2396 Train Prec: 0.3334 Train Rcll: 0.3805 Train F1: 0.3171 Val Loss: 1.8872 Val Acc: 0.2314 Val Prec: 0.3898 Val Rcll: 0.3698 Val F1: 0.3061\n",
      "EPOCH: 2/10 Train Loss: 1.6074 Train Acc: 0.4821 Train Prec: 0.6437 Train Rcll: 0.6199 Train F1: 0.5838 Val Loss: 1.1944 Val Acc: 0.4673 Val Prec: 0.6320 Val Rcll: 0.6035 Val F1: 0.5689\n",
      "EPOCH: 3/10 Train Loss: 1.0744 Train Acc: 0.6098 Train Prec: 0.7519 Train Rcll: 0.7523 Train F1: 0.7093 Val Loss: 0.8416 Val Acc: 0.6055 Val Prec: 0.7441 Val Rcll: 0.7459 Val F1: 0.7038\n",
      "EPOCH: 4/10 Train Loss: 0.8473 Train Acc: 0.6853 Train Prec: 0.7917 Train Rcll: 0.8006 Train F1: 0.7799 Val Loss: 0.6830 Val Acc: 0.6632 Val Prec: 0.7700 Val Rcll: 0.7853 Val F1: 0.7627\n",
      "EPOCH: 5/10 Train Loss: 0.7169 Train Acc: 0.7241 Train Prec: 0.8312 Train Rcll: 0.8271 Train F1: 0.8168 Val Loss: 0.5942 Val Acc: 0.7061 Val Prec: 0.8149 Val Rcll: 0.8115 Val F1: 0.8007\n",
      "EPOCH: 6/10 Train Loss: 0.6464 Train Acc: 0.7274 Train Prec: 0.8273 Train Rcll: 0.8331 Train F1: 0.8209 Val Loss: 0.5461 Val Acc: 0.7062 Val Prec: 0.8073 Val Rcll: 0.8169 Val F1: 0.8014\n",
      "EPOCH: 7/10 Train Loss: 0.6029 Train Acc: 0.7354 Train Prec: 0.8421 Train Rcll: 0.8331 Train F1: 0.8247 Val Loss: 0.5198 Val Acc: 0.7280 Val Prec: 0.8361 Val Rcll: 0.8287 Val F1: 0.8180\n",
      "EPOCH: 8/10 Train Loss: 0.5791 Train Acc: 0.7327 Train Prec: 0.8225 Train Rcll: 0.8312 Train F1: 0.8213 Val Loss: 0.5144 Val Acc: 0.7081 Val Prec: 0.8016 Val Rcll: 0.8155 Val F1: 0.7998\n",
      "EPOCH: 9/10 Train Loss: 0.5510 Train Acc: 0.7689 Train Prec: 0.8540 Train Rcll: 0.8537 Train F1: 0.8520 Val Loss: 0.4855 Val Acc: 0.7396 Val Prec: 0.8283 Val Rcll: 0.8311 Val F1: 0.8272\n",
      "EPOCH: 10/10 Train Loss: 0.5350 Train Acc: 0.7683 Train Prec: 0.8591 Train Rcll: 0.8541 Train F1: 0.8522 Val Loss: 0.4667 Val Acc: 0.7468 Val Prec: 0.8394 Val Rcll: 0.8389 Val F1: 0.8335\n",
      "\n",
      "SPLIT 6/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.05264706 1.03799304 5.45579268 0.84093045 0.85052281 0.84569943\n",
      " 0.84890892 0.84890892]\n",
      "EPOCH: 1/10 Train Loss: 1.9845 Train Acc: 0.1931 Train Prec: 0.3507 Train Rcll: 0.3576 Train F1: 0.2736 Val Loss: 1.7915 Val Acc: 0.1898 Val Prec: 0.3001 Val Rcll: 0.3523 Val F1: 0.2662\n",
      "EPOCH: 2/10 Train Loss: 1.5122 Train Acc: 0.5117 Train Prec: 0.6294 Train Rcll: 0.6510 Train F1: 0.6086 Val Loss: 1.1391 Val Acc: 0.5029 Val Prec: 0.6190 Val Rcll: 0.6418 Val F1: 0.5986\n",
      "EPOCH: 3/10 Train Loss: 1.0114 Train Acc: 0.6293 Train Prec: 0.7366 Train Rcll: 0.7581 Train F1: 0.7253 Val Loss: 0.8157 Val Acc: 0.6166 Val Prec: 0.7266 Val Rcll: 0.7452 Val F1: 0.7137\n",
      "EPOCH: 4/10 Train Loss: 0.7863 Train Acc: 0.7091 Train Prec: 0.8002 Train Rcll: 0.8096 Train F1: 0.8002 Val Loss: 0.6708 Val Acc: 0.6778 Val Prec: 0.7806 Val Rcll: 0.7859 Val F1: 0.7765\n",
      "EPOCH: 5/10 Train Loss: 0.6587 Train Acc: 0.7227 Train Prec: 0.8323 Train Rcll: 0.8268 Train F1: 0.8136 Val Loss: 0.5974 Val Acc: 0.6957 Val Prec: 0.8149 Val Rcll: 0.8040 Val F1: 0.7918\n",
      "EPOCH: 6/10 Train Loss: 0.5782 Train Acc: 0.7491 Train Prec: 0.8330 Train Rcll: 0.8402 Train F1: 0.8342 Val Loss: 0.5461 Val Acc: 0.7164 Val Prec: 0.8122 Val Rcll: 0.8182 Val F1: 0.8117\n",
      "EPOCH: 7/10 Train Loss: 0.5478 Train Acc: 0.7644 Train Prec: 0.8578 Train Rcll: 0.8498 Train F1: 0.8462 Val Loss: 0.5200 Val Acc: 0.7321 Val Prec: 0.8335 Val Rcll: 0.8315 Val F1: 0.8240\n",
      "EPOCH: 8/10 Train Loss: 0.5260 Train Acc: 0.7674 Train Prec: 0.8629 Train Rcll: 0.8509 Train F1: 0.8481 Val Loss: 0.5061 Val Acc: 0.7364 Val Prec: 0.8431 Val Rcll: 0.8338 Val F1: 0.8268\n",
      "EPOCH: 9/10 Train Loss: 0.5045 Train Acc: 0.7699 Train Prec: 0.8531 Train Rcll: 0.8562 Train F1: 0.8518 Val Loss: 0.4877 Val Acc: 0.7390 Val Prec: 0.8324 Val Rcll: 0.8377 Val F1: 0.8303\n",
      "EPOCH: 10/10 Train Loss: 0.4917 Train Acc: 0.7898 Train Prec: 0.8806 Train Rcll: 0.8648 Train F1: 0.8653 Val Loss: 0.4794 Val Acc: 0.7568 Val Prec: 0.8568 Val Rcll: 0.8452 Val F1: 0.8421\n",
      "\n",
      "SPLIT 7/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.05264706 1.03799304 5.45579268 0.84093045 0.85052281 0.84569943\n",
      " 0.84890892 0.84890892]\n",
      "EPOCH: 1/10 Train Loss: 1.9759 Train Acc: 0.2103 Train Prec: 0.3696 Train Rcll: 0.3720 Train F1: 0.3015 Val Loss: 1.7890 Val Acc: 0.2104 Val Prec: 0.3648 Val Rcll: 0.3736 Val F1: 0.3013\n",
      "EPOCH: 2/10 Train Loss: 1.5646 Train Acc: 0.4754 Train Prec: 0.5813 Train Rcll: 0.6002 Train F1: 0.5745 Val Loss: 1.2064 Val Acc: 0.4798 Val Prec: 0.5908 Val Rcll: 0.6022 Val F1: 0.5798\n",
      "EPOCH: 3/10 Train Loss: 1.0819 Train Acc: 0.6116 Train Prec: 0.7225 Train Rcll: 0.7513 Train F1: 0.7200 Val Loss: 0.8795 Val Acc: 0.5983 Val Prec: 0.7008 Val Rcll: 0.7281 Val F1: 0.7001\n",
      "EPOCH: 4/10 Train Loss: 0.8597 Train Acc: 0.6756 Train Prec: 0.7664 Train Rcll: 0.7862 Train F1: 0.7675 Val Loss: 0.7383 Val Acc: 0.6607 Val Prec: 0.7509 Val Rcll: 0.7750 Val F1: 0.7521\n",
      "EPOCH: 5/10 Train Loss: 0.7528 Train Acc: 0.6814 Train Prec: 0.7797 Train Rcll: 0.7878 Train F1: 0.7716 Val Loss: 0.6508 Val Acc: 0.6631 Val Prec: 0.7627 Val Rcll: 0.7694 Val F1: 0.7519\n",
      "EPOCH: 6/10 Train Loss: 0.6634 Train Acc: 0.7220 Train Prec: 0.8074 Train Rcll: 0.8217 Train F1: 0.8133 Val Loss: 0.5856 Val Acc: 0.7144 Val Prec: 0.7976 Val Rcll: 0.8097 Val F1: 0.8027\n",
      "EPOCH: 7/10 Train Loss: 0.5955 Train Acc: 0.7306 Train Prec: 0.8178 Train Rcll: 0.8260 Train F1: 0.8180 Val Loss: 0.5487 Val Acc: 0.7220 Val Prec: 0.8092 Val Rcll: 0.8099 Val F1: 0.8078\n",
      "EPOCH: 8/10 Train Loss: 0.5691 Train Acc: 0.7505 Train Prec: 0.8305 Train Rcll: 0.8366 Train F1: 0.8329 Val Loss: 0.5285 Val Acc: 0.7330 Val Prec: 0.8171 Val Rcll: 0.8197 Val F1: 0.8178\n",
      "EPOCH: 9/10 Train Loss: 0.5264 Train Acc: 0.7636 Train Prec: 0.8490 Train Rcll: 0.8474 Train F1: 0.8450 Val Loss: 0.5034 Val Acc: 0.7420 Val Prec: 0.8313 Val Rcll: 0.8285 Val F1: 0.8276\n",
      "EPOCH: 10/10 Train Loss: 0.5250 Train Acc: 0.7714 Train Prec: 0.8507 Train Rcll: 0.8536 Train F1: 0.8514 Val Loss: 0.4925 Val Acc: 0.7522 Val Prec: 0.8371 Val Rcll: 0.8347 Val F1: 0.8352\n",
      "\n",
      "SPLIT 8/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.05264706 1.03799304 5.45579268 0.84093045 0.85052281 0.84569943\n",
      " 0.84890892 0.84890892]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1/10 Train Loss: 1.9943 Train Acc: 0.3048 Train Prec: 0.4722 Train Rcll: 0.4934 Train F1: 0.4152 Val Loss: 1.8292 Val Acc: 0.3004 Val Prec: 0.4304 Val Rcll: 0.4820 Val F1: 0.4052\n",
      "EPOCH: 2/10 Train Loss: 1.5914 Train Acc: 0.4402 Train Prec: 0.5899 Train Rcll: 0.5992 Train F1: 0.5321 Val Loss: 1.2127 Val Acc: 0.4322 Val Prec: 0.5613 Val Rcll: 0.5853 Val F1: 0.5264\n",
      "EPOCH: 3/10 Train Loss: 1.0869 Train Acc: 0.5586 Train Prec: 0.6938 Train Rcll: 0.7136 Train F1: 0.6547 Val Loss: 0.8400 Val Acc: 0.5618 Val Prec: 0.6887 Val Rcll: 0.7111 Val F1: 0.6544\n",
      "EPOCH: 4/10 Train Loss: 0.8245 Train Acc: 0.6007 Train Prec: 0.7151 Train Rcll: 0.7490 Train F1: 0.7063 Val Loss: 0.6598 Val Acc: 0.6023 Val Prec: 0.7095 Val Rcll: 0.7455 Val F1: 0.7026\n",
      "EPOCH: 5/10 Train Loss: 0.6927 Train Acc: 0.6767 Train Prec: 0.7719 Train Rcll: 0.7908 Train F1: 0.7745 Val Loss: 0.5724 Val Acc: 0.6853 Val Prec: 0.7761 Val Rcll: 0.7985 Val F1: 0.7798\n",
      "EPOCH: 6/10 Train Loss: 0.6332 Train Acc: 0.7035 Train Prec: 0.7925 Train Rcll: 0.8120 Train F1: 0.7996 Val Loss: 0.5364 Val Acc: 0.7046 Val Prec: 0.7910 Val Rcll: 0.8122 Val F1: 0.7985\n",
      "EPOCH: 7/10 Train Loss: 0.5993 Train Acc: 0.7219 Train Prec: 0.8124 Train Rcll: 0.8195 Train F1: 0.8141 Val Loss: 0.5030 Val Acc: 0.7201 Val Prec: 0.8063 Val Rcll: 0.8220 Val F1: 0.8117\n",
      "EPOCH: 8/10 Train Loss: 0.5738 Train Acc: 0.7270 Train Prec: 0.8215 Train Rcll: 0.8308 Train F1: 0.8204 Val Loss: 0.4911 Val Acc: 0.7221 Val Prec: 0.8156 Val Rcll: 0.8310 Val F1: 0.8167\n",
      "EPOCH: 9/10 Train Loss: 0.5597 Train Acc: 0.7403 Train Prec: 0.8282 Train Rcll: 0.8377 Train F1: 0.8305 Val Loss: 0.4714 Val Acc: 0.7325 Val Prec: 0.8173 Val Rcll: 0.8351 Val F1: 0.8233\n",
      "EPOCH: 10/10 Train Loss: 0.5282 Train Acc: 0.7388 Train Prec: 0.8233 Train Rcll: 0.8347 Train F1: 0.8281 Val Loss: 0.4581 Val Acc: 0.7319 Val Prec: 0.8133 Val Rcll: 0.8353 Val F1: 0.8222\n",
      "\n",
      "SPLIT 9/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.05264706 1.03799304 5.45579268 0.84093045 0.85052281 0.84569943\n",
      " 0.84890892 0.84890892]\n",
      "EPOCH: 1/10 Train Loss: 2.0141 Train Acc: 0.1392 Train Prec: 0.2878 Train Rcll: 0.3216 Train F1: 0.2264 Val Loss: 1.8704 Val Acc: 0.1387 Val Prec: 0.2755 Val Rcll: 0.3145 Val F1: 0.2255\n",
      "EPOCH: 2/10 Train Loss: 1.6364 Train Acc: 0.4381 Train Prec: 0.5543 Train Rcll: 0.5979 Train F1: 0.5447 Val Loss: 1.2608 Val Acc: 0.4419 Val Prec: 0.5640 Val Rcll: 0.6005 Val F1: 0.5491\n",
      "EPOCH: 3/10 Train Loss: 1.1259 Train Acc: 0.5733 Train Prec: 0.7154 Train Rcll: 0.7117 Train F1: 0.6874 Val Loss: 0.8661 Val Acc: 0.5777 Val Prec: 0.7129 Val Rcll: 0.7131 Val F1: 0.6910\n",
      "EPOCH: 4/10 Train Loss: 0.8595 Train Acc: 0.6506 Train Prec: 0.7506 Train Rcll: 0.7829 Train F1: 0.7580 Val Loss: 0.6812 Val Acc: 0.6385 Val Prec: 0.7403 Val Rcll: 0.7786 Val F1: 0.7455\n",
      "EPOCH: 5/10 Train Loss: 0.7310 Train Acc: 0.7031 Train Prec: 0.7919 Train Rcll: 0.8126 Train F1: 0.7999 Val Loss: 0.5800 Val Acc: 0.6988 Val Prec: 0.7890 Val Rcll: 0.8063 Val F1: 0.7954\n",
      "EPOCH: 6/10 Train Loss: 0.6511 Train Acc: 0.7234 Train Prec: 0.8128 Train Rcll: 0.8241 Train F1: 0.8157 Val Loss: 0.5227 Val Acc: 0.7201 Val Prec: 0.8074 Val Rcll: 0.8178 Val F1: 0.8095\n",
      "EPOCH: 7/10 Train Loss: 0.5910 Train Acc: 0.7286 Train Prec: 0.8174 Train Rcll: 0.8298 Train F1: 0.8208 Val Loss: 0.4887 Val Acc: 0.7238 Val Prec: 0.8101 Val Rcll: 0.8230 Val F1: 0.8140\n",
      "EPOCH: 8/10 Train Loss: 0.5523 Train Acc: 0.7478 Train Prec: 0.8404 Train Rcll: 0.8417 Train F1: 0.8356 Val Loss: 0.4665 Val Acc: 0.7471 Val Prec: 0.8340 Val Rcll: 0.8413 Val F1: 0.8340\n",
      "EPOCH: 9/10 Train Loss: 0.5257 Train Acc: 0.7584 Train Prec: 0.8523 Train Rcll: 0.8489 Train F1: 0.8438 Val Loss: 0.4524 Val Acc: 0.7532 Val Prec: 0.8423 Val Rcll: 0.8458 Val F1: 0.8386\n",
      "EPOCH: 10/10 Train Loss: 0.5108 Train Acc: 0.7600 Train Prec: 0.8420 Train Rcll: 0.8464 Train F1: 0.8438 Val Loss: 0.4406 Val Acc: 0.7555 Val Prec: 0.8357 Val Rcll: 0.8421 Val F1: 0.8386\n",
      "\n",
      "SPLIT 10/10\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.05264706 1.03799304 5.45579268 0.84093045 0.85052281 0.84569943\n",
      " 0.84890892 0.84890892]\n",
      "EPOCH: 1/10 Train Loss: 1.9682 Train Acc: 0.2499 Train Prec: 0.3762 Train Rcll: 0.4274 Train F1: 0.3490 Val Loss: 1.7858 Val Acc: 0.2298 Val Prec: 0.3567 Val Rcll: 0.3958 Val F1: 0.3252\n",
      "EPOCH: 2/10 Train Loss: 1.5551 Train Acc: 0.4417 Train Prec: 0.5560 Train Rcll: 0.5835 Train F1: 0.5421 Val Loss: 1.2172 Val Acc: 0.4341 Val Prec: 0.5479 Val Rcll: 0.5696 Val F1: 0.5308\n",
      "EPOCH: 3/10 Train Loss: 1.1352 Train Acc: 0.5904 Train Prec: 0.7176 Train Rcll: 0.6945 Train F1: 0.6985 Val Loss: 0.9322 Val Acc: 0.5600 Val Prec: 0.7003 Val Rcll: 0.6615 Val F1: 0.6697\n",
      "EPOCH: 4/10 Train Loss: 0.9217 Train Acc: 0.6580 Train Prec: 0.7638 Train Rcll: 0.7629 Train F1: 0.7611 Val Loss: 0.7928 Val Acc: 0.6248 Val Prec: 0.7303 Val Rcll: 0.7336 Val F1: 0.7304\n",
      "EPOCH: 5/10 Train Loss: 0.8054 Train Acc: 0.6851 Train Prec: 0.7806 Train Rcll: 0.7864 Train F1: 0.7824 Val Loss: 0.6800 Val Acc: 0.6683 Val Prec: 0.7592 Val Rcll: 0.7741 Val F1: 0.7654\n",
      "EPOCH: 6/10 Train Loss: 0.7117 Train Acc: 0.6965 Train Prec: 0.7989 Train Rcll: 0.7949 Train F1: 0.7942 Val Loss: 0.6191 Val Acc: 0.6804 Val Prec: 0.7811 Val Rcll: 0.7785 Val F1: 0.7780\n",
      "EPOCH: 7/10 Train Loss: 0.6473 Train Acc: 0.7227 Train Prec: 0.8144 Train Rcll: 0.8190 Train F1: 0.8145 Val Loss: 0.5506 Val Acc: 0.7116 Val Prec: 0.7994 Val Rcll: 0.8128 Val F1: 0.8039\n",
      "EPOCH: 8/10 Train Loss: 0.6024 Train Acc: 0.7322 Train Prec: 0.8288 Train Rcll: 0.8283 Train F1: 0.8237 Val Loss: 0.5186 Val Acc: 0.7323 Val Prec: 0.8227 Val Rcll: 0.8276 Val F1: 0.8220\n",
      "EPOCH: 9/10 Train Loss: 0.5642 Train Acc: 0.7592 Train Prec: 0.8494 Train Rcll: 0.8500 Train F1: 0.8466 Val Loss: 0.5017 Val Acc: 0.7466 Val Prec: 0.8353 Val Rcll: 0.8405 Val F1: 0.8355\n",
      "EPOCH: 10/10 Train Loss: 0.5413 Train Acc: 0.7658 Train Prec: 0.8504 Train Rcll: 0.8525 Train F1: 0.8508 Val Loss: 0.4826 Val Acc: 0.7435 Val Prec: 0.8277 Val Rcll: 0.8379 Val F1: 0.8322\n",
      "SUBJECT 2 VALIDATION RESULTS: \n",
      "Accuracy: 0.7477152215478371\n",
      "Precision: 0.8368068912146787\n",
      "Recall: 0.8391077717977367\n",
      "F1: 0.8341160445105111\n",
      "\n",
      "VALIDATION RESULTS (PER CLASS): \n",
      "\n",
      "Accuracy:\n",
      "   climbing_down: 0.42848827763551506\n",
      "   climbing_up: 0.4176861423210398\n",
      "   jumping: 0.8230063794127102\n",
      "   lying: 0.9817780206777119\n",
      "   running: 0.94765981351038\n",
      "   sitting: 0.9621722191128598\n",
      "   standing: 0.8785754610223965\n",
      "   walking: 0.5423554586900836\n",
      "\n",
      "Precision:\n",
      "   climbing_down: 0.6946616437558807\n",
      "   climbing_up: 0.5920518942236155\n",
      "   jumping: 0.8644482194309964\n",
      "   lying: 0.99604655420606\n",
      "   running: 0.9958030596838212\n",
      "   sitting: 0.975961935885332\n",
      "   standing: 0.9074235744852551\n",
      "   walking: 0.6680582480464682\n",
      "\n",
      "Recall:\n",
      "   climbing_down: 0.5401408450704225\n",
      "   climbing_up: 0.5905923344947736\n",
      "   jumping: 0.9454545454545455\n",
      "   lying: 0.9856338028169015\n",
      "   running: 0.9514285714285714\n",
      "   sitting: 0.9855524079320113\n",
      "   standing: 0.9650568181818182\n",
      "   walking: 0.749002849002849\n",
      "\n",
      "F1:\n",
      "   climbing_down: 0.5995600687656759\n",
      "   climbing_up: 0.5885136199731242\n",
      "   jumping: 0.9021225660784884\n",
      "   lying: 0.9907992527812525\n",
      "   running: 0.9730692872183747\n",
      "   sitting: 0.9807054817972425\n",
      "   standing: 0.9353056129885695\n",
      "   walking: 0.7028524664813622\n",
      "\n",
      "GENERALIZATION GAP ANALYSIS: \n",
      "\n",
      "Accuracy: [ 0.02486895  0.03299572  0.06455762 -0.00167396  0.01664308 -0.00784969\n",
      " -0.01526293  0.02642875]\n",
      "Precision: [ 0.03324009  0.02551507  0.05466331  0.00261755  0.00301619 -0.0055976\n",
      " -0.01348879  0.02543109]\n",
      "Recall: [ 0.01515327  0.03933806  0.01674058 -0.00424283  0.01397067 -0.00256564\n",
      " -0.00319724  0.01589279]\n",
      "F1: [ 0.02397208  0.03235424  0.03764176 -0.00084954  0.00873469 -0.00408287\n",
      " -0.00868323  0.02209541]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "# size of the train portion within each split and number of splits per subject\n",
    "config['size_sss'] = 0.6\n",
    "config['splits_sss'] = 10\n",
    "\n",
    "# needed for saving results\n",
    "log_date = time.strftime('%Y%m%d')\n",
    "log_timestamp = time.strftime('%H%M%S')\n",
    "\n",
    "# iterate over all subjects\n",
    "for i, sbj in enumerate(np.unique(train_valid_data.iloc[:, 0])):\n",
    "    print('\\n VALIDATING FOR SUBJECT {0} OF {1}'.format(int(sbj) + 1, int(np.max(train_valid_data.iloc[:, 0])) + 1))\n",
    "    \n",
    "    # define the stratified shuffle split object for the current subject\n",
    "    # pass it the size of the train portion of the split, number of splits and seed\n",
    "    sss = StratifiedShuffleSplit(train_size=config['size_sss'],\n",
    "                                 n_splits=config['splits_sss'],\n",
    "                                 random_state=config['seed'])\n",
    "    \n",
    "    # define the subject data by filtering the train + valid dataset for the identifier of the current subject \n",
    "    subject_data = train_valid_data[train_valid_data.iloc[:, 0] == sbj]\n",
    "    \n",
    "    print(subject_data.shape)\n",
    "    \n",
    "    # apply the sliding window on top of both the subject data; use the \"apply_sliding_window\" function\n",
    "    # found in data_processing.sliding_window \n",
    "    X_subject, y_subject = apply_sliding_window(subject_data.iloc[:, :-1], subject_data.iloc[:, -1],\n",
    "                                                        sliding_window_size=config['sw_length'],\n",
    "                                                        unit=config['sw_unit'],\n",
    "                                                        sampling_rate=config['sampling_rate'],\n",
    "                                                        sliding_window_overlap=config['sw_overlap'])\n",
    "\n",
    "    print(X_subject.shape, y_subject.shape)\n",
    "    \n",
    "    # omit the first feature column (subject_identifier) from the subject data\n",
    "    X_subject = X_subject[:, :, 1:]\n",
    "    \n",
    "    # result objects used for accumulating the scores across splits; add each split results to the objects so that\n",
    "    # they are averaged at the end of the stratified shuffle split loop\n",
    "    subject_accuracy = np.zeros(config['nb_classes'])\n",
    "    subject_precision = np.zeros(config['nb_classes'])\n",
    "    subject_recall = np.zeros(config['nb_classes'])\n",
    "    subject_f1 = np.zeros(config['nb_classes'])\n",
    "    \n",
    "    subject_accuracy_gap = 0\n",
    "    subject_precision_gap = 0\n",
    "    subject_recall_gap = 0\n",
    "    subject_f1_gap = 0\n",
    "    \n",
    "    # stratified shuffle split validation loop; for each loop iteration returns a split identifier and indeces \n",
    "    # which can be used to split the subject data into train and validation data according to the current split\n",
    "    for j, (train_index, test_index) in enumerate(sss.split(X_subject, y_subject)):\n",
    "        print('\\nSPLIT {0}/{1}'.format(j + 1, config['splits_sss']))\n",
    "\n",
    "        # split the data into train and validation data; to do so, use the indeces produces by the split function\n",
    "        X_train, X_valid = X_subject[train_index], X_subject[test_index]\n",
    "        y_train, y_valid = y_subject[train_index], y_subject[test_index]\n",
    "        \n",
    "        # within the config file, set the parameters 'window_size' and 'nb_channels' accordingly\n",
    "        # window_size = size of the sliding window in units\n",
    "        # nb_channels = number of feature channels\n",
    "        config['window_size'] = X_train.shape[1]\n",
    "        config['nb_channels'] = X_train.shape[2]\n",
    "        \n",
    "        # define the network to be a DeepConvLSTM object; can be imported from model.DeepConvLSTM\n",
    "        # pass it the config object\n",
    "        net = DeepConvLSTM(config=config)\n",
    "        \n",
    "        # convert the features of the train and validation to float32 and labels to uint8 for GPU compatibility \n",
    "        X_train, y_train,  = X_train.astype(np.float32), y_train.astype(np.uint8)\n",
    "        X_valid, y_valid = X_valid.astype(np.float32), y_valid.astype(np.uint8)\n",
    "        \n",
    "        # feed the datasets into the train function; can be imported from model.train\n",
    "        per_participant_net, val_output, train_output = train(X_train, y_train, X_valid, y_valid, network=net, \n",
    "                                                              config=config, log_date=log_date, \n",
    "                                                              log_timestamp=log_timestamp)\n",
    "        \n",
    "        # in the following validation and train evaluation metrics are calculated\n",
    "        cls = np.array(range(config['nb_classes']))\n",
    "        val_accuracy = jaccard_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "        val_precision = precision_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "        val_recall = recall_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "        val_f1 = f1_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)\n",
    "        train_accuracy = jaccard_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "        train_precision = precision_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "        train_recall = recall_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "        train_f1 = f1_score(train_output[:, 1], train_output[:, 0], average=None, labels=cls)\n",
    "        \n",
    "        # add up the fold results\n",
    "        subject_accuracy += val_accuracy\n",
    "        subject_precision += val_precision\n",
    "        subject_recall += val_recall\n",
    "        subject_f1 += val_f1\n",
    "\n",
    "        # add up train val gap evaluation\n",
    "        subject_accuracy_gap += train_accuracy - val_accuracy\n",
    "        subject_precision_gap += train_precision - val_precision\n",
    "        subject_recall_gap += train_recall - val_recall\n",
    "        subject_f1_gap += train_f1 - val_f1\n",
    "    \n",
    "    # the next bit prints out the average results per subject if you did everything correctly\n",
    "    print(\"\\nSUBJECT {0} VALIDATION RESULTS: \".format(int(sbj) + 1))\n",
    "    print(\"Accuracy: {0}\".format(np.mean(subject_accuracy / config['splits_sss'])))\n",
    "    print(\"Precision: {0}\".format(np.mean(subject_precision / config['splits_sss'])))\n",
    "    print(\"Recall: {0}\".format(np.mean(subject_recall / config['splits_sss'])))\n",
    "    print(\"F1: {0}\".format(np.mean(subject_f1 / config['splits_sss'])))\n",
    "    \n",
    "    print(\"\\nVALIDATION RESULTS (PER CLASS): \")\n",
    "    print(\"\\nAccuracy:\")\n",
    "    for i, rslt in enumerate(subject_accuracy / config['splits_sss']):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    print(\"\\nPrecision:\")\n",
    "    for i, rslt in enumerate(subject_precision / config['splits_sss']):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    print(\"\\nRecall:\")\n",
    "    for i, rslt in enumerate(subject_recall / config['splits_sss']):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    print(\"\\nF1:\")\n",
    "    for i, rslt in enumerate(subject_f1 / config['splits_sss']):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    \n",
    "    print(\"\\nGENERALIZATION GAP ANALYSIS: \")\n",
    "    print(\"\\nAccuracy: {0}\".format(subject_accuracy_gap / config['splits_sss']))\n",
    "    print(\"Precision: {0}\".format(subject_precision_gap / config['splits_sss']))\n",
    "    print(\"Recall: {0}\".format(subject_recall_gap / config['splits_sss']))\n",
    "    print(\"F1: {0}\".format(subject_f1_gap / config['splits_sss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Cross-Participant Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Participant cross-validation, also known as Leave-One-Subject-Out Cross-Validation is the most complex, but also most expressive validation method one can apply when dealing with multi-subject data. In general, it can be seen as a variation of the k-fold cross-validation, where each fold is the data of one subject. This way, each subject is treated as the unseen data at least once. \n",
    "\n",
    "Leaving one subject out each fold ensures that the overall evaluation of the algorithm does not overfit on subject-specific traits, i.e. how subjects performed the activities individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Implementing the cross-participant CV loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define a for loop which iterates over all subjects\n",
    "2. Define the train data to be everything but the current subject data and the validation data to be the subject data by filtering the train + valid data\n",
    "3. Apply the sliding window on top of the train and validation data and omit the 'subject_identifier' column from both datasets\n",
    "4. Run the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " VALIDATING FOR SUBJECT 1 OF 2\n",
      "(437639, 5) (221621, 5)\n",
      "(12502, 50, 4) (12502,)\n",
      "(6331, 50, 4) (6331,)\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.04044607 1.00112108 5.50264085 0.8735327  0.79732143 0.87696409\n",
      " 0.88241107 0.84336212]\n",
      "EPOCH: 1/10 Train Loss: 2.1254 Train Acc: 0.2643 Train Prec: 0.2804 Train Rcll: 0.4210 Train F1: 0.3243 Val Loss: 1.9224 Val Acc: 0.1641 Val Prec: 0.1722 Val Rcll: 0.3151 Val F1: 0.2091\n",
      "EPOCH: 2/10 Train Loss: 1.9208 Train Acc: 0.1878 Train Prec: 0.2710 Train Rcll: 0.3559 Train F1: 0.2691 Val Loss: 1.7222 Val Acc: 0.2126 Val Prec: 0.2633 Val Rcll: 0.3910 Val F1: 0.2957\n",
      "EPOCH: 3/10 Train Loss: 1.6549 Train Acc: 0.3011 Train Prec: 0.3991 Train Rcll: 0.4075 Train F1: 0.3391 Val Loss: 1.6762 Val Acc: 0.1000 Val Prec: 0.1701 Val Rcll: 0.2530 Val F1: 0.1515\n",
      "EPOCH: 4/10 Train Loss: 1.5424 Train Acc: 0.2423 Train Prec: 0.3125 Train Rcll: 0.3870 Train F1: 0.3184 Val Loss: 1.5178 Val Acc: 0.2941 Val Prec: 0.3514 Val Rcll: 0.4329 Val F1: 0.3548\n",
      "EPOCH: 5/10 Train Loss: 1.3788 Train Acc: 0.1856 Train Prec: 0.3764 Train Rcll: 0.3433 Train F1: 0.2604 Val Loss: 1.4338 Val Acc: 0.1759 Val Prec: 0.2416 Val Rcll: 0.3471 Val F1: 0.2227\n",
      "EPOCH: 6/10 Train Loss: 1.3032 Train Acc: 0.3588 Train Prec: 0.4976 Train Rcll: 0.4988 Train F1: 0.4365 Val Loss: 1.4039 Val Acc: 0.1848 Val Prec: 0.1899 Val Rcll: 0.3631 Val F1: 0.2330\n",
      "EPOCH: 7/10 Train Loss: 1.2197 Train Acc: 0.4693 Train Prec: 0.6175 Train Rcll: 0.5805 Train F1: 0.5547 Val Loss: 1.6211 Val Acc: 0.1053 Val Prec: 0.1481 Val Rcll: 0.2631 Val F1: 0.1593\n",
      "EPOCH: 8/10 Train Loss: 1.1240 Train Acc: 0.4819 Train Prec: 0.6745 Train Rcll: 0.5763 Train F1: 0.5725 Val Loss: 1.2295 Val Acc: 0.2959 Val Prec: 0.5371 Val Rcll: 0.4739 Val F1: 0.3945\n",
      "EPOCH: 9/10 Train Loss: 1.0537 Train Acc: 0.5188 Train Prec: 0.6843 Train Rcll: 0.6151 Train F1: 0.5989 Val Loss: 1.0411 Val Acc: 0.4524 Val Prec: 0.5728 Val Rcll: 0.5995 Val F1: 0.5338\n",
      "EPOCH: 10/10 Train Loss: 0.9968 Train Acc: 0.5437 Train Prec: 0.6909 Train Rcll: 0.6406 Train F1: 0.6328 Val Loss: 0.9798 Val Acc: 0.4535 Val Prec: 0.5999 Val Rcll: 0.5951 Val F1: 0.5381\n",
      "\n",
      "VALIDATION RESULTS FOR SUBJECT 1: \n",
      "\n",
      "Avg. Accuracy: 0.4535314416137711\n",
      "Avg. Precision: 0.5999375184910536\n",
      "Avg. Recall: 0.595103782337471\n",
      "Avg. F1: 0.5381405360165193\n",
      "\n",
      "VALIDATION RESULTS (PER CLASS): \n",
      "\n",
      "Accuracy:\n",
      "   climbing_down: 0.27392871445734884\n",
      "   climbing_up: 0.0030991735537190084\n",
      "   jumping: 0.16216216216216217\n",
      "   lying: 0.92880978865406\n",
      "   running: 0.8264462809917356\n",
      "   sitting: 0.7271973466003316\n",
      "   standing: 0.6641000962463908\n",
      "   walking: 0.04250797024442083\n",
      "\n",
      "Precision:\n",
      "   climbing_down: 0.2786150712830957\n",
      "   climbing_up: 0.09090909090909091\n",
      "   jumping: 0.32967032967032966\n",
      "   lying: 0.9988038277511961\n",
      "   running: 0.8958566629339306\n",
      "   sitting: 0.7666083916083916\n",
      "   standing: 0.8508014796547472\n",
      "   walking: 0.5882352941176471\n",
      "\n",
      "Recall:\n",
      "   climbing_down: 0.9421487603305785\n",
      "   climbing_up: 0.0031982942430703624\n",
      "   jumping: 0.24193548387096775\n",
      "   lying: 0.9298440979955457\n",
      "   running: 0.9142857142857143\n",
      "   sitting: 0.933972310969116\n",
      "   standing: 0.7516339869281046\n",
      "   walking: 0.04381161007667032\n",
      "\n",
      "F1:\n",
      "   climbing_down: 0.43005344231373777\n",
      "   climbing_up: 0.006179196704428425\n",
      "   jumping: 0.27906976744186046\n",
      "   lying: 0.9630911188004614\n",
      "   running: 0.9049773755656109\n",
      "   sitting: 0.8420547287566009\n",
      "   standing: 0.7981492192018507\n",
      "   walking: 0.08154943934760447\n",
      "\n",
      "GENERALIZATION GAP ANALYSIS: \n",
      "\n",
      "Train-Val-Accuracy Difference: 0.09018345938780314\n",
      "Train-Val-Precision Difference: 0.09096385580290178\n",
      "Train-Val-Recall Difference: 0.04553650133691933\n",
      "Train-Val-F1 Difference: 0.09464319259789022\n",
      "\n",
      " VALIDATING FOR SUBJECT 2 OF 2\n",
      "(450425, 5) (208835, 5)\n",
      "(12867, 50, 4) (12867,)\n",
      "(5966, 50, 4) (5966,)\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_blocks.0.conv1.weight |    704     |\n",
      "|  conv_blocks.0.conv1.bias  |     64     |\n",
      "| conv_blocks.0.conv2.weight |   45056    |\n",
      "|  conv_blocks.0.conv2.bias  |     64     |\n",
      "| conv_blocks.1.conv1.weight |   45056    |\n",
      "|  conv_blocks.1.conv1.bias  |     64     |\n",
      "| conv_blocks.1.conv2.weight |   45056    |\n",
      "|  conv_blocks.1.conv2.bias  |     64     |\n",
      "| lstm_layers.0.weight_ih_l0 |   98304    |\n",
      "| lstm_layers.0.weight_hh_l0 |   65536    |\n",
      "|  lstm_layers.0.bias_ih_l0  |    512     |\n",
      "|  lstm_layers.0.bias_hh_l0  |    512     |\n",
      "|         fc.weight          |    1024    |\n",
      "|          fc.bias           |     8      |\n",
      "+----------------------------+------------+\n",
      "Total Params: 302024\n",
      "Applied weighted class weights: \n",
      "[1.05883805 0.90307412 5.9349631  0.89354167 0.82101838 0.87459217\n",
      " 0.88860497 0.85189354]\n",
      "EPOCH: 1/10 Train Loss: 2.1773 Train Acc: 0.1408 Train Prec: 0.2379 Train Rcll: 0.2503 Train F1: 0.1969 Val Loss: 2.0147 Val Acc: 0.2691 Val Prec: 0.2740 Val Rcll: 0.3701 Val F1: 0.2895\n",
      "EPOCH: 2/10 Train Loss: 2.0674 Train Acc: 0.1048 Train Prec: 0.1109 Train Rcll: 0.2752 Train F1: 0.1579 Val Loss: 1.8672 Val Acc: 0.0814 Val Prec: 0.0892 Val Rcll: 0.2482 Val F1: 0.1254\n",
      "EPOCH: 3/10 Train Loss: 1.8521 Train Acc: 0.1311 Train Prec: 0.1386 Train Rcll: 0.3140 Train F1: 0.1895 Val Loss: 1.6870 Val Acc: 0.0695 Val Prec: 0.0794 Val Rcll: 0.2035 Val F1: 0.1121\n",
      "EPOCH: 4/10 Train Loss: 1.7478 Train Acc: 0.1301 Train Prec: 0.1539 Train Rcll: 0.3572 Train F1: 0.1941 Val Loss: 1.7519 Val Acc: 0.0734 Val Prec: 0.0916 Val Rcll: 0.2516 Val F1: 0.1050\n",
      "EPOCH: 5/10 Train Loss: 1.5321 Train Acc: 0.1904 Train Prec: 0.2597 Train Rcll: 0.4100 Train F1: 0.2725 Val Loss: 1.6607 Val Acc: 0.2539 Val Prec: 0.2620 Val Rcll: 0.3830 Val F1: 0.2716\n",
      "EPOCH: 6/10 Train Loss: 1.4694 Train Acc: 0.3392 Train Prec: 0.3712 Train Rcll: 0.4539 Train F1: 0.3947 Val Loss: 1.4365 Val Acc: 0.2464 Val Prec: 0.2526 Val Rcll: 0.3307 Val F1: 0.2677\n",
      "EPOCH: 7/10 Train Loss: 1.3181 Train Acc: 0.5064 Train Prec: 0.6658 Train Rcll: 0.6258 Train F1: 0.6010 Val Loss: 1.3878 Val Acc: 0.2320 Val Prec: 0.2699 Val Rcll: 0.3949 Val F1: 0.2795\n",
      "EPOCH: 8/10 Train Loss: 1.1648 Train Acc: 0.5782 Train Prec: 0.7557 Train Rcll: 0.7022 Train F1: 0.6765 Val Loss: 1.0540 Val Acc: 0.4281 Val Prec: 0.5351 Val Rcll: 0.5934 Val F1: 0.5106\n",
      "EPOCH: 9/10 Train Loss: 1.0898 Train Acc: 0.5622 Train Prec: 0.7452 Train Rcll: 0.6956 Train F1: 0.6443 Val Loss: 1.0596 Val Acc: 0.3897 Val Prec: 0.4910 Val Rcll: 0.5331 Val F1: 0.4425\n",
      "EPOCH: 10/10 Train Loss: 1.0262 Train Acc: 0.6033 Train Prec: 0.6938 Train Rcll: 0.7155 Train F1: 0.6961 Val Loss: 1.2676 Val Acc: 0.3868 Val Prec: 0.4398 Val Rcll: 0.5576 Val F1: 0.4394\n",
      "\n",
      "VALIDATION RESULTS FOR SUBJECT 2: \n",
      "\n",
      "Avg. Accuracy: 0.3867855809558717\n",
      "Avg. Precision: 0.43978255265027855\n",
      "Avg. Recall: 0.5576127358150843\n",
      "Avg. F1: 0.4393686477591652\n",
      "\n",
      "VALIDATION RESULTS (PER CLASS): \n",
      "\n",
      "Accuracy:\n",
      "   climbing_down: 0.0067842605156037995\n",
      "   climbing_up: 0.260707635009311\n",
      "   jumping: 0.11759425493716337\n",
      "   lying: 0.9853768278965129\n",
      "   running: 0.021820448877805487\n",
      "   sitting: 0.8584070796460177\n",
      "   standing: 0.8413719185423365\n",
      "   walking: 0.0022222222222222222\n",
      "\n",
      "Precision:\n",
      "   climbing_down: 0.15151515151515152\n",
      "   climbing_up: 0.31987814166031986\n",
      "   jumping: 0.11823104693140794\n",
      "   lying: 0.9977220956719818\n",
      "   running: 0.045871559633027525\n",
      "   sitting: 0.8660714285714286\n",
      "   standing: 0.9356376638855781\n",
      "   walking: 0.08333333333333333\n",
      "\n",
      "Recall:\n",
      "   climbing_down: 0.007052186177715092\n",
      "   climbing_up: 0.584958217270195\n",
      "   jumping: 0.9562043795620438\n",
      "   lying: 0.9875986471251409\n",
      "   running: 0.03995433789954338\n",
      "   sitting: 0.9897959183673469\n",
      "   standing: 0.8930602957906713\n",
      "   walking: 0.002277904328018223\n",
      "\n",
      "F1:\n",
      "   climbing_down: 0.013477088948787064\n",
      "   climbing_up: 0.413589364844904\n",
      "   jumping: 0.2104417670682731\n",
      "   lying: 0.9926345609065156\n",
      "   running: 0.04270896888346553\n",
      "   sitting: 0.9238095238095239\n",
      "   standing: 0.9138533178114087\n",
      "   walking: 0.004434589800443459\n",
      "\n",
      "GENERALIZATION GAP ANALYSIS: \n",
      "\n",
      "Train-Val-Accuracy Difference: 0.21653668830868433\n",
      "Train-Val-Precision Difference: 0.2539814995647741\n",
      "Train-Val-Recall Difference: 0.15786755590393386\n",
      "Train-Val-F1 Difference: 0.25677633663085087\n"
     ]
    }
   ],
   "source": [
    "# needed for saving results\n",
    "log_date = time.strftime('%Y%m%d')\n",
    "log_timestamp = time.strftime('%H%M%S')\n",
    "\n",
    "# iterate over all subjects\n",
    "for i, sbj in enumerate(np.unique(train_valid_data.iloc[:, 0])):\n",
    "    print('\\n VALIDATING FOR SUBJECT {0} OF {1}'.format(int(sbj) + 1, int(np.max(train_valid_data.iloc[:, 0])) + 1))\n",
    "    \n",
    "    # define the train data to be everything, but the data of the current subject\n",
    "    train_data = data[data.iloc[:, 0] != sbj]\n",
    "    # define the validation data to be the data of the current subject\n",
    "    valid_data = data[data.iloc[:, 0] == sbj]\n",
    "    \n",
    "    print(train_data.shape, valid_data.shape)\n",
    "    \n",
    "    # apply the sliding window on top of both the train and validation data; use the \"apply_sliding_window\" function\n",
    "    # found in data_processing.sliding_window\n",
    "    X_train, y_train = apply_sliding_window(train_data.iloc[:, :-1], train_data.iloc[:, -1],\n",
    "                                            sliding_window_size=config['sw_length'],\n",
    "                                            unit=config['sw_unit'],\n",
    "                                            sampling_rate=config['sampling_rate'],\n",
    "                                            sliding_window_overlap=config['sw_overlap'],\n",
    "                                            )\n",
    "\n",
    "    print(X_train.shape, y_train.shape)\n",
    "\n",
    "    X_valid, y_valid = apply_sliding_window(valid_data.iloc[:, :-1], valid_data.iloc[:, -1],\n",
    "                                            sliding_window_size=config['sw_length'],\n",
    "                                            unit=config['sw_unit'],\n",
    "                                            sampling_rate=config['sampling_rate'],\n",
    "                                            sliding_window_overlap=config['sw_overlap'],\n",
    "                                            )\n",
    "\n",
    "    print(X_valid.shape, y_valid.shape)\n",
    "    \n",
    "    # omit the first feature column (subject_identifier) from the train and validation dataset\n",
    "    X_train, X_valid = X_train[:, :, 1:], X_valid[:, :, 1:]\n",
    "    \n",
    "    # within the config file, set the parameters 'window_size' and 'nb_channels' accordingly\n",
    "    # window_size = size of the sliding window in units\n",
    "    # nb_channels = number of feature channels\n",
    "    config['window_size'] = X_train.shape[1]\n",
    "    config['nb_channels'] = X_train.shape[2]\n",
    "    \n",
    "    # define the network to be a DeepConvLSTM object; can be imported from model.DeepConvLSTM\n",
    "    # pass it the config object\n",
    "    net = DeepConvLSTM(config=config)\n",
    "\n",
    "    X_train, y_train,  = X_train.astype(np.float32), y_train.astype(np.uint8)\n",
    "    X_valid, y_valid = X_valid.astype(np.float32), y_valid.astype(np.uint8)\n",
    "\n",
    "    cross_participant_net, val_output, train_output = train(X_train, y_train, X_valid, y_valid,\n",
    "                                                            network=net, \n",
    "                                                            config=config, \n",
    "                                                            log_date=log_date,\n",
    "                                                            log_timestamp=log_timestamp)\n",
    "    \n",
    "    # the next bit prints out the average results per subject if you did everything correctly\n",
    "    cls = np.array(range(config['nb_classes']))\n",
    "    \n",
    "    print('\\nVALIDATION RESULTS FOR SUBJECT {0}: '.format(int(sbj) + 1))\n",
    "    print(\"\\nAvg. Accuracy: {0}\".format(jaccard_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "    print(\"Avg. Precision: {0}\".format(precision_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "    print(\"Avg. Recall: {0}\".format(recall_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "    print(\"Avg. F1: {0}\".format(f1_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "\n",
    "    print(\"\\nVALIDATION RESULTS (PER CLASS): \")\n",
    "    print(\"\\nAccuracy:\")\n",
    "    for i, rslt in enumerate(jaccard_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    print(\"\\nPrecision:\")\n",
    "    for i, rslt in enumerate(precision_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    print(\"\\nRecall:\")\n",
    "    for i, rslt in enumerate(recall_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "    print(\"\\nF1:\")\n",
    "    for i, rslt in enumerate(f1_score(val_output[:, 1], val_output[:, 0], average=None, labels=cls)):\n",
    "        print(\"   {0}: {1}\".format(class_names[i], rslt))\n",
    "\n",
    "    print(\"\\nGENERALIZATION GAP ANALYSIS: \")\n",
    "    print(\"\\nTrain-Val-Accuracy Difference: {0}\".format(jaccard_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                      jaccard_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "    print(\"Train-Val-Precision Difference: {0}\".format(precision_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                       precision_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "    print(\"Train-Val-Recall Difference: {0}\".format(recall_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                    recall_score(val_output[:, 1], val_output[:, 0], average='macro')))\n",
    "    print(\"Train-Val-F1 Difference: {0}\".format(f1_score(train_output[:, 1], train_output[:, 0], average='macro') -\n",
    "                                                f1_score(val_output[:, 1], val_output[:, 0], average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after having implemented each of the validation techniques we want to get an unbiased view of how our trained algorithm perfoms on unseen data. To so we use the testing set which we split off the original dataset within the first step of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Testing your trained networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Apply the sliding window on top of the test data and omit the 'subject_identifier' column from the dataset\n",
    "2. Using the predict function of the DL-ARC obtain results on the test dataset using each of the trained networks\n",
    "3. Which model does perform the best and why? Was this expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6536, 50, 4) (6536,)\n",
      "COMPILED TEST RESULTS: \n",
      "\n",
      "Test results (train-valid-split): \n",
      "TEST RESULTS: \n",
      "Avg. Accuracy: 0.09035619618330168\n",
      "Avg. Precision: 0.11683614302774101\n",
      "Avg. Recall: 0.1316811109437625\n",
      "Avg. F1: 0.11418997366534062\n",
      "TEST RESULTS (PER CLASS): \n",
      "Accuracy: [0.         0.         0.00552486 0.         0.66101695 0.05630776\n",
      " 0.         0.        ]\n",
      "Precision: [0.         0.         0.02857143 0.         0.84086242 0.06525529\n",
      " 0.         0.        ]\n",
      "Recall: [0.         0.         0.00680272 0.         0.75553506 0.29111111\n",
      " 0.         0.        ]\n",
      "F1: [0.         0.         0.01098901 0.         0.79591837 0.10661241\n",
      " 0.         0.        ]\n",
      "\n",
      "Test results (k-fold): \n",
      "TEST RESULTS: \n",
      "Avg. Accuracy: 0.020392950378779043\n",
      "Avg. Precision: 0.04028757833309204\n",
      "Avg. Recall: 0.048827137250964346\n",
      "Avg. F1: 0.036964828253996565\n",
      "TEST RESULTS (PER CLASS): \n",
      "Accuracy: [0.         0.         0.03783102 0.         0.12507778 0.0002348\n",
      " 0.         0.        ]\n",
      "Precision: [0.         0.         0.0443787  0.         0.27762431 0.00029762\n",
      " 0.         0.        ]\n",
      "Recall: [0.         0.         0.20408163 0.         0.18542435 0.00111111\n",
      " 0.         0.        ]\n",
      "F1: [0.         0.         0.07290401 0.         0.22234513 0.00046948\n",
      " 0.         0.        ]\n",
      "\n",
      "Test results (per-participant): \n",
      "TEST RESULTS: \n",
      "Avg. Accuracy: 0.24225554629981255\n",
      "Avg. Precision: 0.24928732102758092\n",
      "Avg. Recall: 0.24292652131066766\n",
      "Avg. F1: 0.24606297346614742\n",
      "TEST RESULTS (PER CLASS): \n",
      "Accuracy: [0.         0.         0.         0.96132597 0.         0.9767184\n",
      " 0.         0.        ]\n",
      "Precision: [0.         0.         0.         0.99656357 0.         0.99773499\n",
      " 0.         0.        ]\n",
      "Recall: [0.         0.         0.         0.96452328 0.         0.97888889\n",
      " 0.         0.        ]\n",
      "F1: [0.         0.         0.         0.98028169 0.         0.9882221\n",
      " 0.         0.        ]\n",
      "\n",
      "Test results (cross-participant): \n",
      "TEST RESULTS: \n",
      "Avg. Accuracy: 0.6290430893990748\n",
      "Avg. Precision: 0.8200280665493734\n",
      "Avg. Recall: 0.7325943431684342\n",
      "Avg. F1: 0.7138195789837728\n",
      "TEST RESULTS (PER CLASS): \n",
      "Accuracy: [0.06922126 0.259375   0.95973154 0.96039604 0.79428044 0.95361381\n",
      " 0.63724625 0.39848037]\n",
      "Precision: [0.77777778 0.68032787 0.9862069  0.99204545 1.         0.97036224\n",
      " 0.74974039 0.4037639 ]\n",
      "Recall: [0.07061791 0.29537367 0.97278912 0.96784922 0.79428044 0.98222222\n",
      " 0.80941704 0.96820513]\n",
      "F1: [0.12947977 0.41191067 0.97945205 0.97979798 0.88534704 0.97625621\n",
      " 0.77843666 0.56987625]\n"
     ]
    }
   ],
   "source": [
    "from model.train import predict\n",
    "\n",
    "\n",
    "X_test, y_test = apply_sliding_window(test_data.iloc[:, :-1], test_data.iloc[:, -1],\n",
    "                                      sliding_window_size=config['sw_length'],\n",
    "                                      unit=config['sw_unit'],\n",
    "                                      sampling_rate=config['sampling_rate'],\n",
    "                                      sliding_window_overlap=config['sw_overlap'],\n",
    "                                      )\n",
    "\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "X_test = X_test[:, :, 1:]\n",
    "\n",
    "print('COMPILED TEST RESULTS: ')\n",
    "print('\\nTest results (train-valid-split): ')\n",
    "predict(X_test, y_test, train_valid_net, config, log_date, log_timestamp)\n",
    "\n",
    "print('\\nTest results (k-fold): ')\n",
    "predict(X_test, y_test, kfold_net, config, log_date, log_timestamp)\n",
    "\n",
    "print('\\nTest results (per-participant): ')\n",
    "predict(X_test, y_test, per_participant_net, config, log_date, log_timestamp)\n",
    "\n",
    "print('\\nTest results (cross-participant): ')\n",
    "predict(X_test, y_test, cross_participant_net, config, log_date, log_timestamp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
